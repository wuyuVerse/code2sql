# 模型基本配置
model_name: qwen3_14b
model_path: /data/cloud_disk_1/home/wuyu/code2sql/model/qwen/Qwen3-14B
template: qwen
trust_remote_code: true

# 全量微调配置
stage: sft
do_train: true
finetuning_type: full
deepspeed: examples/deepspeed/ds_z3_config.json
rope_scaling: linear
flash_attn: fa2
use_unsloth: false

# GPU配置 - 使用GPU 4,5,6,7
cuda_visible_devices: "4,5,6,7"  # 指定要使用的GPU设备，用逗号分隔

# 数据集配置 - 使用ORM2SQL数据集（数据集名称和样本数将在训练时自动更新）
dataset_dir: model/data/orm2sql_training_data
dataset: auto_generated  # 训练时自动更新为实际数据集名称
template: qwen
cutoff_len: 2048
max_samples: 0  # 训练时自动更新为实际样本数
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 4

# 训练参数
learning_rate: 2.0e-5  # 针对代码生成任务稍微提高学习率
num_train_epochs: 2.0  # 训练轮次，总步数 = (max_samples / 实际batch_size) * num_train_epochs
# max_steps: 100  # 可选：直接指定最大训练步数，会覆盖num_train_epochs
max_grad_norm: 1.0
per_device_train_batch_size: 2 # 每个GPU的训练批次大小
gradient_accumulation_steps: 8 # 梯度累积步数
lr_scheduler_type: "cosine" # 学习率调度器
warmup_ratio: 0.1 # 预热比例
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null
weight_decay: 0.1

# 当前设置下的Step计算：
# 实际Batch Size = 2 * 4 GPUs * 8 = 64
# Steps per Epoch = 17761 samples / 64 = ~278 steps  
# Total Steps = 278 * 2 epochs = 556 steps

# 日志和保存配置
logging_steps: 10  # 每10步记录一次日志到SwanLab
save_steps: 278  # 每个epoch保存一次（约278步）
eval_steps: 278
save_total_limit: 3
save_safetensors: true
plot_loss: true
overwrite_output_dir: true
save_only_model: false
report_to: swanlab

# SwanLab 项目配置
swanlab_project: code2sql  # SwanLab 项目名称

# 输出路径配置（相对于 LLaMA-Factory 目录）- 使用不同的输出目录避免冲突
output_dir: saves/qwen3-14b-orm2sql-ft-gpu4567 