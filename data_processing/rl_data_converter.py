#!/usr/bin/env python3
"""
RLè®­ç»ƒæ•°æ®è½¬æ¢å™¨

å°†è®­ç»ƒæ•°æ®è½¬æ¢ä¸ºRLHFè®­ç»ƒæ ¼å¼ï¼Œè¾“å‡ºä¸ºparquetæ–‡ä»¶
"""

import json
import os
import logging
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import glob
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parents[1]
sys.path.insert(0, str(project_root))

from config.rl.data_conversion.orm2sql_prompt_template import PROMPT_TEMPLATE
from utils.preprocess import preprocess_record

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RLDataConverter:
    """RLè®­ç»ƒæ•°æ®è½¬æ¢å™¨"""
    
    def __init__(self, project_root: Optional[str] = None):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        """
        if project_root is None:
            project_root = str(Path(__file__).parents[1])
        
        self.project_root = Path(project_root)
        self.workflow_output_dir = self.project_root / "workflow_output"
        self.rl_data_dir = self.project_root / "model" / "data" / "orm2sql_rl_data"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.rl_data_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")
        logger.info(f"å·¥ä½œæµè¾“å‡ºç›®å½•: {self.workflow_output_dir}")
        logger.info(f"RLæ•°æ®è¾“å‡ºç›®å½•: {self.rl_data_dir}")
    
    def find_latest_workflow_output(self) -> Optional[Path]:
        """
        æŸ¥æ‰¾æœ€æ–°çš„workflowè¾“å‡ºç›®å½•
        
        Returns:
            æœ€æ–°workflowç›®å½•è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
        """
        if not self.workflow_output_dir.exists():
            logger.error(f"å·¥ä½œæµè¾“å‡ºç›®å½•ä¸å­˜åœ¨: {self.workflow_output_dir}")
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰workflowç›®å½•
        workflow_dirs = list(self.workflow_output_dir.glob("workflow_*"))
        if not workflow_dirs:
            logger.error("æœªæ‰¾åˆ°ä»»ä½•workflowè¾“å‡ºç›®å½•")
            return None
        
        # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè·å–æœ€æ–°çš„
        workflow_dirs.sort(key=lambda x: x.name, reverse=True)
        latest_dir = workflow_dirs[0]
        
        logger.info(f"æ‰¾åˆ°æœ€æ–°workflowç›®å½•: {latest_dir}")
        return latest_dir
    
    def load_workflow_data(self, workflow_dir: Path) -> List[Dict]:
        """
        åŠ è½½workflowå¤„ç†åçš„æ•°æ®
        
        Args:
            workflow_dir: workflowè¾“å‡ºç›®å½•
            
        Returns:
            å¤„ç†åçš„æ•°æ®åˆ—è¡¨
        """
        final_data_file = workflow_dir / "final_processed_dataset.json"
        
        if not final_data_file.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æœ€ç»ˆå¤„ç†æ•°æ®æ–‡ä»¶: {final_data_file}")
        
        logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {final_data_file}")
        logger.info(f"æ–‡ä»¶å¤§å°: {final_data_file.stat().st_size / (1024*1024):.1f} MB")
        
        with open(final_data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(data)} æ¡è®°å½•")
        return data
    
    def format_code_metadata(self, code_meta_data: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–ä»£ç å…ƒæ•°æ®ä¸ºå­—ç¬¦ä¸²
        
        Args:
            code_meta_data: ä»£ç å…ƒæ•°æ®åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
        """
        if not code_meta_data:
            return ""
        
        formatted_parts = []
        for meta in code_meta_data:
            if 'code_key' in meta and 'code_value' in meta:
                part = f"**{meta['code_key']}**:\n{meta['code_value']}"
                if 'code_file' in meta:
                    part += f"\n(æ–‡ä»¶: {meta['code_file']})"
                formatted_parts.append(part)
        
        return "\n\n".join(formatted_parts)
    
    def create_rl_prompt(self, record: Dict) -> List[Dict]:
        """
        æ ¹æ®è®°å½•åˆ›å»ºRLè®­ç»ƒæç¤ºè¯ï¼ˆèŠå¤©æ ¼å¼ï¼‰
        
        Args:
            record: å•æ¡ORMè®°å½•
            
        Returns:
            èŠå¤©æ ¼å¼çš„æç¤ºè¯åˆ—è¡¨
        """
        function_name = record.get("function_name", "æœªçŸ¥å‡½æ•°")
        orm_code = record.get("orm_code", "")
        caller = record.get("caller", "")
        callee = record.get("callee", "")
        code_meta_data_str = self.format_code_metadata(record.get("code_meta_data", []))
        
        # ä½¿ç”¨orm2sql_prompt_template.pyä¸­çš„å®Œæ•´æ¨¡æ¿
        user_content = PROMPT_TEMPLATE.format(
            function_name=function_name,
            orm_code=orm_code,
            caller=caller,
            code_meta_data_str=code_meta_data_str
        )
        
        return [{"role": "user", "content": user_content}]
    
    def extract_ground_truth(self, record: Dict) -> str:
        """
        æå–æ ‡å‡†ç­”æ¡ˆä½œä¸ºground_truth
        
        Args:
            record: å•æ¡ORMè®°å½•
            
        Returns:
            æ ‡å‡†ç­”æ¡ˆå­—ç¬¦ä¸²
        """
        sql_statement_list = record.get('sql_statement_list', [])
        return json.dumps(sql_statement_list, ensure_ascii=False, indent=None)
    
    def convert_to_rl_format(self, data: List[Dict]) -> pd.DataFrame:
        """
        å°†ORMæ•°æ®è½¬æ¢ä¸ºRLè®­ç»ƒæ ¼å¼
        
        Args:
            data: workflowå¤„ç†åçš„æ•°æ®
            
        Returns:
            è½¬æ¢åçš„RLè®­ç»ƒæ•°æ®DataFrame
        """
        rl_data = {
            "data_source": [],
            "prompt": [],
            "ability": [],
            "reward_model": [],
            "extra_info": []
        }
        
        logger.info("å¼€å§‹è½¬æ¢RLè®­ç»ƒæ•°æ®...")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_records = len(data)
        filtered_count = 0
        has_keywords_count = 0
        
        for i, record in enumerate(data):
            if i % 1000 == 0:
                logger.info(f"å·²å¤„ç† {i}/{len(data)} æ¡è®°å½•")
            
            try:
                # === æ–°å¢ï¼šé¢„å¤„ç†æ­¥éª¤ï¼ˆä»…è¡¨åå­—æ®µåæŠ½å–ï¼‰ ===
                ok, pre_tables, pre_columns = preprocess_record(record)
                if not ok:
                    filtered_count += 1
                    logger.debug(f"è®°å½• {i} è¢«è¿‡æ»¤ï¼šæŠ½å–å¤±è´¥æˆ–åŒ…å«LACK INFORMATION")
                    continue
                
                # ç»Ÿè®¡å…³é”®è¯æ ·æœ¬æ•°ï¼ˆä½¿ç”¨åŸå§‹æ•°æ®ï¼‰
                if record.get("llm_keyword_analysis", {}).get("has_special_keywords", False):
                    has_keywords_count += 1
                
                # åˆ›å»ºèŠå¤©æ ¼å¼çš„æç¤ºè¯
                prompt = self.create_rl_prompt(record)
                
                # æå–æ ‡å‡†ç­”æ¡ˆ
                ground_truth = self.extract_ground_truth(record)
                
                # æ„å»ºreward_modelé…ç½®
                reward_model = {
                    "style": "rule",  # ä½¿ç”¨è§„åˆ™è¯„åˆ†ï¼Œä¸æ˜¯æ¨¡å‹è¯„åˆ†
                    "ground_truth": ground_truth
                }
                
                # æ„å»ºextra_infoï¼ŒåŒ…å«æ‰€æœ‰ORMç›¸å…³ä¿¡æ¯
                extra_info = {
                    "index": i,
                    "split": "train",  # é»˜è®¤ä¸ºè®­ç»ƒé›†
                    "function_name": record.get('function_name', ''),
                    "source_file": record.get('source_file', ''),
                    "sql_pattern_cnt": record.get('sql_pattern_cnt', 0),
                    "sql_types": record.get('sql_types', []),
                    # ä¿æŒåŸæœ‰ORMä¿¡æ¯
                    "orm_code": record.get('orm_code', ''),
                    "caller": record.get('caller', ''),
                    "callee": record.get('callee', ''),
                    "code_meta_data": record.get('code_meta_data', []),
                    # === æ–°å¢ï¼šé¢„å¤„ç†çš„è¡¨åå­—æ®µåç»“æœ ===
                    "pre_tables": list(pre_tables),
                    "pre_columns": list(pre_columns),
                    # === ä¿æŒåŸæœ‰å…³é”®è¯ä¿¡æ¯ä¸å˜ ===
                    "llm_keyword_analysis": record.get("llm_keyword_analysis", {})
                }
                
                # æ·»åŠ åˆ°æ•°æ®é›†
                rl_data["data_source"].append("code2sql_orm")
                rl_data["prompt"].append(prompt)
                rl_data["ability"].append("code_generation")
                rl_data["reward_model"].append(reward_model)
                rl_data["extra_info"].append(extra_info)
                
            except Exception as e:
                logger.error(f"å¤„ç†ç¬¬ {i} æ¡è®°å½•æ—¶å‡ºé”™: {e}")
                continue
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        final_count = len(rl_data['data_source'])
        logger.info(f"=== é¢„å¤„ç†ç»Ÿè®¡ä¿¡æ¯ ===")
        logger.info(f"åŸå§‹æ ·æœ¬æ•°: {total_records}")
        logger.info(f"è¿‡æ»¤æ ·æœ¬æ•°: {filtered_count}")
        logger.info(f"ä¿ç•™æ ·æœ¬æ•°: {final_count}")
        logger.info(f"ä¿ç•™ç‡: {final_count/total_records*100:.1f}%")
        logger.info(f"æœ‰å…³é”®è¯æ ·æœ¬æ•°: {has_keywords_count}")
        logger.info(f"å…³é”®è¯æ ·æœ¬å æ¯”: {has_keywords_count/final_count*100:.1f}%")
        
        logger.info(f"è½¬æ¢å®Œæˆï¼Œå…±ç”Ÿæˆ {final_count} æ¡RLè®­ç»ƒæ ·æœ¬")
        return pd.DataFrame(rl_data)
    
    def split_train_val(self, df: pd.DataFrame, val_ratio: float = 0.1) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        
        Args:
            df: å®Œæ•´æ•°æ®é›†DataFrame
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            
        Returns:
            (è®­ç»ƒé›†DataFrame, éªŒè¯é›†DataFrame)
        """
        # éšæœºæ‰“ä¹±æ•°æ®
        df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        # è®¡ç®—åˆ’åˆ†ç‚¹
        val_size = int(len(df_shuffled) * val_ratio)
        
        val_df = df_shuffled.iloc[:val_size].copy()
        train_df = df_shuffled.iloc[val_size:].copy()
        
        # æ›´æ–°splitæ ‡è®°
        train_df.loc[:, 'extra_info'] = train_df['extra_info'].apply(
            lambda x: {**x, 'split': 'train'}
        )
        val_df.loc[:, 'extra_info'] = val_df['extra_info'].apply(
            lambda x: {**x, 'split': 'val'}
        )
        
        logger.info(f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ: è®­ç»ƒé›† {len(train_df)} æ¡, éªŒè¯é›† {len(val_df)} æ¡")
        return train_df, val_df
    
    def save_rl_data(self, train_df: pd.DataFrame, val_df: pd.DataFrame, 
                     output_name: Optional[str] = None) -> Tuple[Path, Path]:
        """
        ä¿å­˜RLè®­ç»ƒæ•°æ®ä¸ºparquetæ ¼å¼
        
        Args:
            train_df: è®­ç»ƒé›†DataFrame
            val_df: éªŒè¯é›†DataFrame
            output_name: è¾“å‡ºæ–‡ä»¶åå‰ç¼€ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            (è®­ç»ƒé›†æ–‡ä»¶è·¯å¾„, éªŒè¯é›†æ–‡ä»¶è·¯å¾„)
        """
        if output_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_name = f"orm2sql_rl_{timestamp}"
        
        train_path = self.rl_data_dir / f"{output_name}_train.parquet"
        val_path = self.rl_data_dir / f"{output_name}_val.parquet"
        
        logger.info(f"æ­£åœ¨ä¿å­˜è®­ç»ƒé›†åˆ°: {train_path}")
        train_df.to_parquet(train_path, index=False)
        train_size = train_path.stat().st_size / (1024 * 1024)
        logger.info(f"è®­ç»ƒé›†ä¿å­˜å®Œæˆï¼Œæ–‡ä»¶å¤§å°: {train_size:.1f} MB")
        
        logger.info(f"æ­£åœ¨ä¿å­˜éªŒè¯é›†åˆ°: {val_path}")
        val_df.to_parquet(val_path, index=False)
        val_size = val_path.stat().st_size / (1024 * 1024)
        logger.info(f"éªŒè¯é›†ä¿å­˜å®Œæˆï¼Œæ–‡ä»¶å¤§å°: {val_size:.1f} MB")
        
        return train_path, val_path
    
    def create_dataset_info(self, train_df: pd.DataFrame, val_df: pd.DataFrame, 
                           dataset_name: str) -> Dict:
        """
        åˆ›å»ºæ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
        
        Args:
            train_df: è®­ç»ƒé›†DataFrame
            val_df: éªŒè¯é›†DataFrame
            dataset_name: æ•°æ®é›†åç§°
            
        Returns:
            æ•°æ®é›†ä¿¡æ¯å­—å…¸
        """
        return {
            "dataset_name": dataset_name,
            "description": "ORMåˆ°SQLè½¬æ¢çš„RLè®­ç»ƒæ•°æ®é›†ï¼ŒåŸºäºçœŸå®ä»£ç åˆ†æç”Ÿæˆ",
            "data_source": "code2sql_orm",
            "ability": "code_generation",
            "train": {
                "file_name": f"{dataset_name}_train.parquet",
                "num_samples": len(train_df),
                "size_mb": f"{(self.rl_data_dir / f'{dataset_name}_train.parquet').stat().st_size / (1024*1024):.1f}"
            },
            "val": {
                "file_name": f"{dataset_name}_val.parquet",
                "num_samples": len(val_df),
                "size_mb": f"{(self.rl_data_dir / f'{dataset_name}_val.parquet').stat().st_size / (1024*1024):.1f}"
            },
            "total_samples": len(train_df) + len(val_df),
            "reward_model_style": "rule",
            "format": "RLHF parquet format with chat template"
        }
    
    def run_conversion(self, workflow_dir: Optional[Path] = None, output_name: Optional[str] = None,
                      val_ratio: float = 0.1) -> Tuple[Path, Path, Dict]:
        """
        æ‰§è¡Œå®Œæ•´çš„RLæ•°æ®è½¬æ¢æµç¨‹
        
        Args:
            workflow_dir: æŒ‡å®šçš„workflowç›®å½•ï¼ˆå¯é€‰ï¼‰
            output_name: è¾“å‡ºæ–‡ä»¶åå‰ç¼€ï¼ˆå¯é€‰ï¼‰
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            
        Returns:
            (è®­ç»ƒé›†æ–‡ä»¶è·¯å¾„, éªŒè¯é›†æ–‡ä»¶è·¯å¾„, æ•°æ®é›†ä¿¡æ¯)
        """
        # 1. æŸ¥æ‰¾æˆ–ä½¿ç”¨æŒ‡å®šçš„workflowç›®å½•
        if workflow_dir is None:
            workflow_dir = self.find_latest_workflow_output()
            if workflow_dir is None:
                raise FileNotFoundError("æœªæ‰¾åˆ°workflowè¾“å‡ºç›®å½•")
        
        # 2. åŠ è½½æ•°æ®
        data = self.load_workflow_data(workflow_dir)
        
        # 3. è½¬æ¢ä¸ºRLæ ¼å¼
        rl_df = self.convert_to_rl_format(data)
        
        # 4. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_df, val_df = self.split_train_val(rl_df, val_ratio)
        
        # 5. ä¿å­˜æ•°æ®
        if output_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_name = f"orm2sql_rl_{timestamp}"
        
        train_path, val_path = self.save_rl_data(train_df, val_df, output_name)
        
        # 6. åˆ›å»ºæ•°æ®é›†ä¿¡æ¯
        dataset_info = self.create_dataset_info(train_df, val_df, output_name)
        
        # 7. ä¿å­˜æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
        info_path = self.rl_data_dir / f"{output_name}_info.json"
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ•°æ®é›†ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")
        
        return train_path, val_path, dataset_info


def main():
    """ä¸»å‡½æ•°"""
    converter = RLDataConverter()
    
    try:
        # æ‰§è¡Œè½¬æ¢
        train_path, val_path, dataset_info = converter.run_conversion()
        
        print(f"\nâœ… RLæ•°æ®è½¬æ¢å®Œæˆ!")
        print(f"ğŸ“ è®­ç»ƒé›†ä¿å­˜è·¯å¾„: {train_path}")
        print(f"ğŸ“ éªŒè¯é›†ä¿å­˜è·¯å¾„: {val_path}")
        print(f"ğŸ“Š è®­ç»ƒé›†æ ·æœ¬æ•°: {dataset_info['train']['num_samples']}")
        print(f"ğŸ“Š éªŒè¯é›†æ ·æœ¬æ•°: {dataset_info['val']['num_samples']}")
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {dataset_info['total_samples']}")
        info_file = converter.rl_data_dir / f"{dataset_info['dataset_name']}_info.json"
        print(f"ğŸ“ æ•°æ®é›†ä¿¡æ¯: {info_file}")
        
    except Exception as e:
        logger.error(f"RLæ•°æ®è½¬æ¢å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main() 