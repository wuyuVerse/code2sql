"""
æ ¸å¿ƒéªŒè¯å™¨æ¨¡å—
"""
import asyncio
import json
import logging
import sys
from pathlib import Path

import yaml
from tqdm import tqdm

from utils.llm_client import LLMClientManager
from config.prompts import REANALYSIS_PROMPT
from config.validation_prompts import (
    ANALYSIS_PROMPT_TEMPLATE,
    VERIFICATION_PROMPT_TEMPLATE,
    FORMATTING_PROMPT_TEMPLATE,
)

logger = logging.getLogger(__name__)


class RerunValidator:
    """å°è£…é‡æ–°è¿è¡Œåˆ†æå’ŒéªŒè¯é€»è¾‘çš„ç±»"""

    def __init__(self, config_path="config/rerun_config.yaml"):
        """
        åˆå§‹åŒ–éªŒè¯å™¨ã€‚
        Args:
            config_path: é…ç½®æ–‡ä»¶çš„è·¯å¾„ã€‚
        """
        self.config = self._load_config(config_path)
        self.client_manager = LLMClientManager()
        self._setup_logging()

    def _load_config(self, config_path: str) -> dict:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
            sys.exit(1)
        except yaml.YAMLError as e:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶YAMLæ ¼å¼é”™è¯¯: {config_path} - {e}")
            sys.exit(1)

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—è®°å½•å™¨"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

    def _format_rerun_prompt(self, record: dict) -> str:
        """æ ¼å¼åŒ–ç”¨äºé‡æ–°åˆ†æçš„æç¤ºè¯ï¼ˆæ—§é€»è¾‘ï¼‰"""
        code_value = record.get('orm_code', '')
        if not code_value:
            if record.get('code_meta_data') and isinstance(record['code_meta_data'], list) and record['code_meta_data']:
                code_value = record['code_meta_data'][0].get('code_value', '')

        function_name = record.get('function_name', 'N/A')
        caller = record.get('caller', 'N/A')
        code_meta_data_str = json.dumps(record.get('code_meta_data', []), ensure_ascii=False, indent=2)
        callee = "N/A"
        
        return REANALYSIS_PROMPT.format(
            function_name=function_name,
            code_value=code_value,
            caller=caller,
            code_meta_data_str=code_meta_data_str,
            callee=callee
        )

    async def _run_single_analysis(self, semaphore: asyncio.Semaphore, record: dict, pbar: tqdm, output_file, file_lock) -> dict:
        """å¯¹å•ä¸ªè®°å½•è¿›è¡Œåˆ†æï¼Œå¹¶ç«‹å³å°†ç»“æœå†™å…¥æ–‡ä»¶"""
        async with semaphore:
            prompt = self._format_rerun_prompt(record)
            client = self.client_manager.get_client(self.config['server'])
            
            try:
                loop = asyncio.get_event_loop()
                result_content = await loop.run_in_executor(
                    None, 
                    lambda: client.call_openai(prompt, max_tokens=4096, temperature=0.0)
                )
                
                try:
                    new_sql = json.loads(result_content)
                except (json.JSONDecodeError, TypeError):
                    new_sql = result_content
                
                analysis_result = {
                    "function_name": record["function_name"],
                    "source_file": record["source_file"],
                    "original_orm_code": record.get("orm_code", ""),
                    "new_sql_analysis_result": new_sql
                }
            except Exception as e:
                logger.error(f"åˆ†æå¤±è´¥: {record['function_name']} - {e}")
                analysis_result = {
                    "function_name": record["function_name"],
                    "source_file": record["source_file"],
                    "error": str(e)
                }

            async with file_lock:
                output_file.write(json.dumps(analysis_result, ensure_ascii=False) + '\n')
                output_file.flush()

            pbar.update(1)
            return analysis_result

    def _get_common_prompt_fields(self, record: dict) -> dict:
        """ä»è®°å½•ä¸­æå–ç”¨äºæ ¼å¼åŒ–æç¤ºè¯çš„é€šç”¨å­—æ®µ"""
        code_value = record.get('orm_code', '')
        if not code_value and record.get('code_meta_data'):
             if isinstance(record['code_meta_data'], list) and record['code_meta_data']:
                code_value = record['code_meta_data'][0].get('code_value', '')

        return {
            "function_name": record.get('function_name', 'N/A'),
            "code_value": code_value,
            "code_meta_data_str": json.dumps(record.get('code_meta_data', []), ensure_ascii=False, indent=2)
        }

    def generate_precheck_prompts(self, record: dict, analysis_result: str = "") -> dict:
        """
        ä¸ºç»™å®šçš„è®°å½•ç”Ÿæˆä¸‰é˜¶æ®µçš„é¢„æ£€æŸ¥æç¤ºè¯ã€‚
        
        Args:
            record: éœ€è¦åˆ†æçš„æ•°æ®è®°å½•ã€‚
            analysis_result: (å¯é€‰) ç¬¬äºŒé˜¶æ®µéªŒè¯æ—¶éœ€è¦çš„å‰ä¸€é˜¶æ®µåˆ†æç»“æœã€‚

        Returns:
            ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªé˜¶æ®µæç¤ºè¯çš„å­—å…¸ã€‚
        """
        common_fields = self._get_common_prompt_fields(record)

        prompt1 = ANALYSIS_PROMPT_TEMPLATE.format(**common_fields)
        
        prompt2 = VERIFICATION_PROMPT_TEMPLATE.format(
            analysis_result=analysis_result,
            **common_fields
        )

        # ç¬¬ä¸‰é˜¶æ®µçš„è¾“å…¥æ˜¯ç¬¬äºŒé˜¶æ®µçš„è¾“å‡ºï¼Œè¿™é‡Œæˆ‘ä»¬åªå‡†å¤‡æ¨¡æ¿
        # å®é™…ä½¿ç”¨æ—¶ï¼Œéœ€è¦ç”¨ç¬¬äºŒé˜¶æ®µçš„LLMè¾“å‡ºæ¥å¡«å…… {analysis_to_format}
        prompt3_template = FORMATTING_PROMPT_TEMPLATE

        return {
            "analysis_prompt": prompt1,
            "verification_prompt": prompt2,
            "formatting_prompt_template": prompt3_template
        }

    async def run_rerun_analysis(self):
        """æ‰§è¡Œé‡æ–°åˆ†æçš„å®Œæ•´æµç¨‹"""
        logger.info(f"å¼€å§‹é‡æ–°åˆ†æè¿‡ç¨‹ï¼Œè¾“å…¥æ–‡ä»¶: {self.config['input_file']}")
        
        try:
            with open(self.config['input_file'], 'r', encoding='utf-8') as f:
                all_data = json.load(f)
        except FileNotFoundError:
            logger.error(f"âŒ è¾“å…¥æ–‡ä»¶æœªæ‰¾åˆ°: {self.config['input_file']}")
            return
        except json.JSONDecodeError:
            logger.error(f"âŒ è¾“å…¥æ–‡ä»¶JSONæ ¼å¼é”™è¯¯: {self.config['input_file']}")
            return

        records_to_process = [r for r in all_data if r.get("sql_statement_list") == "<NO SQL GENERATE>"]
        
        if not records_to_process:
            logger.warning("æœªæ‰¾åˆ°éœ€è¦é‡æ–°åˆ†æçš„è®°å½• (<NO SQL GENERATE>)ã€‚")
            return

        logger.info(f"æ‰¾åˆ° {len(records_to_process)} æ¡è®°å½•éœ€è¦é‡æ–°åˆ†æã€‚")
        
        output_dir = Path(self.config['output_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / self.config['output_filename']
        
        semaphore = asyncio.Semaphore(self.config['concurrency'])
        file_lock = asyncio.Lock()
        
        results = []
        with open(output_path, 'w', encoding='utf-8') as f:
            with tqdm(total=len(records_to_process), desc="é‡æ–°åˆ†æè¿›åº¦") as pbar:
                tasks = [
                    self._run_single_analysis(semaphore, record, pbar, f, file_lock) 
                    for record in records_to_process
                ]
                results = await asyncio.gather(*tasks)

        self._print_summary_report(results, records_to_process, output_path)

    def _print_summary_report(self, results: list, records_to_process: list, output_path: Path):
        """æ‰“å°æœ€ç»ˆçš„æ€»ç»“æŠ¥å‘Š"""
        successful_results = [r for r in results if "error" not in r]
        failed_results = [r for r in results if "error" in r]
        
        newly_generated_count = 0
        for r in successful_results:
            analysis = r.get("new_sql_analysis_result")
            if isinstance(analysis, list) and analysis:
                first_item = analysis[0]
                if isinstance(first_item, dict):
                    should_gen_val = first_item.get("should_generate_sql")
                    if str(should_gen_val).strip().lower() == 'false':
                        newly_generated_count += 1
        
        logger.info(f"âœ… é‡æ–°åˆ†æå®Œæˆã€‚ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        print("\n" + "="*50)
        print("ğŸ“Š é‡æ–°åˆ†ææ€»ç»“æŠ¥å‘Š")
        print("="*50)
        print(f"æ€»å¤„ç†è®°å½•æ•°: {len(records_to_process)}")
        print(f"æˆåŠŸåˆ†ææ•°: {len(successful_results)}")
        print(f"å¤±è´¥åˆ†ææ•°: {len(failed_results)}")
        print("-" * 50)
        print(f"ğŸ‰ æ–°ç”ŸæˆSQLçš„è®°å½•æ•°: {newly_generated_count}")
        print(f"ä»æœªç”ŸæˆSQLçš„è®°å½•æ•°: {len(successful_results) - newly_generated_count}")
        print("="*50)
        if failed_results:
            print("\nå¤±è´¥çš„è®°å½• (å‰5æ¡):")
            for failed in failed_results[:5]:
                print(f"  - {failed['function_name']}: {failed['error']}") 