"""
Workflowç®¡ç†å™¨

ç®¡ç†æ•°æ®å¤„ç†çš„æ•´ä¸ªå·¥ä½œæµï¼ŒåŒ…æ‹¬æ•°æ®è¯»å–ã€æ¸…æ´—ã€éªŒè¯ç­‰æ­¥éª¤
"""

import json
import logging
import asyncio
import aiohttp
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime
from tqdm.asyncio import tqdm_asyncio
import re

# å°è¯•ç›¸å¯¹å¯¼å…¥ï¼Œå¦‚æœå¤±è´¥åˆ™ç›´æ¥å¯¼å…¥
try:
    from ..data_reader import DataReader
    from ..cleaning.sql_cleaner import SQLCleaner
except ImportError:
    from data_reader import DataReader
    from cleaning.sql_cleaner import SQLCleaner

logger = logging.getLogger(__name__)


class WorkflowManager:
    """å·¥ä½œæµç®¡ç†å™¨
    
    è´Ÿè´£åè°ƒæ•°æ®å¤„ç†çš„å„ä¸ªæ­¥éª¤ï¼Œè®°å½•å¤„ç†è¿‡ç¨‹å’Œç»“æœ
    """
    
    def __init__(self, base_output_dir: str = "workflow_output"):
        """
        åˆå§‹åŒ–å·¥ä½œæµç®¡ç†å™¨
        
        Args:
            base_output_dir: å·¥ä½œæµè¾“å‡ºåŸºç›®å½•
        """
        self.base_output_dir = Path(base_output_dir)
        self.base_output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå½“å‰workflowå®ä¾‹çš„ç›®å½•
        self.workflow_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.workflow_dir = self.base_output_dir / f"workflow_{self.workflow_timestamp}"
        self.workflow_dir.mkdir(exist_ok=True)
        
        # å·¥ä½œæµæ­¥éª¤è®°å½•
        self.workflow_steps = []
        self.current_data = None
        self.extracted_data = None  # æå–çš„å…³é”®è¯æ•°æ®
        
        logger.info(f"å·¥ä½œæµç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.workflow_dir}")
    
    def load_raw_dataset(self, data_dir: str) -> Dict[str, Any]:
        """
        ä»åŸå§‹æ•°æ®é›†åŠ è½½æ‰€æœ‰æ•°æ®
        
        Args:
            data_dir: åŸå§‹æ•°æ®ç›®å½•
            
        Returns:
            åŠ è½½ç»“æœä¿¡æ¯
        """
        logger.info(f"å¼€å§‹ä»åŸå§‹æ•°æ®é›†åŠ è½½æ‰€æœ‰æ•°æ®: {data_dir}")
        
        # åˆ›å»ºæ•°æ®è¯»å–å™¨å¹¶è¯»å–æ‰€æœ‰æ•°æ®
        reader = DataReader(data_dir)
        reader.read_all_files()
        
        # è½¬æ¢ä¸ºdictæ ¼å¼çš„æ•°æ®
        self.current_data = []
        for record in reader.records:
            record_dict = {
                'function_name': record.function_name,
                'orm_code': record.orm_code,
                'caller': record.caller,
                'sql_statement_list': record.sql_statement_list,
                'sql_types': record.sql_types,
                'code_meta_data': [
                    {
                        'code_file': meta.code_file,
                        'code_start_line': meta.code_start_line,
                        'code_end_line': meta.code_end_line,
                        'code_key': meta.code_key,
                        'code_value': meta.code_value,
                        'code_label': meta.code_label,
                        'code_type': meta.code_type,
                        'code_version': meta.code_version
                    } for meta in record.code_meta_data
                ],
                'sql_pattern_cnt': record.sql_pattern_cnt,
                'source_file': record.source_file
            }
            self.current_data.append(record_dict)
        
        step_info = {
            'step_name': 'load_raw_dataset',
            'step_type': 'data_loading',
            'timestamp': datetime.now().isoformat(),
            'input_source': str(data_dir),
            'total_records_loaded': len(self.current_data),
            'data_size_mb': sum(len(str(record)) for record in self.current_data) / (1024 * 1024)
        }
        
        self.workflow_steps.append(step_info)
        
        logger.info(f"åŸå§‹æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(self.current_data):,} æ¡è®°å½•")
        return step_info
    
    def run_sql_cleaning(self, step_name: str = "sql_cleaning_step1") -> Dict[str, Any]:
        """
        è¿è¡ŒSQLæ¸…æ´—æ­¥éª¤ï¼ˆæ¸…æ´—å…¨ä½“æ•°æ®ï¼‰
        
        Args:
            step_name: æ­¥éª¤åç§°
            
        Returns:
            æ¸…æ´—ç»“æœä¿¡æ¯
        """
        if self.current_data is None:
            raise ValueError("è¯·å…ˆåŠ è½½æ•°æ®")
        
        logger.info(f"å¼€å§‹å¯¹å…¨ä½“æ•°æ®é›†è¿›è¡ŒSQLæ¸…æ´—: {step_name}")
        
        # åˆ›å»ºSQLæ¸…æ´—å™¨
        cleaner_output_dir = self.workflow_dir / "cleaning_steps"
        sql_cleaner = SQLCleaner(str(cleaner_output_dir))
        
        # æ‰§è¡Œæ¸…æ´—
        cleaning_result = sql_cleaner.clean_dataset(self.current_data, step_name)
        
        # åŠ è½½æ¸…æ´—åçš„æ•°æ®ä½œä¸ºå½“å‰æ•°æ®
        cleaned_data_file = Path(cleaning_result['output_directory']) / "cleaned_records.json"
        with open(cleaned_data_file, 'r', encoding='utf-8') as f:
            self.current_data = json.load(f)
        
        # è®°å½•å·¥ä½œæµæ­¥éª¤
        step_info = {
            'step_name': step_name,
            'step_type': 'sql_cleaning',
            'timestamp': datetime.now().isoformat(),
            'input_records': cleaning_result['input_records_count'],
            'output_records': cleaning_result['output_records_count'],
            'records_modified': cleaning_result['records_modified'],
            'invalid_sql_removed': cleaning_result['invalid_sql_removed'],
            'valid_sql_retained': cleaning_result['valid_sql_retained'],
            'param_dependent_sql_retained': cleaning_result['param_dependent_sql_retained'],
            'empty_sql_lists_found': cleaning_result.get('empty_sql_lists_found', 0),
            'lists_emptied_after_cleaning': cleaning_result.get('lists_emptied_after_cleaning', 0),
            'output_directory': cleaning_result['output_directory']
        }
        
        self.workflow_steps.append(step_info)
        
        logger.info(f"å…¨ä½“æ•°æ®é›†SQLæ¸…æ´—å®Œæˆ - ç§»é™¤äº† {cleaning_result['invalid_sql_removed']:,} ä¸ªæ— æ•ˆSQLï¼Œä¿®æ”¹äº† {cleaning_result['records_modified']:,} æ¡è®°å½•")
        return cleaning_result
    
    async def tag_lack_information_data(self, step_name: str = "sql_completeness_check_step") -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMæ£€æŸ¥æ•°æ®çš„SQLå®Œæ•´æ€§å¹¶æ ‡è®°ç¼ºå°‘ä¿¡æ¯çš„æ•°æ®
        
        Args:
            step_name: æ­¥éª¤åç§°
            
        Returns:
            æ ‡è®°ç»“æœä¿¡æ¯
        """
        if self.current_data is None:
            raise ValueError("è¯·å…ˆåŠ è½½å¹¶æ¸…æ´—æ•°æ®")
        
        logger.info(f"å¼€å§‹ä½¿ç”¨LLMæ£€æŸ¥SQLå®Œæ•´æ€§å¹¶æ ‡è®°æ•°æ®: {step_name}")
        
        # ç­›é€‰å‡ºéœ€è¦å¤„ç†çš„è®°å½•å’Œç›´æ¥è·³è¿‡çš„è®°å½•
        records_to_process = []
        excluded_records = []
        if self.current_data:
            for record in self.current_data:
                # å‡è®¾ sql_statement_list å­˜åœ¨
                if record.get('sql_statement_list') == '<NO SQL GENERATE>':
                    excluded_records.append(record)
                else:
                    records_to_process.append(record)
        
        logger.info(f"ä» {len(self.current_data):,} æ¡è®°å½•ä¸­ç­›é€‰å‡º {len(records_to_process):,} æ¡è®°å½•è¿›è¡Œå®Œæ•´æ€§æ£€æŸ¥ï¼Œæ’é™¤äº† {len(excluded_records):,} æ¡ '<NO SQL GENERATE>' è®°å½•ã€‚")

        # å¦‚æœæ²¡æœ‰éœ€è¦å¤„ç†çš„è®°å½•ï¼Œåˆ™ç›´æ¥è·³è¿‡
        if not records_to_process:
            logger.info("æ²¡æœ‰éœ€è¦å¤„ç†çš„è®°å½•ï¼Œè·³è¿‡LLMå®Œæ•´æ€§æ£€æŸ¥æ­¥éª¤ã€‚")
            step_info = {
                'step_name': step_name,
                'step_type': 'sql_completeness_check',
                'timestamp': datetime.now().isoformat(),
                'input_records': len(self.current_data),
                'records_to_check': 0,
                'excluded_no_sql_records': len(excluded_records),
                'lack_info_records': 0,
                'complete_records': 0,
                'error_records': 0,
                'lack_info_rate': 0.0,
                'concurrent_requests': 0,
                'output_file': None
            }
            self.workflow_steps.append(step_info)
            return step_info

        # åŠ¨æ€å¯¼å…¥LLMç›¸å…³æ¨¡å—
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
        
        try:
            from utils.llm_client import LLMClient
            from config.data_clean.sql_completeness_check_prompt import get_sql_completeness_check_prompt  # type: ignore
        except ImportError as e:
            logger.error(f"æ— æ³•å¯¼å…¥LLMç›¸å…³æ¨¡å—: {e}")
            raise ValueError("LLMæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡ŒSQLå®Œæ•´æ€§æ£€æŸ¥")
        
        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        llm_client = LLMClient("v3")
        
        # å¹¶å‘å¤„ç†çš„å‡½æ•°
        async def check_single_record(session: aiohttp.ClientSession, record: Dict[str, Any]) -> Dict[str, Any]:
            """æ£€æŸ¥å•æ¡è®°å½•çš„SQLå®Œæ•´æ€§"""
            try:
                # å‡†å¤‡æ£€æŸ¥ææ–™
                caller = record.get('caller', '')
                orm_code = record.get('orm_code', '')
                sql_statements = str(record.get('sql_statement_list', []))
                
                # å¤„ç†å…ƒæ•°æ®
                code_meta_data = record.get('code_meta_data', [])
                if isinstance(code_meta_data, list) and code_meta_data:
                    code_meta = str(code_meta_data[0]) if code_meta_data else ''
                else:
                    code_meta = str(code_meta_data)
                
                # ç”Ÿæˆæç¤ºè¯
                prompt = get_sql_completeness_check_prompt(
                    caller=caller,
                    code_meta=code_meta,
                    orm_code=orm_code,
                    sql_statements=sql_statements
                )
                
                # è°ƒç”¨LLM
                response = await llm_client.call_async(session, prompt, max_tokens=100, temperature=0.0)
                
                # å¤„ç†å“åº”
                is_complete = True
                reason = ""
                
                if response:
                    response_lower = response.strip().lower()
                    if response_lower.startswith('å¦'):
                        is_complete = False
                        # æå–åŸå› 
                        if 'ï¼Œ' in response:
                            reason = response.split('ï¼Œ', 1)[1].strip()
                        elif ',' in response:
                            reason = response.split(',', 1)[1].strip()
                        else:
                            reason = response.replace('å¦', '').strip()
                
                # åˆ›å»ºæ–°è®°å½•
                new_record = record.copy()
                
                # æ·»åŠ æ£€æŸ¥ç»“æœ
                if not is_complete:
                    new_record['completeness_check'] = {
                        'is_complete': False,
                        'reason': reason,
                        'tag': '<LACK INFORMATION>',
                        'checked_at': datetime.now().isoformat()
                    }
                    new_record['sql_statement_list'] = "<LACK INFORMATION>"
                else:
                    new_record['completeness_check'] = {
                        'is_complete': True,
                        'reason': '',
                        'tag': '',
                        'checked_at': datetime.now().isoformat()
                    }
                
                return new_record
                
            except Exception as e:
                logger.warning(f"æ£€æŸ¥è®°å½•å¤±è´¥: {e}")
                # å‡ºé”™æ—¶ä¿ç•™åŸè®°å½•å¹¶æ ‡è®°ä¸ºæœªæ£€æŸ¥
                error_record = record.copy()
                error_record['completeness_check'] = {
                    'is_complete': True,  # é»˜è®¤è®¤ä¸ºå®Œæ•´ï¼Œé¿å…é”™è¯¯æ ‡è®°
                    'reason': f'æ£€æŸ¥å¤±è´¥: {str(e)}',
                    'tag': '',
                    'checked_at': datetime.now().isoformat(),
                    'check_error': True
                }
                return error_record
        
        # ä½¿ç”¨100å¹¶å‘å¤„ç†æ‰€æœ‰è®°å½•
        semaphore = asyncio.Semaphore(100)
        
        async def process_with_semaphore(session: aiohttp.ClientSession, record: Dict[str, Any]) -> Dict[str, Any]:
            async with semaphore:
                return await check_single_record(session, record)
        
        # æ‰§è¡Œå¹¶å‘å¤„ç†
        logger.info(f"ä½¿ç”¨ {semaphore._value} å¹¶å‘è¯·æ±‚å¤„ç† {len(records_to_process)} æ¡è®°å½•...")
        
        processed_records = []
        with tqdm_asyncio(total=len(records_to_process), desc=f"æ£€æŸ¥SQLå®Œæ•´æ€§ ({step_name})") as pbar:
            async with aiohttp.ClientSession() as session:
                tasks = []
                for record in records_to_process:
                    task = asyncio.ensure_future(process_with_semaphore(session, record))
                    
                    def update_progress(fut, pbar=pbar):
                        pbar.update(1)
                    
                    task.add_done_callback(update_progress)
                    tasks.append(task)
                
                processed_records = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        tagged_data = []
        error_count = 0
        lack_info_count = 0
        
        for i, result in enumerate(processed_records):
            if isinstance(result, Exception):
                logger.warning(f"å¤„ç†ç¬¬{i+1}æ¡è®°å½•æ—¶å‡ºé”™: {result}")
                error_record = records_to_process[i].copy()
                error_record['completeness_check'] = {
                    'is_complete': True,
                    'reason': f'å¤„ç†å¼‚å¸¸: {str(result)}',
                    'tag': '',
                    'checked_at': datetime.now().isoformat(),
                    'process_error': True
                }
                tagged_data.append(error_record)
                error_count += 1
            else:
                tagged_data.append(result)
                # æ£€æŸ¥completeness_checkå­—æ®µï¼Œç¡®ä¿resultæ˜¯å­—å…¸ç±»å‹
                if isinstance(result, dict) and not result.get('completeness_check', {}).get('is_complete', True):
                    lack_info_count += 1
        
        # æ›´æ–°å½“å‰æ•°æ®
        self.current_data = excluded_records + tagged_data
        
        # ä¿å­˜æ ‡è®°åçš„æ•°æ®
        tagging_output_dir = self.workflow_dir / "sql_completeness_check"
        tagging_output_dir.mkdir(exist_ok=True)
        
        tagged_data_file = tagging_output_dir / f"{step_name}.json"
        with open(tagged_data_file, 'w', encoding='utf-8') as f:
            json.dump(self.current_data, f, ensure_ascii=False, indent=2)
        
        # è®°å½•å·¥ä½œæµæ­¥éª¤
        step_info = {
            'step_name': step_name,
            'step_type': 'sql_completeness_check',
            'timestamp': datetime.now().isoformat(),
            'input_records': len(self.current_data),
            'records_to_check': len(records_to_process),
            'excluded_no_sql_records': len(excluded_records),
            'lack_info_records': lack_info_count,
            'complete_records': len(records_to_process) - lack_info_count - error_count,
            'error_records': error_count,
            'lack_info_rate': lack_info_count / len(records_to_process) * 100 if records_to_process else 0.0,
            'concurrent_requests': 100,
            'output_file': str(tagged_data_file)
        }
        
        self.workflow_steps.append(step_info)
        
        logger.info(f"SQLå®Œæ•´æ€§æ£€æŸ¥å®Œæˆ - åœ¨ {len(records_to_process):,} æ¡å¾…æŸ¥è®°å½•ä¸­ï¼Œæ ‡è®°äº† {lack_info_count:,} æ¡ç¼ºå°‘ä¿¡æ¯çš„è®°å½•ï¼Œ{error_count:,} æ¡å¤„ç†é”™è¯¯ã€‚")
        return step_info

    async def check_sql_correctness(self, step_name: str = "sql_correctness_check_step") -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMæ£€æŸ¥æ•°æ®çš„SQLæ­£ç¡®æ€§å¹¶è¿›è¡Œæ ‡è®°
        
        Args:
            step_name: æ­¥éª¤åç§°
            
        Returns:
            æ ‡è®°ç»“æœä¿¡æ¯
        """
        if self.current_data is None:
            raise ValueError("è¯·å…ˆåŠ è½½å¹¶å¤„ç†æ•°æ®")

        logger.info(f"å¼€å§‹ä½¿ç”¨LLMæ£€æŸ¥SQLæ­£ç¡®æ€§å¹¶æ ‡è®°æ•°æ®: {step_name}")

        # ç­›é€‰å‡ºéœ€è¦è¿›è¡Œæ­£ç¡®æ€§æ£€æŸ¥çš„è®°å½•
        records_to_process = []
        excluded_records = []
        for record in self.current_data:
            is_no_sql = record.get('sql_statement_list') == '<NO SQL GENERATE>'
            has_lack_info_tag = record.get('completeness_check', {}).get('tag') == '<LACK INFORMATION>'
            
            if is_no_sql or has_lack_info_tag:
                excluded_records.append(record)
            else:
                records_to_process.append(record)

        logger.info(f"ä» {len(self.current_data):,} æ¡è®°å½•ä¸­ç­›é€‰å‡º {len(records_to_process):,} æ¡è®°å½•è¿›è¡Œæ­£ç¡®æ€§æ£€æŸ¥ï¼Œæ’é™¤äº† {len(excluded_records):,} æ¡ä¸é€‚ç”¨è®°å½•ã€‚")

        if not records_to_process:
            logger.info("æ²¡æœ‰éœ€è¦è¿›è¡Œæ­£ç¡®æ€§æ£€æŸ¥çš„è®°å½•ï¼Œè·³è¿‡æ­¤æ­¥éª¤ã€‚")
            step_info = {
                'step_name': step_name,
                'step_type': 'sql_correctness_check',
                'timestamp': datetime.now().isoformat(),
                'input_records': len(self.current_data),
                'records_to_check': 0,
                'excluded_records': len(excluded_records),
                'correct_records': 0,
                'incorrect_records': 0,
                'error_records': 0,
                'incorrect_rate': 0.0,
                'output_file': None
            }
            self.workflow_steps.append(step_info)
            return step_info

        # åŠ¨æ€å¯¼å…¥LLMç›¸å…³æ¨¡å—
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
        
        try:
            from utils.llm_client import LLMClient
            from config.data_clean.sql_completeness_check_prompt import get_sql_correctness_check_prompt, get_sql_correctness_check_prompt
        except ImportError as e:
            logger.error(f"æ— æ³•å¯¼å…¥LLMç›¸å…³æ¨¡å—: {e}")
            raise ValueError("LLMæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡ŒSQLæ­£ç¡®æ€§æ£€æŸ¥")
        
        llm_client = LLMClient("v3")

        async def check_single_record(session: aiohttp.ClientSession, record: Dict[str, Any]) -> Dict[str, Any]:
            """æ£€æŸ¥å•æ¡è®°å½•çš„SQLæ­£ç¡®æ€§"""
            try:
                prompt = get_sql_correctness_check_prompt(
                    caller=record.get('caller', ''),
                    code_meta=str(record.get('code_meta_data', [{}])[0]),
                    orm_code=record.get('orm_code', ''),
                    sql_statements=str(record.get('sql_statement_list', []))
                )
                
                response = await llm_client.call_async(session, prompt, max_tokens=100, temperature=0.0)
                
                is_correct = True
                reason = ""
                correction_override = None
                
                if response and response.strip().lower().startswith('å¦'):
                    is_correct = False
                    reason = response.replace('å¦', '').strip(' ï¼Œ,')

                    # æ–°å¢é€»è¾‘ï¼šå¦‚æœç†ç”±æ¶‰åŠç‰¹å®šå…³é”®è¯ï¼Œåˆ™è¦†ç›–ä¸ºæ­£ç¡®
                    if re.search(r'äº‹åŠ¡|è¡¨å', reason):
                        is_correct = True
                        correction_override = f"Keyword match: {reason}"
                
                new_record = record.copy()
                new_record['correctness_check'] = {
                    'is_correct': is_correct,
                    'reason': reason,
                    'tag': '' if is_correct else '<INCORRECT SQL>',
                    'checked_at': datetime.now().isoformat(),
                    'correction_override': correction_override
                }
                return new_record

            except Exception as e:
                logger.warning(f"æ£€æŸ¥è®°å½•æ­£ç¡®æ€§å¤±è´¥: {e}")
                error_record = record.copy()
                error_record['correctness_check'] = {
                    'is_correct': True,  # é»˜è®¤æ­£ç¡®
                    'reason': f'æ£€æŸ¥å¤±è´¥: {str(e)}',
                    'tag': '',
                    'checked_at': datetime.now().isoformat(),
                    'check_error': True
                }
                return error_record

        semaphore = asyncio.Semaphore(100)
        async def process_with_semaphore(session: aiohttp.ClientSession, record: Dict[str, Any]) -> Dict[str, Any]:
            async with semaphore:
                return await check_single_record(session, record)

        processed_records = []
        with tqdm_asyncio(total=len(records_to_process), desc=f"æ£€æŸ¥SQLæ­£ç¡®æ€§ ({step_name})") as pbar:
            async with aiohttp.ClientSession() as session:
                tasks = [asyncio.ensure_future(process_with_semaphore(session, r)) for r in records_to_process]
                for task in tasks:
                    task.add_done_callback(lambda p: pbar.update(1))
                processed_records = await asyncio.gather(*tasks, return_exceptions=True)

        final_data = []
        error_count = 0
        incorrect_count = 0
        override_count = 0
        for i, result in enumerate(processed_records):
            if isinstance(result, Exception):
                error_count += 1
                error_record = records_to_process[i].copy()
                error_record['correctness_check'] = {
                    'is_correct': True,  # é»˜è®¤æ­£ç¡®
                    'reason': f'å¤„ç†å¼‚å¸¸: {str(result)}',
                    'tag': '',
                    'checked_at': datetime.now().isoformat(),
                    'process_error': True
                }
                final_data.append(error_record)
            elif isinstance(result, dict):
                final_data.append(result)
                correctness_info = result.get('correctness_check', {})
                if not correctness_info.get('is_correct', True):
                    incorrect_count += 1
                if correctness_info.get('correction_override'):
                    override_count += 1
            else:
                # å¤„ç†å…¶ä»–æ„å¤–æƒ…å†µ
                error_count += 1
                error_record = records_to_process[i].copy()
                error_record['correctness_check'] = {
                    'is_correct': True,
                    'reason': f'æœªçŸ¥å¤„ç†ç»“æœç±»å‹: {type(result)}',
                    'tag': '',
                    'checked_at': datetime.now().isoformat(),
                    'process_error': True
                }
                final_data.append(error_record)

        self.current_data = excluded_records + final_data
        
        output_dir = self.workflow_dir / "sql_correctness_check"
        output_dir.mkdir(exist_ok=True)
        output_file = output_dir / f"{step_name}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.current_data, f, ensure_ascii=False, indent=2)
            
        step_info = {
            'step_name': step_name,
            'step_type': 'sql_correctness_check',
            'timestamp': datetime.now().isoformat(),
            'input_records': len(self.current_data),
            'records_to_check': len(records_to_process),
            'excluded_records': len(excluded_records),
            'correct_records': len(records_to_process) - incorrect_count - error_count,
            'incorrect_records': incorrect_count,
            'error_records': error_count,
            'overridden_as_correct': override_count,
            'incorrect_rate': incorrect_count / len(records_to_process) * 100 if records_to_process else 0.0,
            'output_file': str(output_file)
        }
        
        self.workflow_steps.append(step_info)
        logger.info(f"SQLæ­£ç¡®æ€§æ£€æŸ¥å®Œæˆ - åœ¨ {len(records_to_process):,} æ¡è®°å½•ä¸­ï¼Œå‘ç° {incorrect_count:,} æ¡ä¸æ­£ç¡®ï¼Œ{override_count:,} æ¡å› å…³é”®è¯è¢«è¦†ç›–ä¸ºæ­£ç¡®ï¼Œ{error_count:,} æ¡å¤„ç†é”™è¯¯ã€‚")
        return step_info

    def extract_keyword_data(self, keywords: Optional[List[str]] = None, step_name: str = "keyword_extraction_step2") -> Dict[str, Any]:
        """
        ä»æ¸…æ´—åçš„æ•°æ®ä¸­æå–å…³é”®è¯æ•°æ®
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨GORMå…³é”®è¯
            step_name: æ­¥éª¤åç§°
            
        Returns:
            æå–ç»“æœä¿¡æ¯
        """
        if self.current_data is None:
            raise ValueError("è¯·å…ˆåŠ è½½å¹¶æ¸…æ´—æ•°æ®")
        
        logger.info(f"å¼€å§‹ä»æ¸…æ´—åçš„æ•°æ®ä¸­æå–å…³é”®è¯: {step_name}")
        
        # åˆ›å»ºä¸´æ—¶çš„DataReaderæ¥ä½¿ç”¨å…¶æå–åŠŸèƒ½
        try:
            from ..data_reader import FunctionRecord, CodeMetaData
        except ImportError:
            from data_reader import FunctionRecord, CodeMetaData
        
        # è½¬æ¢å›FunctionRecordæ ¼å¼
        temp_records = []
        for record_dict in self.current_data:
            code_meta_data = [
                CodeMetaData(
                    code_file=meta['code_file'],
                    code_start_line=meta['code_start_line'],
                    code_end_line=meta['code_end_line'],
                    code_key=meta['code_key'],
                    code_value=meta['code_value'],
                    code_label=meta['code_label'],
                    code_type=meta['code_type'],
                    code_version=meta['code_version']
                ) for meta in record_dict['code_meta_data']
            ]
            
            record = FunctionRecord(
                function_name=record_dict['function_name'],
                orm_code=record_dict['orm_code'],
                caller=record_dict['caller'],
                sql_statement_list=record_dict['sql_statement_list'],
                sql_types=record_dict['sql_types'],
                code_meta_data=code_meta_data,
                sql_pattern_cnt=record_dict['sql_pattern_cnt'],
                source_file=record_dict['source_file']
            )
            temp_records.append(record)
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        import tempfile
        import os
        
        with tempfile.TemporaryDirectory() as temp_dir:
            # åˆ›å»ºä¸´æ—¶DataReaderå¹¶è®¾ç½®æ•°æ®ï¼ˆä¸ä¾èµ–å®é™…æ–‡ä»¶ï¼‰
            temp_reader = DataReader(temp_dir)
            temp_reader.records = temp_records
            
            # æ‰§è¡Œå…³é”®è¯æå–
            extraction_output_dir = self.workflow_dir / "keyword_extraction"
            if keywords is None:
                extract_result = temp_reader.extract_gorm_keywords(str(extraction_output_dir))
            else:
                extract_result = temp_reader.extract_by_keywords(
                    keywords=keywords,
                    output_dir=str(extraction_output_dir),
                    step_name=step_name
                )
        
        # åŠ è½½æå–çš„æ•°æ®
        extracted_data_file = Path(extract_result['output_directory']) / "keyword_matched_records.json"
        with open(extracted_data_file, 'r', encoding='utf-8') as f:
            self.extracted_data = json.load(f)
        
        step_info = {
            'step_name': step_name,
            'step_type': 'keyword_extraction',
            'timestamp': datetime.now().isoformat(),
            'input_records': len(self.current_data),
            'extracted_records': len(self.extracted_data),
            'extraction_rate': len(self.extracted_data) / len(self.current_data) * 100,
            'keywords_used': keywords or "GORMé¢„å®šä¹‰å…³é”®è¯",
            'output_directory': extract_result['output_directory']
        }
        
        self.workflow_steps.append(step_info)
        
        logger.info(f"å…³é”®è¯æå–å®Œæˆ - ä» {len(self.current_data):,} æ¡è®°å½•ä¸­æå–äº† {len(self.extracted_data):,} æ¡åŒ¹é…è®°å½•")
        return step_info
    
    def process_extracted_data(self, step_name: str = "special_processing_step3") -> Dict[str, Any]:
        """
        å¯¹æå–çš„æ•°æ®è¿›è¡Œç‰¹æ®Šå¤„ç†
        
        Args:
            step_name: æ­¥éª¤åç§°
            
        Returns:
            å¤„ç†ç»“æœä¿¡æ¯
        """
        if self.extracted_data is None:
            raise ValueError("è¯·å…ˆæå–å…³é”®è¯æ•°æ®")
        
        logger.info(f"å¼€å§‹å¯¹æå–çš„æ•°æ®è¿›è¡Œç‰¹æ®Šå¤„ç†: {step_name}")
        
        # TODO: è¿™é‡Œé¢„ç•™ç‰¹æ®Šå¤„ç†é€»è¾‘çš„æ¥å£
        # å½“å‰åªæ˜¯ç®€å•å¤åˆ¶ï¼Œåç»­å¯ä»¥æ·»åŠ æ•°æ®å¢å¼ºã€æ ‡æ³¨ç­‰å¤„ç†
        processed_data = []
        for record in self.extracted_data:
            # å¤åˆ¶åŸè®°å½•
            processed_record = record.copy()
            
            # TODO: åœ¨è¿™é‡Œæ·»åŠ ç‰¹æ®Šå¤„ç†é€»è¾‘
            # ä¾‹å¦‚ï¼š
            # - æ•°æ®å¢å¼º
            # - è‡ªåŠ¨æ ‡æ³¨
            # - æ ¼å¼è½¬æ¢
            # - è´¨é‡è¯„ä¼°
            
            # æ·»åŠ å¤„ç†æ ‡è®°
            processed_record['processing_metadata'] = {
                'processed_at': datetime.now().isoformat(),
                'processing_step': step_name,
                'processing_applied': []  # åç»­å¯ä»¥è®°å½•åº”ç”¨çš„å¤„ç†æ–¹æ³•
            }
            
            processed_data.append(processed_record)
        
        self.extracted_data = processed_data
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        processing_output_dir = self.workflow_dir / "special_processing"
        processing_output_dir.mkdir(exist_ok=True)
        
        processed_data_file = processing_output_dir / f"{step_name}.json"
        with open(processed_data_file, 'w', encoding='utf-8') as f:
            json.dump(self.extracted_data, f, ensure_ascii=False, indent=2)
        
        step_info = {
            'step_name': step_name,
            'step_type': 'special_processing',
            'timestamp': datetime.now().isoformat(),
            'input_records': len(self.extracted_data),
            'output_records': len(self.extracted_data),
            'processing_applied': [],  # ç›®å‰ä¸ºç©ºï¼Œåç»­å¯ä»¥è®°å½•å…·ä½“å¤„ç†
            'output_file': str(processed_data_file)
        }
        
        self.workflow_steps.append(step_info)
        
        logger.info(f"ç‰¹æ®Šå¤„ç†å®Œæˆ - å¤„ç†äº† {len(self.extracted_data):,} æ¡æå–çš„è®°å½•")
        return step_info
    
    def merge_processed_data_back(self, step_name: str = "merge_back_step4") -> Dict[str, Any]:
        """
        å°†å¤„ç†åçš„æ•°æ®åˆå¹¶å›åŸæ•°æ®é›†
        
        Args:
            step_name: æ­¥éª¤åç§°
            
        Returns:
            åˆå¹¶ç»“æœä¿¡æ¯
        """
        if self.extracted_data is None or self.current_data is None:
            raise ValueError("è¯·å…ˆå®Œæˆæ•°æ®æå–å’Œç‰¹æ®Šå¤„ç†")
        
        logger.info(f"å¼€å§‹å°†å¤„ç†åçš„æ•°æ®åˆå¹¶å›åŸæ•°æ®é›†: {step_name}")
        
        # åˆ›å»ºfunction_nameåˆ°è®°å½•çš„æ˜ å°„ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾
        extracted_data_map = {record['function_name']: record for record in self.extracted_data}
        
        # åˆå¹¶æ•°æ®
        merged_data = []
        updated_count = 0
        
        for original_record in self.current_data:
            function_name = original_record['function_name']
            
            if function_name in extracted_data_map:
                # å¦‚æœåœ¨æå–æ•°æ®ä¸­æ‰¾åˆ°å¯¹åº”è®°å½•ï¼Œä½¿ç”¨å¤„ç†åçš„ç‰ˆæœ¬
                processed_record = extracted_data_map[function_name].copy()
                
                # ä¿ç•™åŸå§‹è®°å½•ä¸­å¯èƒ½ä¸åœ¨æå–æ•°æ®ä¸­çš„å­—æ®µ
                for key, value in original_record.items():
                    if key not in processed_record:
                        processed_record[key] = value
                
                # æ·»åŠ åˆå¹¶æ ‡è®°
                if 'processing_metadata' not in processed_record:
                    processed_record['processing_metadata'] = {}
                processed_record['processing_metadata']['merged_back'] = True
                processed_record['processing_metadata']['merge_timestamp'] = datetime.now().isoformat()
                
                merged_data.append(processed_record)
                updated_count += 1
            else:
                # å¦‚æœä¸åœ¨æå–æ•°æ®ä¸­ï¼Œä¿ç•™åŸå§‹è®°å½•
                merged_data.append(original_record)
        
        self.current_data = merged_data
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        merge_output_dir = self.workflow_dir / "merged_data"
        merge_output_dir.mkdir(exist_ok=True)
        
        merged_data_file = merge_output_dir / f"{step_name}.json"
        with open(merged_data_file, 'w', encoding='utf-8') as f:
            json.dump(self.current_data, f, ensure_ascii=False, indent=2)
        
        step_info = {
            'step_name': step_name,
            'step_type': 'data_merging',
            'timestamp': datetime.now().isoformat(),
            'total_records': len(self.current_data),
            'updated_records': updated_count,
            'unchanged_records': len(self.current_data) - updated_count,
            'update_rate': updated_count / len(self.current_data) * 100,
            'output_file': str(merged_data_file)
        }
        
        self.workflow_steps.append(step_info)
        
        logger.info(f"æ•°æ®åˆå¹¶å®Œæˆ - æ›´æ–°äº† {updated_count:,} æ¡è®°å½•ï¼Œä¿æŒäº† {len(self.current_data) - updated_count:,} æ¡åŸå§‹è®°å½•")
        return step_info
    
    def save_workflow_summary(self) -> str:
        """
        ä¿å­˜å·¥ä½œæµæ‘˜è¦
        
        Returns:
            æ‘˜è¦æ–‡ä»¶è·¯å¾„
        """
        summary = {
            'workflow_id': f"workflow_{self.workflow_timestamp}",
            'start_time': self.workflow_steps[0]['timestamp'] if self.workflow_steps else None,
            'end_time': datetime.now().isoformat(),
            'total_steps': len(self.workflow_steps),
            'steps': self.workflow_steps,
            'final_data_count': len(self.current_data) if self.current_data else 0,
            'extracted_data_count': len(self.extracted_data) if self.extracted_data else 0,
            'workflow_directory': str(self.workflow_dir)
        }
        
        summary_file = self.workflow_dir / "workflow_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å·¥ä½œæµæ‘˜è¦å·²ä¿å­˜: {summary_file}")
        return str(summary_file)
    
    def export_final_data(self, output_file: str = "final_processed_data.json") -> str:
        """
        å¯¼å‡ºæœ€ç»ˆå¤„ç†åçš„æ•°æ®
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶å
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not self.current_data:
            raise ValueError("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
        
        export_path = self.workflow_dir / output_file
        with open(export_path, 'w', encoding='utf-8') as f:
            json.dump(self.current_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æœ€ç»ˆæ•°æ®å·²å¯¼å‡º: {export_path}")
        return str(export_path)
    
    def print_workflow_summary(self):
        """æ‰“å°å·¥ä½œæµæ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ”„ æ•°æ®å¤„ç†å·¥ä½œæµæ‘˜è¦")
        print("=" * 60)
        
        print(f"ğŸ“ å·¥ä½œæµç›®å½•: {self.workflow_dir}")
        print(f"â° å·¥ä½œæµID: workflow_{self.workflow_timestamp}")
        print(f"ğŸ“Š æ€»æ­¥éª¤æ•°: {len(self.workflow_steps)}")
        print(f"ğŸ“‹ æœ€ç»ˆæ•°æ®é‡: {len(self.current_data) if self.current_data else 0} æ¡è®°å½•")
        print(f"ğŸ¯ æå–æ•°æ®é‡: {len(self.extracted_data) if self.extracted_data else 0} æ¡è®°å½•")
        
        print(f"\nğŸ” å¤„ç†æ­¥éª¤è¯¦æƒ…:")
        for i, step in enumerate(self.workflow_steps, 1):
            print(f"  {i}. {step['step_name']} ({step['step_type']})")
            
            if step['step_type'] == 'data_loading':
                print(f"     ğŸ“¥ åŠ è½½è®°å½•: {step['total_records_loaded']:,}")
                print(f"     ğŸ’¾ æ•°æ®å¤§å°: {step['data_size_mb']:.2f} MB")
                
            elif step['step_type'] == 'sql_cleaning':
                print(f"     ğŸ“Š è¾“å…¥è®°å½•: {step['input_records']:,}")
                print(f"     ğŸ“Š è¾“å‡ºè®°å½•: {step['output_records']:,}")
                print(f"     ğŸ—‘ï¸ ç§»é™¤æ— æ•ˆSQL: {step['invalid_sql_removed']:,}")
                print(f"     âœï¸ ä¿®æ”¹è®°å½•: {step['records_modified']:,}")
                print(f"     âœ… ä¿ç•™æœ‰æ•ˆSQL: {step['valid_sql_retained']:,}")
                if 'empty_sql_lists_found' in step:
                    print(f"     ğŸ“‹ åŸå§‹ç©ºåˆ—è¡¨: {step['empty_sql_lists_found']:,}")
                if 'lists_emptied_after_cleaning' in step:
                    print(f"     ğŸ§¹ æ¸…æ´—åç©ºåˆ—è¡¨: {step['lists_emptied_after_cleaning']:,}")
                
            elif step['step_type'] == 'sql_completeness_check':
                print(f"     ğŸ“Š è¾“å…¥è®°å½•: {step['input_records']:,}")
                print(f"     ğŸ·ï¸ æ ‡è®°ç¼ºå°‘ä¿¡æ¯: {step['lack_info_records']:,}")
                print(f"     âœ… å®Œæ•´è®°å½•: {step['complete_records']:,}")
                print(f"     âŒ å¤„ç†é”™è¯¯: {step['error_records']:,}")
                print(f"     ğŸ“ˆ ç¼ºå°‘ä¿¡æ¯ç‡: {step['lack_info_rate']:.2f}%")
                print(f"     ğŸ”„ å¹¶å‘è¯·æ±‚æ•°: {step['concurrent_requests']}")
                
            elif step['step_type'] == 'sql_correctness_check':
                print(f"     ğŸ“Š è¾“å…¥è®°å½•: {step['records_to_check']:,} (ä» {step['input_records']:,} ä¸­ç­›é€‰)")
                overridden_count = step.get('overridden_as_correct', 0)
                if overridden_count > 0:
                    print(f"     âœ… æ­£ç¡®è®°å½•: {step['correct_records']:,} (å…¶ä¸­ {overridden_count:,} æ¡ä¸ºå…³é”®è¯è¦†ç›–)")
                else:
                    print(f"     âœ… æ­£ç¡®è®°å½•: {step['correct_records']:,}")
                print(f"     âŒ é”™è¯¯è®°å½•: {step['incorrect_records']:,}")
                print(f"     ğŸ”¥ å¤„ç†å¼‚å¸¸: {step['error_records']:,}")
                print(f"     ğŸ“ˆ é”™è¯¯ç‡: {step['incorrect_rate']:.2f}%")
                
            elif step['step_type'] == 'keyword_extraction':
                print(f"     ğŸ“Š è¾“å…¥è®°å½•: {step['input_records']:,}")
                print(f"     ğŸ¯ æå–è®°å½•: {step['extracted_records']:,}")
                print(f"     ğŸ“ˆ æå–ç‡: {step['extraction_rate']:.2f}%")
                
            elif step['step_type'] == 'special_processing':
                print(f"     ğŸ”§ å¤„ç†è®°å½•: {step['input_records']:,}")
                print(f"     ğŸ“¤ è¾“å‡ºè®°å½•: {step['output_records']:,}")
                
            elif step['step_type'] == 'data_merging':
                print(f"     ğŸ“Š æ€»è®°å½•æ•°: {step['total_records']:,}")
                print(f"     ğŸ”„ æ›´æ–°è®°å½•: {step['updated_records']:,}")
                print(f"     ğŸ“ˆ æ›´æ–°ç‡: {step['update_rate']:.2f}%")
        
        print(f"\nğŸ’¾ è¾“å‡ºæ–‡ä»¶:")
        for step in self.workflow_steps:
            if 'output_directory' in step and step['output_directory']:
                print(f"   ğŸ“ {step['step_name']}: {step['output_directory']}")
            elif 'output_file' in step and step['output_file']:
                print(f"   ğŸ“„ {step['step_name']}: {step['output_file']}")


def run_complete_workflow_from_raw_data(data_dir: str, keywords: Optional[List[str]] = None, base_output_dir: str = "workflow_output") -> Dict[str, Any]:
    """
    è¿è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†å·¥ä½œæµï¼ˆæ–°æ¶æ„ï¼šæ¸…æ´— -> æ ‡ç­¾ -> æå– -> å¤„ç† -> åˆå¹¶ï¼‰
    
    Args:
        data_dir: åŸå§‹æ•°æ®ç›®å½•
        keywords: å…³é”®è¯åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨GORMå…³é”®è¯
        base_output_dir: è¾“å‡ºåŸºç›®å½•
        
    Returns:
        å·¥ä½œæµç»“æœä¿¡æ¯
    """
    logger.info("å¼€å§‹æ–°æ¶æ„çš„å®Œæ•´æ•°æ®å¤„ç†å·¥ä½œæµ")
    
    # åˆ›å»ºå·¥ä½œæµç®¡ç†å™¨
    workflow = WorkflowManager(base_output_dir)
    
    try:
        # æ­¥éª¤1: åŠ è½½åŸå§‹æ•°æ®é›†
        load_result = workflow.load_raw_dataset(data_dir)
        
        # æ­¥éª¤2: å¯¹å…¨ä½“æ•°æ®è¿›è¡ŒSQLæ¸…æ´—
        cleaning_result = workflow.run_sql_cleaning("sql_cleaning_step1")
        
        # æ­¥éª¤2.5: ä½¿ç”¨LLMæ£€æŸ¥SQLå®Œæ•´æ€§å¹¶æ ‡è®°ç¼ºå°‘ä¿¡æ¯çš„æ•°æ®
        logger.info("å¼€å§‹æ‰§è¡ŒSQLå®Œæ•´æ€§æ£€æŸ¥å’Œæ•°æ®æ ‡è®°...")
        tagging_result = asyncio.run(workflow.tag_lack_information_data("sql_completeness_check_step2"))
        
        # æ­¥éª¤2.6: ä½¿ç”¨LLMæ£€æŸ¥SQLæ­£ç¡®æ€§
        logger.info("å¼€å§‹æ‰§è¡ŒSQLæ­£ç¡®æ€§æ£€æŸ¥...")
        correctness_result = asyncio.run(workflow.check_sql_correctness("sql_correctness_check_step2.6"))

        # æ­¥éª¤3: ä»æ¸…æ´—åçš„æ•°æ®ä¸­æå–å…³é”®è¯æ•°æ®
        extraction_result = workflow.extract_keyword_data(keywords, "keyword_extraction_step3")
        
        # æ­¥éª¤4: å¯¹æå–çš„æ•°æ®è¿›è¡Œç‰¹æ®Šå¤„ç†
        processing_result = workflow.process_extracted_data("special_processing_step4")
        
        # æ­¥éª¤5: å°†å¤„ç†åçš„æ•°æ®åˆå¹¶å›åŸæ•°æ®é›†
        merge_result = workflow.merge_processed_data_back("merge_back_step5")
        
        # å¯¼å‡ºæœ€ç»ˆæ•°æ®
        final_data_path = workflow.export_final_data("final_processed_dataset.json")
        
        # ä¿å­˜å·¥ä½œæµæ‘˜è¦
        summary_path = workflow.save_workflow_summary()
        
        # æ‰“å°æ‘˜è¦
        workflow.print_workflow_summary()
        
        result = {
            'workflow_completed': True,
            'workflow_directory': str(workflow.workflow_dir),
            'final_data_path': final_data_path,
            'summary_path': summary_path,
            'load_result': load_result,
            'cleaning_result': cleaning_result,
            'tagging_result': tagging_result,
            'correctness_result': correctness_result,
            'extraction_result': extraction_result,
            'processing_result': processing_result,
            'merge_result': merge_result
        }
        
        logger.info("æ–°æ¶æ„çš„å®Œæ•´æ•°æ®å¤„ç†å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
        return result
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        raise


# ä¿ç•™æ—§çš„å‡½æ•°ä»¥å…¼å®¹ç°æœ‰ä»£ç 
def run_complete_sql_cleaning_workflow(extracted_data_path: str, base_output_dir: str = "workflow_output") -> Dict[str, Any]:
    """
    è¿è¡ŒSQLæ¸…æ´—å·¥ä½œæµï¼ˆä»å·²æå–æ•°æ®å¼€å§‹ï¼‰- ä¿ç•™å…¼å®¹æ€§
    """
    logger.warning("ä½¿ç”¨æ—§ç‰ˆworkflowï¼Œå»ºè®®ä½¿ç”¨ run_complete_workflow_from_raw_data")
    
    workflow = WorkflowManager(base_output_dir)
    
    try:
        # åŠ è½½å·²æå–çš„æ•°æ®
        with open(extracted_data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        workflow.current_data = data
        
        # SQLæ¸…æ´—
        cleaning_result = workflow.run_sql_cleaning("sql_cleaning_step1")
        
        # å¯¼å‡ºæœ€ç»ˆæ•°æ®
        final_data_path = workflow.export_final_data()
        
        # ä¿å­˜å·¥ä½œæµæ‘˜è¦
        summary_path = workflow.save_workflow_summary()
        
        workflow.print_workflow_summary()
        
        return {
            'workflow_completed': True,
            'workflow_directory': str(workflow.workflow_dir),
            'final_data_path': final_data_path,
            'summary_path': summary_path,
            'cleaning_result': cleaning_result
        }
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        raise 