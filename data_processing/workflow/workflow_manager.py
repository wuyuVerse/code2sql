"""
Workflowç®¡ç†å™¨

ç®¡ç†æ•°æ®å¤„ç†çš„æ•´ä¸ªå·¥ä½œæµï¼ŒåŒ…æ‹¬æ•°æ®è¯»å–ã€æ¸…æ´—ã€éªŒè¯ç­‰æ­¥éª¤
"""

import json
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime

from ..data_reader import DataReader
from ..cleaning.sql_cleaner import SQLCleaner

logger = logging.getLogger(__name__)


class WorkflowManager:
    """å·¥ä½œæµç®¡ç†å™¨
    
    è´Ÿè´£åè°ƒæ•°æ®å¤„ç†çš„å„ä¸ªæ­¥éª¤ï¼Œè®°å½•å¤„ç†è¿‡ç¨‹å’Œç»“æœ
    """
    
    def __init__(self, base_output_dir: str = "workflow_output"):
        """
        åˆå§‹åŒ–å·¥ä½œæµç®¡ç†å™¨
        
        Args:
            base_output_dir: å·¥ä½œæµè¾“å‡ºåŸºç›®å½•
        """
        self.base_output_dir = Path(base_output_dir)
        self.base_output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå½“å‰workflowå®ä¾‹çš„ç›®å½•
        self.workflow_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.workflow_dir = self.base_output_dir / f"workflow_{self.workflow_timestamp}"
        self.workflow_dir.mkdir(exist_ok=True)
        
        # å·¥ä½œæµæ­¥éª¤è®°å½•
        self.workflow_steps = []
        self.current_data = None
        
        logger.info(f"å·¥ä½œæµç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.workflow_dir}")
    
    def load_raw_dataset(self, data_dir: str, keywords: List[str] = None) -> Dict[str, Any]:
        """
        ä»åŸå§‹æ•°æ®é›†å¼€å§‹åŠ è½½å’Œæå–æ•°æ®
        
        Args:
            data_dir: åŸå§‹æ•°æ®ç›®å½•
            keywords: å…³é”®è¯åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨GORMå…³é”®è¯
            
        Returns:
            åŠ è½½å’Œæå–ç»“æœä¿¡æ¯
        """
        logger.info(f"å¼€å§‹ä»åŸå§‹æ•°æ®é›†åŠ è½½æ•°æ®: {data_dir}")
        
        # åˆ›å»ºæ•°æ®è¯»å–å™¨
        reader = DataReader(data_dir)
        
        # æ‰§è¡Œå…³é”®è¯æå–
        if keywords is None:
            # ä½¿ç”¨GORMå…³é”®è¯
            extraction_output_dir = self.workflow_dir / "keyword_extraction"
            extract_result = reader.extract_gorm_keywords(str(extraction_output_dir))
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰å…³é”®è¯
            extraction_output_dir = self.workflow_dir / "keyword_extraction"
            extract_result = reader.extract_by_keywords(
                keywords=keywords,
                output_dir=str(extraction_output_dir),
                step_name="custom_keyword_extraction"
            )
        
        # åŠ è½½æå–çš„æ•°æ®
        extracted_data_file = Path(extract_result['output_directory']) / "keyword_matched_records.json"
        with open(extracted_data_file, 'r', encoding='utf-8') as f:
            self.current_data = json.load(f)
        
        step_info = {
            'step_name': 'load_raw_dataset_and_extract',
            'step_type': 'data_loading_and_extraction',
            'timestamp': datetime.now().isoformat(),
            'input_source': str(data_dir),
            'total_raw_records': extract_result['total_records_processed'],
            'extracted_records': extract_result['matched_records'],
            'extraction_rate': extract_result['match_rate'],
            'keywords_used': keywords or "GORMé¢„å®šä¹‰å…³é”®è¯",
            'output_directory': extract_result['output_directory']
        }
        
        self.workflow_steps.append(step_info)
        
        logger.info(f"æ•°æ®åŠ è½½å’Œæå–å®Œæˆï¼Œä» {extract_result['total_records_processed']:,} æ¡åŸå§‹è®°å½•ä¸­æå–äº† {len(self.current_data):,} æ¡åŒ¹é…è®°å½•")
        return step_info
    
    def load_extracted_data(self, extracted_data_path: str) -> Dict[str, Any]:
        """
        åŠ è½½å·²æå–çš„æ•°æ®
        
        Args:
            extracted_data_path: æå–æ•°æ®çš„è·¯å¾„
            
        Returns:
            åŠ è½½ç»“æœä¿¡æ¯
        """
        logger.info(f"å¼€å§‹åŠ è½½æå–çš„æ•°æ®: {extracted_data_path}")
        
        data_path = Path(extracted_data_path)
        
        # æŸ¥æ‰¾å…³é”®è¯åŒ¹é…è®°å½•æ–‡ä»¶
        if data_path.is_dir():
            keyword_file = data_path / "keyword_matched_records.json"
            if not keyword_file.exists():
                raise FileNotFoundError(f"åœ¨ç›®å½• {data_path} ä¸­æœªæ‰¾åˆ° keyword_matched_records.json")
        else:
            keyword_file = data_path
        
        # åŠ è½½æ•°æ®
        with open(keyword_file, 'r', encoding='utf-8') as f:
            self.current_data = json.load(f)
        
        step_info = {
            'step_name': 'load_extracted_data',
            'step_type': 'data_loading',
            'timestamp': datetime.now().isoformat(),
            'input_source': str(keyword_file),
            'records_loaded': len(self.current_data),
            'output_file': None
        }
        
        self.workflow_steps.append(step_info)
        
        logger.info(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(self.current_data)} æ¡è®°å½•")
        return step_info
    
    def run_sql_cleaning(self, step_name: str = "sql_cleaning") -> Dict[str, Any]:
        """
        è¿è¡ŒSQLæ¸…æ´—æ­¥éª¤
        
        Args:
            step_name: æ­¥éª¤åç§°
            
        Returns:
            æ¸…æ´—ç»“æœä¿¡æ¯
        """
        if self.current_data is None:
            raise ValueError("è¯·å…ˆåŠ è½½æ•°æ®")
        
        logger.info(f"å¼€å§‹SQLæ¸…æ´—æ­¥éª¤: {step_name}")
        
        # åˆ›å»ºSQLæ¸…æ´—å™¨
        cleaner_output_dir = self.workflow_dir / "cleaning_steps"
        sql_cleaner = SQLCleaner(str(cleaner_output_dir))
        
        # æ‰§è¡Œæ¸…æ´—
        cleaning_result = sql_cleaner.clean_dataset(self.current_data, step_name)
        
        # åŠ è½½æ¸…æ´—åçš„æ•°æ®ä½œä¸ºå½“å‰æ•°æ®
        cleaned_data_file = Path(cleaning_result['output_directory']) / "cleaned_records.json"
        with open(cleaned_data_file, 'r', encoding='utf-8') as f:
            self.current_data = json.load(f)
        
        # è®°å½•å·¥ä½œæµæ­¥éª¤
        step_info = {
            'step_name': step_name,
            'step_type': 'sql_cleaning',
            'timestamp': datetime.now().isoformat(),
            'input_records': cleaning_result['input_records_count'],
            'output_records': cleaning_result['output_records_count'],
            'records_modified': cleaning_result['records_modified'],
            'invalid_sql_removed': cleaning_result['invalid_sql_removed'],
            'valid_sql_retained': cleaning_result['valid_sql_retained'],
            'param_dependent_sql_retained': cleaning_result['param_dependent_sql_retained'],
            'output_directory': cleaning_result['output_directory']
        }
        
        self.workflow_steps.append(step_info)
        
        logger.info(f"SQLæ¸…æ´—å®Œæˆ - ç§»é™¤äº† {cleaning_result['invalid_sql_removed']} ä¸ªæ— æ•ˆSQLï¼Œä¿®æ”¹äº† {cleaning_result['records_modified']} æ¡è®°å½•")
        return cleaning_result
    
    def save_workflow_summary(self) -> str:
        """
        ä¿å­˜å·¥ä½œæµæ‘˜è¦
        
        Returns:
            æ‘˜è¦æ–‡ä»¶è·¯å¾„
        """
        summary = {
            'workflow_id': f"workflow_{self.workflow_timestamp}",
            'start_time': self.workflow_steps[0]['timestamp'] if self.workflow_steps else None,
            'end_time': datetime.now().isoformat(),
            'total_steps': len(self.workflow_steps),
            'steps': self.workflow_steps,
            'final_data_count': len(self.current_data) if self.current_data else 0,
            'workflow_directory': str(self.workflow_dir)
        }
        
        summary_file = self.workflow_dir / "workflow_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å·¥ä½œæµæ‘˜è¦å·²ä¿å­˜: {summary_file}")
        return str(summary_file)
    
    def get_current_data_sample(self, sample_size: int = 3) -> List[Dict[str, Any]]:
        """
        è·å–å½“å‰æ•°æ®çš„æ ·æœ¬
        
        Args:
            sample_size: æ ·æœ¬å¤§å°
            
        Returns:
            æ•°æ®æ ·æœ¬
        """
        if not self.current_data:
            return []
        
        return self.current_data[:sample_size]
    
    def export_final_data(self, output_file: str = "final_cleaned_data.json") -> str:
        """
        å¯¼å‡ºæœ€ç»ˆæ¸…æ´—åçš„æ•°æ®
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶å
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not self.current_data:
            raise ValueError("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
        
        export_path = self.workflow_dir / output_file
        with open(export_path, 'w', encoding='utf-8') as f:
            json.dump(self.current_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æœ€ç»ˆæ•°æ®å·²å¯¼å‡º: {export_path}")
        return str(export_path)
    
    def print_workflow_summary(self):
        """æ‰“å°å·¥ä½œæµæ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ”„ æ•°æ®æ¸…æ´—å·¥ä½œæµæ‘˜è¦")
        print("=" * 60)
        
        print(f"ğŸ“ å·¥ä½œæµç›®å½•: {self.workflow_dir}")
        print(f"â° å·¥ä½œæµID: workflow_{self.workflow_timestamp}")
        print(f"ğŸ“Š æ€»æ­¥éª¤æ•°: {len(self.workflow_steps)}")
        print(f"ğŸ“‹ æœ€ç»ˆæ•°æ®é‡: {len(self.current_data) if self.current_data else 0} æ¡è®°å½•")
        
        print(f"\nğŸ” å¤„ç†æ­¥éª¤è¯¦æƒ…:")
        for i, step in enumerate(self.workflow_steps, 1):
            print(f"  {i}. {step['step_name']} ({step['step_type']})")
            if step['step_type'] == 'data_loading':
                print(f"     ğŸ“¥ åŠ è½½è®°å½•: {step['records_loaded']:,}")
            elif step['step_type'] == 'data_loading_and_extraction':
                print(f"     ğŸ“¥ åŸå§‹è®°å½•: {step['total_raw_records']:,}")
                print(f"     ğŸ¯ æå–è®°å½•: {step['extracted_records']:,}")
                print(f"     ğŸ“ˆ æå–ç‡: {step['extraction_rate']:.2f}%")
            elif step['step_type'] == 'sql_cleaning':
                print(f"     ğŸ“Š è¾“å…¥è®°å½•: {step['input_records']:,}")
                print(f"     ğŸ“Š è¾“å‡ºè®°å½•: {step['output_records']:,}")
                print(f"     ğŸ—‘ï¸ ç§»é™¤æ— æ•ˆSQL: {step['invalid_sql_removed']:,}")
                print(f"     âœï¸ ä¿®æ”¹è®°å½•: {step['records_modified']:,}")
                print(f"     âœ… ä¿ç•™æœ‰æ•ˆSQL: {step['valid_sql_retained']:,}")
                print(f"     ğŸ”§ ä¿ç•™å‚æ•°SQL: {step['param_dependent_sql_retained']:,}")
        
        print(f"\nğŸ’¾ è¾“å‡ºæ–‡ä»¶:")
        for step in self.workflow_steps:
            if 'output_directory' in step and step['output_directory']:
                print(f"   ğŸ“ {step['step_name']}: {step['output_directory']}")


def run_complete_sql_cleaning_workflow(extracted_data_path: str, base_output_dir: str = "workflow_output") -> Dict[str, Any]:
    """
    è¿è¡Œå®Œæ•´çš„SQLæ¸…æ´—å·¥ä½œæµï¼ˆä»å·²æå–æ•°æ®å¼€å§‹ï¼‰
    
    Args:
        extracted_data_path: æå–æ•°æ®çš„è·¯å¾„
        base_output_dir: è¾“å‡ºåŸºç›®å½•
        
    Returns:
        å·¥ä½œæµç»“æœä¿¡æ¯
    """
    logger.info("å¼€å§‹å®Œæ•´çš„SQLæ¸…æ´—å·¥ä½œæµ")
    
    # åˆ›å»ºå·¥ä½œæµç®¡ç†å™¨
    workflow = WorkflowManager(base_output_dir)
    
    try:
        # æ­¥éª¤1: åŠ è½½æå–çš„æ•°æ®
        load_result = workflow.load_extracted_data(extracted_data_path)
        
        # æ­¥éª¤2: SQLæ¸…æ´—
        cleaning_result = workflow.run_sql_cleaning("sql_cleaning_step1")
        
        # å¯¼å‡ºæœ€ç»ˆæ•°æ®
        final_data_path = workflow.export_final_data()
        
        # ä¿å­˜å·¥ä½œæµæ‘˜è¦
        summary_path = workflow.save_workflow_summary()
        
        # æ‰“å°æ‘˜è¦
        workflow.print_workflow_summary()
        
        result = {
            'workflow_completed': True,
            'workflow_directory': str(workflow.workflow_dir),
            'final_data_path': final_data_path,
            'summary_path': summary_path,
            'load_result': load_result,
            'cleaning_result': cleaning_result
        }
        
        logger.info("å®Œæ•´çš„SQLæ¸…æ´—å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
        return result
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        raise


def run_complete_workflow_from_raw_data(data_dir: str, keywords: List[str] = None, base_output_dir: str = "workflow_output") -> Dict[str, Any]:
    """
    è¿è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†å·¥ä½œæµï¼ˆä»åŸå§‹æ•°æ®é›†å¼€å§‹ï¼‰
    
    Args:
        data_dir: åŸå§‹æ•°æ®ç›®å½•
        keywords: å…³é”®è¯åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨GORMå…³é”®è¯
        base_output_dir: è¾“å‡ºåŸºç›®å½•
        
    Returns:
        å·¥ä½œæµç»“æœä¿¡æ¯
    """
    logger.info("å¼€å§‹ä»åŸå§‹æ•°æ®é›†çš„å®Œæ•´å·¥ä½œæµ")
    
    # åˆ›å»ºå·¥ä½œæµç®¡ç†å™¨
    workflow = WorkflowManager(base_output_dir)
    
    try:
        # æ­¥éª¤1: ä»åŸå§‹æ•°æ®é›†åŠ è½½å¹¶æå–
        load_result = workflow.load_raw_dataset(data_dir, keywords)
        
        # æ­¥éª¤2: SQLæ¸…æ´—
        cleaning_result = workflow.run_sql_cleaning("sql_cleaning_step1")
        
        # å¯¼å‡ºæœ€ç»ˆæ•°æ®
        final_data_path = workflow.export_final_data("final_cleaned_data_from_raw.json")
        
        # ä¿å­˜å·¥ä½œæµæ‘˜è¦
        summary_path = workflow.save_workflow_summary()
        
        # æ‰“å°æ‘˜è¦
        workflow.print_workflow_summary()
        
        result = {
            'workflow_completed': True,
            'workflow_directory': str(workflow.workflow_dir),
            'final_data_path': final_data_path,
            'summary_path': summary_path,
            'load_result': load_result,
            'cleaning_result': cleaning_result
        }
        
        logger.info("ä»åŸå§‹æ•°æ®é›†çš„å®Œæ•´å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
        return result
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        raise 