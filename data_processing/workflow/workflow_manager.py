"""
Workflowç®¡ç†å™¨

ç®¡ç†æ•°æ®å¤„ç†çš„æ•´ä¸ªå·¥ä½œæµï¼ŒåŒ…æ‹¬æ•°æ®è¯»å–ã€æ¸…æ´—ã€éªŒè¯ç­‰æ­¥éª¤
"""

import json
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime

# å°è¯•ç›¸å¯¹å¯¼å…¥ï¼Œå¦‚æœå¤±è´¥åˆ™ç›´æ¥å¯¼å…¥
try:
    from ..data_reader import DataReader
    from ..cleaning.sql_cleaner import SQLCleaner
except ImportError:
    from data_reader import DataReader
    from cleaning.sql_cleaner import SQLCleaner

logger = logging.getLogger(__name__)


class WorkflowManager:
    """å·¥ä½œæµç®¡ç†å™¨
    
    è´Ÿè´£åè°ƒæ•°æ®å¤„ç†çš„å„ä¸ªæ­¥éª¤ï¼Œè®°å½•å¤„ç†è¿‡ç¨‹å’Œç»“æœ
    """
    
    def __init__(self, base_output_dir: str = "workflow_output"):
        """
        åˆå§‹åŒ–å·¥ä½œæµç®¡ç†å™¨
        
        Args:
            base_output_dir: å·¥ä½œæµè¾“å‡ºåŸºç›®å½•
        """
        self.base_output_dir = Path(base_output_dir)
        self.base_output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå½“å‰workflowå®ä¾‹çš„ç›®å½•
        self.workflow_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.workflow_dir = self.base_output_dir / f"workflow_{self.workflow_timestamp}"
        self.workflow_dir.mkdir(exist_ok=True)
        
        # å·¥ä½œæµæ­¥éª¤è®°å½•
        self.workflow_steps = []
        self.current_data = None
        self.extracted_data = None  # æå–çš„å…³é”®è¯æ•°æ®
        
        logger.info(f"å·¥ä½œæµç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.workflow_dir}")
    
    def load_raw_dataset(self, data_dir: str) -> Dict[str, Any]:
        """
        ä»åŸå§‹æ•°æ®é›†åŠ è½½æ‰€æœ‰æ•°æ®
        
        Args:
            data_dir: åŸå§‹æ•°æ®ç›®å½•
            
        Returns:
            åŠ è½½ç»“æœä¿¡æ¯
        """
        logger.info(f"å¼€å§‹ä»åŸå§‹æ•°æ®é›†åŠ è½½æ‰€æœ‰æ•°æ®: {data_dir}")
        
        # åˆ›å»ºæ•°æ®è¯»å–å™¨å¹¶è¯»å–æ‰€æœ‰æ•°æ®
        reader = DataReader(data_dir)
        reader.read_all_files()
        
        # è½¬æ¢ä¸ºdictæ ¼å¼çš„æ•°æ®
        self.current_data = []
        for record in reader.records:
            record_dict = {
                'function_name': record.function_name,
                'orm_code': record.orm_code,
                'caller': record.caller,
                'sql_statement_list': record.sql_statement_list,
                'sql_types': record.sql_types,
                'code_meta_data': [
                    {
                        'code_file': meta.code_file,
                        'code_start_line': meta.code_start_line,
                        'code_end_line': meta.code_end_line,
                        'code_key': meta.code_key,
                        'code_value': meta.code_value,
                        'code_label': meta.code_label,
                        'code_type': meta.code_type,
                        'code_version': meta.code_version
                    } for meta in record.code_meta_data
                ],
                'sql_pattern_cnt': record.sql_pattern_cnt,
                'source_file': record.source_file
            }
            self.current_data.append(record_dict)
        
        step_info = {
            'step_name': 'load_raw_dataset',
            'step_type': 'data_loading',
            'timestamp': datetime.now().isoformat(),
            'input_source': str(data_dir),
            'total_records_loaded': len(self.current_data),
            'data_size_mb': sum(len(str(record)) for record in self.current_data) / (1024 * 1024)
        }
        
        self.workflow_steps.append(step_info)
        
        logger.info(f"åŸå§‹æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(self.current_data):,} æ¡è®°å½•")
        return step_info
    
    def run_sql_cleaning(self, step_name: str = "sql_cleaning_step1") -> Dict[str, Any]:
        """
        è¿è¡ŒSQLæ¸…æ´—æ­¥éª¤ï¼ˆæ¸…æ´—å…¨ä½“æ•°æ®ï¼‰
        
        Args:
            step_name: æ­¥éª¤åç§°
            
        Returns:
            æ¸…æ´—ç»“æœä¿¡æ¯
        """
        if self.current_data is None:
            raise ValueError("è¯·å…ˆåŠ è½½æ•°æ®")
        
        logger.info(f"å¼€å§‹å¯¹å…¨ä½“æ•°æ®é›†è¿›è¡ŒSQLæ¸…æ´—: {step_name}")
        
        # åˆ›å»ºSQLæ¸…æ´—å™¨
        cleaner_output_dir = self.workflow_dir / "cleaning_steps"
        sql_cleaner = SQLCleaner(str(cleaner_output_dir))
        
        # æ‰§è¡Œæ¸…æ´—
        cleaning_result = sql_cleaner.clean_dataset(self.current_data, step_name)
        
        # åŠ è½½æ¸…æ´—åçš„æ•°æ®ä½œä¸ºå½“å‰æ•°æ®
        cleaned_data_file = Path(cleaning_result['output_directory']) / "cleaned_records.json"
        with open(cleaned_data_file, 'r', encoding='utf-8') as f:
            self.current_data = json.load(f)
        
        # è®°å½•å·¥ä½œæµæ­¥éª¤
        step_info = {
            'step_name': step_name,
            'step_type': 'sql_cleaning',
            'timestamp': datetime.now().isoformat(),
            'input_records': cleaning_result['input_records_count'],
            'output_records': cleaning_result['output_records_count'],
            'records_modified': cleaning_result['records_modified'],
            'invalid_sql_removed': cleaning_result['invalid_sql_removed'],
            'valid_sql_retained': cleaning_result['valid_sql_retained'],
            'param_dependent_sql_retained': cleaning_result['param_dependent_sql_retained'],
            'empty_sql_lists_found': cleaning_result.get('empty_sql_lists_found', 0),
            'lists_emptied_after_cleaning': cleaning_result.get('lists_emptied_after_cleaning', 0),
            'output_directory': cleaning_result['output_directory']
        }
        
        self.workflow_steps.append(step_info)
        
        logger.info(f"å…¨ä½“æ•°æ®é›†SQLæ¸…æ´—å®Œæˆ - ç§»é™¤äº† {cleaning_result['invalid_sql_removed']:,} ä¸ªæ— æ•ˆSQLï¼Œä¿®æ”¹äº† {cleaning_result['records_modified']:,} æ¡è®°å½•")
        return cleaning_result
    
    def extract_keyword_data(self, keywords: Optional[List[str]] = None, step_name: str = "keyword_extraction_step2") -> Dict[str, Any]:
        """
        ä»æ¸…æ´—åçš„æ•°æ®ä¸­æå–å…³é”®è¯æ•°æ®
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨GORMå…³é”®è¯
            step_name: æ­¥éª¤åç§°
            
        Returns:
            æå–ç»“æœä¿¡æ¯
        """
        if self.current_data is None:
            raise ValueError("è¯·å…ˆåŠ è½½å¹¶æ¸…æ´—æ•°æ®")
        
        logger.info(f"å¼€å§‹ä»æ¸…æ´—åçš„æ•°æ®ä¸­æå–å…³é”®è¯: {step_name}")
        
        # åˆ›å»ºä¸´æ—¶çš„DataReaderæ¥ä½¿ç”¨å…¶æå–åŠŸèƒ½
        try:
            from ..data_reader import FunctionRecord, CodeMetaData
        except ImportError:
            from data_reader import FunctionRecord, CodeMetaData
        
        # è½¬æ¢å›FunctionRecordæ ¼å¼
        temp_records = []
        for record_dict in self.current_data:
            code_meta_data = [
                CodeMetaData(
                    code_file=meta['code_file'],
                    code_start_line=meta['code_start_line'],
                    code_end_line=meta['code_end_line'],
                    code_key=meta['code_key'],
                    code_value=meta['code_value'],
                    code_label=meta['code_label'],
                    code_type=meta['code_type'],
                    code_version=meta['code_version']
                ) for meta in record_dict['code_meta_data']
            ]
            
            record = FunctionRecord(
                function_name=record_dict['function_name'],
                orm_code=record_dict['orm_code'],
                caller=record_dict['caller'],
                sql_statement_list=record_dict['sql_statement_list'],
                sql_types=record_dict['sql_types'],
                code_meta_data=code_meta_data,
                sql_pattern_cnt=record_dict['sql_pattern_cnt'],
                source_file=record_dict['source_file']
            )
            temp_records.append(record)
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        import tempfile
        import os
        
        with tempfile.TemporaryDirectory() as temp_dir:
            # åˆ›å»ºä¸´æ—¶DataReaderå¹¶è®¾ç½®æ•°æ®ï¼ˆä¸ä¾èµ–å®é™…æ–‡ä»¶ï¼‰
            temp_reader = DataReader(temp_dir)
            temp_reader.records = temp_records
            
            # æ‰§è¡Œå…³é”®è¯æå–
            extraction_output_dir = self.workflow_dir / "keyword_extraction"
            if keywords is None:
                extract_result = temp_reader.extract_gorm_keywords(str(extraction_output_dir))
            else:
                extract_result = temp_reader.extract_by_keywords(
                    keywords=keywords,
                    output_dir=str(extraction_output_dir),
                    step_name=step_name
                )
        
        # åŠ è½½æå–çš„æ•°æ®
        extracted_data_file = Path(extract_result['output_directory']) / "keyword_matched_records.json"
        with open(extracted_data_file, 'r', encoding='utf-8') as f:
            self.extracted_data = json.load(f)
        
        step_info = {
            'step_name': step_name,
            'step_type': 'keyword_extraction',
            'timestamp': datetime.now().isoformat(),
            'input_records': len(self.current_data),
            'extracted_records': len(self.extracted_data),
            'extraction_rate': len(self.extracted_data) / len(self.current_data) * 100,
            'keywords_used': keywords or "GORMé¢„å®šä¹‰å…³é”®è¯",
            'output_directory': extract_result['output_directory']
        }
        
        self.workflow_steps.append(step_info)
        
        logger.info(f"å…³é”®è¯æå–å®Œæˆ - ä» {len(self.current_data):,} æ¡è®°å½•ä¸­æå–äº† {len(self.extracted_data):,} æ¡åŒ¹é…è®°å½•")
        return step_info
    
    def process_extracted_data(self, step_name: str = "special_processing_step3") -> Dict[str, Any]:
        """
        å¯¹æå–çš„æ•°æ®è¿›è¡Œç‰¹æ®Šå¤„ç†
        
        Args:
            step_name: æ­¥éª¤åç§°
            
        Returns:
            å¤„ç†ç»“æœä¿¡æ¯
        """
        if self.extracted_data is None:
            raise ValueError("è¯·å…ˆæå–å…³é”®è¯æ•°æ®")
        
        logger.info(f"å¼€å§‹å¯¹æå–çš„æ•°æ®è¿›è¡Œç‰¹æ®Šå¤„ç†: {step_name}")
        
        # TODO: è¿™é‡Œé¢„ç•™ç‰¹æ®Šå¤„ç†é€»è¾‘çš„æ¥å£
        # å½“å‰åªæ˜¯ç®€å•å¤åˆ¶ï¼Œåç»­å¯ä»¥æ·»åŠ æ•°æ®å¢å¼ºã€æ ‡æ³¨ç­‰å¤„ç†
        processed_data = []
        for record in self.extracted_data:
            # å¤åˆ¶åŸè®°å½•
            processed_record = record.copy()
            
            # TODO: åœ¨è¿™é‡Œæ·»åŠ ç‰¹æ®Šå¤„ç†é€»è¾‘
            # ä¾‹å¦‚ï¼š
            # - æ•°æ®å¢å¼º
            # - è‡ªåŠ¨æ ‡æ³¨
            # - æ ¼å¼è½¬æ¢
            # - è´¨é‡è¯„ä¼°
            
            # æ·»åŠ å¤„ç†æ ‡è®°
            processed_record['processing_metadata'] = {
                'processed_at': datetime.now().isoformat(),
                'processing_step': step_name,
                'processing_applied': []  # åç»­å¯ä»¥è®°å½•åº”ç”¨çš„å¤„ç†æ–¹æ³•
            }
            
            processed_data.append(processed_record)
        
        self.extracted_data = processed_data
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        processing_output_dir = self.workflow_dir / "special_processing"
        processing_output_dir.mkdir(exist_ok=True)
        
        processed_data_file = processing_output_dir / f"{step_name}.json"
        with open(processed_data_file, 'w', encoding='utf-8') as f:
            json.dump(self.extracted_data, f, ensure_ascii=False, indent=2)
        
        step_info = {
            'step_name': step_name,
            'step_type': 'special_processing',
            'timestamp': datetime.now().isoformat(),
            'input_records': len(self.extracted_data),
            'output_records': len(self.extracted_data),
            'processing_applied': [],  # ç›®å‰ä¸ºç©ºï¼Œåç»­å¯ä»¥è®°å½•å…·ä½“å¤„ç†
            'output_file': str(processed_data_file)
        }
        
        self.workflow_steps.append(step_info)
        
        logger.info(f"ç‰¹æ®Šå¤„ç†å®Œæˆ - å¤„ç†äº† {len(self.extracted_data):,} æ¡æå–çš„è®°å½•")
        return step_info
    
    def merge_processed_data_back(self, step_name: str = "merge_back_step4") -> Dict[str, Any]:
        """
        å°†å¤„ç†åçš„æ•°æ®åˆå¹¶å›åŸæ•°æ®é›†
        
        Args:
            step_name: æ­¥éª¤åç§°
            
        Returns:
            åˆå¹¶ç»“æœä¿¡æ¯
        """
        if self.extracted_data is None or self.current_data is None:
            raise ValueError("è¯·å…ˆå®Œæˆæ•°æ®æå–å’Œç‰¹æ®Šå¤„ç†")
        
        logger.info(f"å¼€å§‹å°†å¤„ç†åçš„æ•°æ®åˆå¹¶å›åŸæ•°æ®é›†: {step_name}")
        
        # åˆ›å»ºfunction_nameåˆ°è®°å½•çš„æ˜ å°„ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾
        extracted_data_map = {record['function_name']: record for record in self.extracted_data}
        
        # åˆå¹¶æ•°æ®
        merged_data = []
        updated_count = 0
        
        for original_record in self.current_data:
            function_name = original_record['function_name']
            
            if function_name in extracted_data_map:
                # å¦‚æœåœ¨æå–æ•°æ®ä¸­æ‰¾åˆ°å¯¹åº”è®°å½•ï¼Œä½¿ç”¨å¤„ç†åçš„ç‰ˆæœ¬
                processed_record = extracted_data_map[function_name].copy()
                
                # ä¿ç•™åŸå§‹è®°å½•ä¸­å¯èƒ½ä¸åœ¨æå–æ•°æ®ä¸­çš„å­—æ®µ
                for key, value in original_record.items():
                    if key not in processed_record:
                        processed_record[key] = value
                
                # æ·»åŠ åˆå¹¶æ ‡è®°
                if 'processing_metadata' not in processed_record:
                    processed_record['processing_metadata'] = {}
                processed_record['processing_metadata']['merged_back'] = True
                processed_record['processing_metadata']['merge_timestamp'] = datetime.now().isoformat()
                
                merged_data.append(processed_record)
                updated_count += 1
            else:
                # å¦‚æœä¸åœ¨æå–æ•°æ®ä¸­ï¼Œä¿ç•™åŸå§‹è®°å½•
                merged_data.append(original_record)
        
        self.current_data = merged_data
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        merge_output_dir = self.workflow_dir / "merged_data"
        merge_output_dir.mkdir(exist_ok=True)
        
        merged_data_file = merge_output_dir / f"{step_name}.json"
        with open(merged_data_file, 'w', encoding='utf-8') as f:
            json.dump(self.current_data, f, ensure_ascii=False, indent=2)
        
        step_info = {
            'step_name': step_name,
            'step_type': 'data_merging',
            'timestamp': datetime.now().isoformat(),
            'total_records': len(self.current_data),
            'updated_records': updated_count,
            'unchanged_records': len(self.current_data) - updated_count,
            'update_rate': updated_count / len(self.current_data) * 100,
            'output_file': str(merged_data_file)
        }
        
        self.workflow_steps.append(step_info)
        
        logger.info(f"æ•°æ®åˆå¹¶å®Œæˆ - æ›´æ–°äº† {updated_count:,} æ¡è®°å½•ï¼Œä¿æŒäº† {len(self.current_data) - updated_count:,} æ¡åŸå§‹è®°å½•")
        return step_info
    
    def save_workflow_summary(self) -> str:
        """
        ä¿å­˜å·¥ä½œæµæ‘˜è¦
        
        Returns:
            æ‘˜è¦æ–‡ä»¶è·¯å¾„
        """
        summary = {
            'workflow_id': f"workflow_{self.workflow_timestamp}",
            'start_time': self.workflow_steps[0]['timestamp'] if self.workflow_steps else None,
            'end_time': datetime.now().isoformat(),
            'total_steps': len(self.workflow_steps),
            'steps': self.workflow_steps,
            'final_data_count': len(self.current_data) if self.current_data else 0,
            'extracted_data_count': len(self.extracted_data) if self.extracted_data else 0,
            'workflow_directory': str(self.workflow_dir)
        }
        
        summary_file = self.workflow_dir / "workflow_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å·¥ä½œæµæ‘˜è¦å·²ä¿å­˜: {summary_file}")
        return str(summary_file)
    
    def export_final_data(self, output_file: str = "final_processed_data.json") -> str:
        """
        å¯¼å‡ºæœ€ç»ˆå¤„ç†åçš„æ•°æ®
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶å
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not self.current_data:
            raise ValueError("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
        
        export_path = self.workflow_dir / output_file
        with open(export_path, 'w', encoding='utf-8') as f:
            json.dump(self.current_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æœ€ç»ˆæ•°æ®å·²å¯¼å‡º: {export_path}")
        return str(export_path)
    
    def print_workflow_summary(self):
        """æ‰“å°å·¥ä½œæµæ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ”„ æ•°æ®å¤„ç†å·¥ä½œæµæ‘˜è¦")
        print("=" * 60)
        
        print(f"ğŸ“ å·¥ä½œæµç›®å½•: {self.workflow_dir}")
        print(f"â° å·¥ä½œæµID: workflow_{self.workflow_timestamp}")
        print(f"ğŸ“Š æ€»æ­¥éª¤æ•°: {len(self.workflow_steps)}")
        print(f"ğŸ“‹ æœ€ç»ˆæ•°æ®é‡: {len(self.current_data) if self.current_data else 0} æ¡è®°å½•")
        print(f"ğŸ¯ æå–æ•°æ®é‡: {len(self.extracted_data) if self.extracted_data else 0} æ¡è®°å½•")
        
        print(f"\nğŸ” å¤„ç†æ­¥éª¤è¯¦æƒ…:")
        for i, step in enumerate(self.workflow_steps, 1):
            print(f"  {i}. {step['step_name']} ({step['step_type']})")
            
            if step['step_type'] == 'data_loading':
                print(f"     ğŸ“¥ åŠ è½½è®°å½•: {step['total_records_loaded']:,}")
                print(f"     ğŸ’¾ æ•°æ®å¤§å°: {step['data_size_mb']:.2f} MB")
                
            elif step['step_type'] == 'sql_cleaning':
                print(f"     ğŸ“Š è¾“å…¥è®°å½•: {step['input_records']:,}")
                print(f"     ğŸ“Š è¾“å‡ºè®°å½•: {step['output_records']:,}")
                print(f"     ğŸ—‘ï¸ ç§»é™¤æ— æ•ˆSQL: {step['invalid_sql_removed']:,}")
                print(f"     âœï¸ ä¿®æ”¹è®°å½•: {step['records_modified']:,}")
                print(f"     âœ… ä¿ç•™æœ‰æ•ˆSQL: {step['valid_sql_retained']:,}")
                if 'empty_sql_lists_found' in step:
                    print(f"     ğŸ“‹ åŸå§‹ç©ºåˆ—è¡¨: {step['empty_sql_lists_found']:,}")
                if 'lists_emptied_after_cleaning' in step:
                    print(f"     ğŸ§¹ æ¸…æ´—åç©ºåˆ—è¡¨: {step['lists_emptied_after_cleaning']:,}")
                
            elif step['step_type'] == 'keyword_extraction':
                print(f"     ğŸ“Š è¾“å…¥è®°å½•: {step['input_records']:,}")
                print(f"     ğŸ¯ æå–è®°å½•: {step['extracted_records']:,}")
                print(f"     ğŸ“ˆ æå–ç‡: {step['extraction_rate']:.2f}%")
                
            elif step['step_type'] == 'special_processing':
                print(f"     ğŸ”§ å¤„ç†è®°å½•: {step['input_records']:,}")
                print(f"     ğŸ“¤ è¾“å‡ºè®°å½•: {step['output_records']:,}")
                
            elif step['step_type'] == 'data_merging':
                print(f"     ğŸ“Š æ€»è®°å½•æ•°: {step['total_records']:,}")
                print(f"     ğŸ”„ æ›´æ–°è®°å½•: {step['updated_records']:,}")
                print(f"     ğŸ“ˆ æ›´æ–°ç‡: {step['update_rate']:.2f}%")
        
        print(f"\nğŸ’¾ è¾“å‡ºæ–‡ä»¶:")
        for step in self.workflow_steps:
            if 'output_directory' in step and step['output_directory']:
                print(f"   ğŸ“ {step['step_name']}: {step['output_directory']}")
            elif 'output_file' in step and step['output_file']:
                print(f"   ğŸ“„ {step['step_name']}: {step['output_file']}")


def run_complete_workflow_from_raw_data(data_dir: str, keywords: Optional[List[str]] = None, base_output_dir: str = "workflow_output") -> Dict[str, Any]:
    """
    è¿è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†å·¥ä½œæµï¼ˆæ–°æ¶æ„ï¼šæ¸…æ´— -> æå– -> å¤„ç† -> åˆå¹¶ï¼‰
    
    Args:
        data_dir: åŸå§‹æ•°æ®ç›®å½•
        keywords: å…³é”®è¯åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨GORMå…³é”®è¯
        base_output_dir: è¾“å‡ºåŸºç›®å½•
        
    Returns:
        å·¥ä½œæµç»“æœä¿¡æ¯
    """
    logger.info("å¼€å§‹æ–°æ¶æ„çš„å®Œæ•´æ•°æ®å¤„ç†å·¥ä½œæµ")
    
    # åˆ›å»ºå·¥ä½œæµç®¡ç†å™¨
    workflow = WorkflowManager(base_output_dir)
    
    try:
        # æ­¥éª¤1: åŠ è½½åŸå§‹æ•°æ®é›†
        load_result = workflow.load_raw_dataset(data_dir)
        
        # æ­¥éª¤2: å¯¹å…¨ä½“æ•°æ®è¿›è¡ŒSQLæ¸…æ´—
        cleaning_result = workflow.run_sql_cleaning("sql_cleaning_step1")
        
        # æ­¥éª¤3: ä»æ¸…æ´—åçš„æ•°æ®ä¸­æå–å…³é”®è¯æ•°æ®
        extraction_result = workflow.extract_keyword_data(keywords, "keyword_extraction_step2")
        
        # æ­¥éª¤4: å¯¹æå–çš„æ•°æ®è¿›è¡Œç‰¹æ®Šå¤„ç†
        processing_result = workflow.process_extracted_data("special_processing_step3")
        
        # æ­¥éª¤5: å°†å¤„ç†åçš„æ•°æ®åˆå¹¶å›åŸæ•°æ®é›†
        merge_result = workflow.merge_processed_data_back("merge_back_step4")
        
        # å¯¼å‡ºæœ€ç»ˆæ•°æ®
        final_data_path = workflow.export_final_data("final_processed_dataset.json")
        
        # ä¿å­˜å·¥ä½œæµæ‘˜è¦
        summary_path = workflow.save_workflow_summary()
        
        # æ‰“å°æ‘˜è¦
        workflow.print_workflow_summary()
        
        result = {
            'workflow_completed': True,
            'workflow_directory': str(workflow.workflow_dir),
            'final_data_path': final_data_path,
            'summary_path': summary_path,
            'load_result': load_result,
            'cleaning_result': cleaning_result,
            'extraction_result': extraction_result,
            'processing_result': processing_result,
            'merge_result': merge_result
        }
        
        logger.info("æ–°æ¶æ„çš„å®Œæ•´æ•°æ®å¤„ç†å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
        return result
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        raise


# ä¿ç•™æ—§çš„å‡½æ•°ä»¥å…¼å®¹ç°æœ‰ä»£ç 
def run_complete_sql_cleaning_workflow(extracted_data_path: str, base_output_dir: str = "workflow_output") -> Dict[str, Any]:
    """
    è¿è¡ŒSQLæ¸…æ´—å·¥ä½œæµï¼ˆä»å·²æå–æ•°æ®å¼€å§‹ï¼‰- ä¿ç•™å…¼å®¹æ€§
    """
    logger.warning("ä½¿ç”¨æ—§ç‰ˆworkflowï¼Œå»ºè®®ä½¿ç”¨ run_complete_workflow_from_raw_data")
    
    workflow = WorkflowManager(base_output_dir)
    
    try:
        # åŠ è½½å·²æå–çš„æ•°æ®
        with open(extracted_data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        workflow.current_data = data
        
        # SQLæ¸…æ´—
        cleaning_result = workflow.run_sql_cleaning("sql_cleaning_step1")
        
        # å¯¼å‡ºæœ€ç»ˆæ•°æ®
        final_data_path = workflow.export_final_data()
        
        # ä¿å­˜å·¥ä½œæµæ‘˜è¦
        summary_path = workflow.save_workflow_summary()
        
        workflow.print_workflow_summary()
        
        return {
            'workflow_completed': True,
            'workflow_directory': str(workflow.workflow_dir),
            'final_data_path': final_data_path,
            'summary_path': summary_path,
            'cleaning_result': cleaning_result
        }
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        raise 