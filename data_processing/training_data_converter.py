#!/usr/bin/env python3
"""
è®­ç»ƒæ•°æ®è½¬æ¢å™¨

å°†workflowå¤„ç†åçš„ORMæ•°æ®è½¬æ¢ä¸ºLLMå¾®è°ƒè®­ç»ƒæ ¼å¼
"""

import json
import os
import logging
import random
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import glob
from config.training.data_conversion.orm2sql_prompt_template import PROMPT_TEMPLATE

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TrainingDataConverter:
    """è®­ç»ƒæ•°æ®è½¬æ¢å™¨"""
    
    def __init__(self, project_root: Optional[str] = None):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        """
        if project_root is None:
            project_root = str(Path(__file__).parents[1])
        
        self.project_root = Path(project_root)
        self.workflow_output_dir = self.project_root / "workflow_output"
        self.training_data_dir = self.project_root / "model" / "data" / "orm2sql_training_data"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.training_data_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")
        logger.info(f"å·¥ä½œæµè¾“å‡ºç›®å½•: {self.workflow_output_dir}")
        logger.info(f"è®­ç»ƒæ•°æ®è¾“å‡ºç›®å½•: {self.training_data_dir}")
    
    def find_latest_workflow_output(self) -> Optional[Path]:
        """
        æŸ¥æ‰¾æœ€æ–°çš„workflowè¾“å‡ºç›®å½•
        
        Returns:
            æœ€æ–°workflowç›®å½•è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
        """
        if not self.workflow_output_dir.exists():
            logger.error(f"å·¥ä½œæµè¾“å‡ºç›®å½•ä¸å­˜åœ¨: {self.workflow_output_dir}")
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰workflowç›®å½•
        workflow_dirs = list(self.workflow_output_dir.glob("workflow_*"))
        if not workflow_dirs:
            logger.error("æœªæ‰¾åˆ°ä»»ä½•workflowè¾“å‡ºç›®å½•")
            return None
        
        # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè·å–æœ€æ–°çš„
        workflow_dirs.sort(key=lambda x: x.name, reverse=True)
        latest_dir = workflow_dirs[0]
        
        logger.info(f"æ‰¾åˆ°æœ€æ–°workflowç›®å½•: {latest_dir}")
        return latest_dir
    
    def load_workflow_data(self, workflow_dir: Path) -> List[Dict]:
        """
        åŠ è½½workflowå¤„ç†åçš„æ•°æ®
        
        Args:
            workflow_dir: workflowè¾“å‡ºç›®å½•
            
        Returns:
            å¤„ç†åçš„æ•°æ®åˆ—è¡¨
        """
        final_data_file = workflow_dir / "final_processed_dataset.json"
        
        if not final_data_file.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æœ€ç»ˆå¤„ç†æ•°æ®æ–‡ä»¶: {final_data_file}")
        
        logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {final_data_file}")
        logger.info(f"æ–‡ä»¶å¤§å°: {final_data_file.stat().st_size / (1024*1024):.1f} MB")
        
        with open(final_data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(data)} æ¡è®°å½•")
        return data
    
    def format_code_metadata(self, code_meta_data: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–ä»£ç å…ƒæ•°æ®ä¸ºå­—ç¬¦ä¸²
        
        Args:
            code_meta_data: ä»£ç å…ƒæ•°æ®åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
        """
        if not code_meta_data:
            return ""
        
        formatted_parts = []
        for meta in code_meta_data:
            if 'code_key' in meta and 'code_value' in meta:
                part = f"**{meta['code_key']}**:\n{meta['code_value']}"
                if 'code_file' in meta:
                    part += f"\n(æ–‡ä»¶: {meta['code_file']})"
                formatted_parts.append(part)
        
        return "\n\n".join(formatted_parts)
    
    def create_training_prompt(self, record: Dict) -> str:
        """
        æ ¹æ®è®°å½•åˆ›å»ºè®­ç»ƒæç¤ºè¯
        
        Args:
            record: å•æ¡ORMè®°å½•
            
        Returns:
            æ ¼å¼åŒ–çš„æç¤ºè¯
        """
        function_name = record.get("function_name", "æœªçŸ¥å‡½æ•°")
        orm_code = record.get("orm_code", "")
        caller = record.get("caller", "")
        callee = record.get("callee", "")
        code_meta_data_str = self.format_code_metadata(record.get("code_meta_data", []))
        prompt = PROMPT_TEMPLATE.format(
            function_name=function_name,
            orm_code=orm_code,
            caller=caller,
            code_meta_data_str=code_meta_data_str,
        )
        return prompt.strip()
    
    def create_training_response(self, record: Dict) -> str:
        """
        åˆ›å»ºè®­ç»ƒå“åº”ï¼ˆæ ‡å‡†ç­”æ¡ˆï¼‰
        
        Args:
            record: å•æ¡ORMè®°å½•
            
        Returns:
            JSONæ ¼å¼çš„SQLè¯­å¥åˆ—è¡¨
        """
        sql_statement_list = record.get('sql_statement_list', [])
        return json.dumps(sql_statement_list, ensure_ascii=False, indent=None)
    
    def convert_to_training_format(self, data: List[Dict], shuffle: bool = True) -> List[Dict]:
        """
        å°†ORMæ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
        
        Args:
            data: workflowå¤„ç†åçš„æ•°æ®
            shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®é¡ºåºï¼Œé»˜è®¤True
            
        Returns:
            è½¬æ¢åçš„è®­ç»ƒæ•°æ®
        """
        training_data = []
        
        logger.info("å¼€å§‹è½¬æ¢è®­ç»ƒæ•°æ®...")
        
        # å¦‚æœéœ€è¦æ‰“ä¹±æ•°æ®ï¼Œå…ˆæ‰“ä¹±åŸå§‹æ•°æ®
        if shuffle:
            logger.info("æ­£åœ¨æ‰“ä¹±æ•°æ®é¡ºåº...")
            data_copy = data.copy()
            random.shuffle(data_copy)
            data = data_copy
            logger.info("æ•°æ®æ‰“ä¹±å®Œæˆ")
        
        for i, record in enumerate(data):
            if i % 1000 == 0:
                logger.info(f"å·²å¤„ç† {i}/{len(data)} æ¡è®°å½•")
            
            try:
                # åˆ›å»ºæç¤ºè¯å’Œå“åº”
                prompt = self.create_training_prompt(record)
                response = self.create_training_response(record)
                
                # æ„å»ºè®­ç»ƒæ ·æœ¬
                training_sample = {
                    "instruction": prompt,
                    "output": response
                }
                
                # å¯é€‰ï¼šæ·»åŠ é¢å¤–çš„å…ƒä¿¡æ¯ç”¨äºè°ƒè¯•
                metadata = {
                    "function_name": record.get('function_name', ''),
                    "source_file": record.get('source_file', ''),
                    "sql_types": record.get('sql_types', [])
                }
                training_sample["metadata"] = metadata
                
                training_data.append(training_sample)
                
            except Exception as e:
                logger.error(f"å¤„ç†ç¬¬ {i} æ¡è®°å½•æ—¶å‡ºé”™: {e}")
                continue
        
        logger.info(f"è½¬æ¢å®Œæˆï¼Œå…±ç”Ÿæˆ {len(training_data)} æ¡è®­ç»ƒæ ·æœ¬")
        return training_data
    
    def save_training_data(self, training_data: List[Dict], output_name: Optional[str] = None) -> Path:
        """
        ä¿å­˜è®­ç»ƒæ•°æ®
        
        Args:
            training_data: è½¬æ¢åçš„è®­ç»ƒæ•°æ®
            output_name: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if output_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_name = f"orm2sql_training_data_{timestamp}.json"
        
        output_path = self.training_data_dir / output_name
        
        logger.info(f"æ­£åœ¨ä¿å­˜è®­ç»ƒæ•°æ®åˆ°: {output_path}")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        # è®¡ç®—æ–‡ä»¶å¤§å°
        file_size = output_path.stat().st_size / (1024 * 1024)
        logger.info(f"è®­ç»ƒæ•°æ®ä¿å­˜å®Œæˆï¼Œæ–‡ä»¶å¤§å°: {file_size:.1f} MB")
        
        return output_path
    
    def create_dataset_info(self, training_data: List[Dict], dataset_name: str) -> Dict:
        """
        åˆ›å»ºæ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
        
        Args:
            training_data: è®­ç»ƒæ•°æ®
            dataset_name: æ•°æ®é›†åç§°
            
        Returns:
            æ•°æ®é›†ä¿¡æ¯å­—å…¸
        """
        return {
            dataset_name: {
                "file_name": f"{dataset_name}.json",
                "columns": {
                    "prompt": "instruction",
                    "response": "output"
                },
                "file_sha1": "",  # å¯ä»¥åç»­è®¡ç®—
                "num_samples": len(training_data),
                "description": "ORMåˆ°SQLè½¬æ¢è®­ç»ƒæ•°æ®é›†ï¼ŒåŸºäºçœŸå®ä»£ç åˆ†æç”Ÿæˆ"
            }
        }
    
    def run_conversion(self, workflow_dir: Optional[Path] = None, output_name: Optional[str] = None) -> Tuple[Path, Dict]:
        """
        æ‰§è¡Œå®Œæ•´çš„æ•°æ®è½¬æ¢æµç¨‹
        
        Args:
            workflow_dir: æŒ‡å®šçš„workflowç›®å½•ï¼ˆå¯é€‰ï¼‰
            output_name: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
            
        Returns:
            (è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„, æ•°æ®é›†ä¿¡æ¯)
        """
        # 1. æŸ¥æ‰¾æˆ–ä½¿ç”¨æŒ‡å®šçš„workflowç›®å½•
        if workflow_dir is None:
            workflow_dir = self.find_latest_workflow_output()
            if workflow_dir is None:
                raise FileNotFoundError("æœªæ‰¾åˆ°workflowè¾“å‡ºç›®å½•")
        
        # 2. åŠ è½½æ•°æ®
        data = self.load_workflow_data(workflow_dir)
        
        # 3. è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼ï¼ˆé»˜è®¤æ‰“ä¹±æ•°æ®ï¼‰
        training_data = self.convert_to_training_format(data, shuffle=True)
        
        # 4. ä¿å­˜è®­ç»ƒæ•°æ®
        output_path = self.save_training_data(training_data, output_name)
        
        # 5. åˆ›å»ºæ•°æ®é›†ä¿¡æ¯
        dataset_name = output_path.stem
        dataset_info = self.create_dataset_info(training_data, dataset_name)
        
        # 6. ä¿å­˜æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
        info_path = self.training_data_dir / "dataset_info.json"
        if info_path.exists():
            with open(info_path, 'r', encoding='utf-8') as f:
                existing_info = json.load(f)
            existing_info.update(dataset_info)
        else:
            existing_info = dataset_info
        
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(existing_info, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ•°æ®é›†ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")
        
        return output_path, dataset_info


def main():
    """ä¸»å‡½æ•°"""
    converter = TrainingDataConverter()
    
    try:
        # æ‰§è¡Œè½¬æ¢
        output_path, dataset_info = converter.run_conversion()
        
        print(f"\nâœ… æ•°æ®è½¬æ¢å®Œæˆ!")
        print(f"ğŸ“ è®­ç»ƒæ•°æ®ä¿å­˜è·¯å¾„: {output_path}")
        print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {dataset_info[list(dataset_info.keys())[0]]['num_samples']}")
        print(f"ğŸ“ æ•°æ®é›†ä¿¡æ¯: {converter.training_data_dir / 'dataset_info.json'}")
        
    except Exception as e:
        logger.error(f"æ•°æ®è½¬æ¢å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main() 