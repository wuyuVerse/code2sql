#!/usr/bin/env python3
"""
è®­ç»ƒæ•°æ®è½¬æ¢å™¨

å°†workflowå¤„ç†åçš„ORMæ•°æ®è½¬æ¢ä¸ºLLMå¾®è°ƒè®­ç»ƒæ ¼å¼
"""

import json
import os
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import glob

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TrainingDataConverter:
    """è®­ç»ƒæ•°æ®è½¬æ¢å™¨"""
    
    def __init__(self, project_root: Optional[str] = None):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        """
        if project_root is None:
            project_root = str(Path(__file__).parents[1])
        
        self.project_root = Path(project_root)
        self.workflow_output_dir = self.project_root / "workflow_output"
        self.training_data_dir = self.project_root / "model" / "data" / "orm2sql_training_data"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.training_data_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")
        logger.info(f"å·¥ä½œæµè¾“å‡ºç›®å½•: {self.workflow_output_dir}")
        logger.info(f"è®­ç»ƒæ•°æ®è¾“å‡ºç›®å½•: {self.training_data_dir}")
    
    def find_latest_workflow_output(self) -> Optional[Path]:
        """
        æŸ¥æ‰¾æœ€æ–°çš„workflowè¾“å‡ºç›®å½•
        
        Returns:
            æœ€æ–°workflowç›®å½•è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
        """
        if not self.workflow_output_dir.exists():
            logger.error(f"å·¥ä½œæµè¾“å‡ºç›®å½•ä¸å­˜åœ¨: {self.workflow_output_dir}")
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰workflowç›®å½•
        workflow_dirs = list(self.workflow_output_dir.glob("workflow_*"))
        if not workflow_dirs:
            logger.error("æœªæ‰¾åˆ°ä»»ä½•workflowè¾“å‡ºç›®å½•")
            return None
        
        # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè·å–æœ€æ–°çš„
        workflow_dirs.sort(key=lambda x: x.name, reverse=True)
        latest_dir = workflow_dirs[0]
        
        logger.info(f"æ‰¾åˆ°æœ€æ–°workflowç›®å½•: {latest_dir}")
        return latest_dir
    
    def load_workflow_data(self, workflow_dir: Path) -> List[Dict]:
        """
        åŠ è½½workflowå¤„ç†åçš„æ•°æ®
        
        Args:
            workflow_dir: workflowè¾“å‡ºç›®å½•
            
        Returns:
            å¤„ç†åçš„æ•°æ®åˆ—è¡¨
        """
        final_data_file = workflow_dir / "final_processed_dataset.json"
        
        if not final_data_file.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æœ€ç»ˆå¤„ç†æ•°æ®æ–‡ä»¶: {final_data_file}")
        
        logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {final_data_file}")
        logger.info(f"æ–‡ä»¶å¤§å°: {final_data_file.stat().st_size / (1024*1024):.1f} MB")
        
        with open(final_data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(data)} æ¡è®°å½•")
        return data
    
    def format_code_metadata(self, code_meta_data: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–ä»£ç å…ƒæ•°æ®ä¸ºå­—ç¬¦ä¸²
        
        Args:
            code_meta_data: ä»£ç å…ƒæ•°æ®åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
        """
        if not code_meta_data:
            return ""
        
        formatted_parts = []
        for meta in code_meta_data:
            if 'code_key' in meta and 'code_value' in meta:
                part = f"**{meta['code_key']}**:\n{meta['code_value']}"
                if 'code_file' in meta:
                    part += f"\n(æ–‡ä»¶: {meta['code_file']})"
                formatted_parts.append(part)
        
        return "\n\n".join(formatted_parts)
    
    def create_training_prompt(self, record: Dict) -> str:
        """
        æ ¹æ®è®°å½•åˆ›å»ºè®­ç»ƒæç¤ºè¯
        
        Args:
            record: å•æ¡ORMè®°å½•
            
        Returns:
            æ ¼å¼åŒ–çš„æç¤ºè¯
        """
        function_name = record.get('function_name', 'æœªçŸ¥å‡½æ•°')
        orm_code = record.get('orm_code', '')
        caller = record.get('caller', '')
        callee = record.get('callee', '')  # å¦‚æœæœ‰è¢«è°ƒç”¨è€…ä¿¡æ¯
        code_meta_data = record.get('code_meta_data', [])
        
        # æ ¼å¼åŒ–å…ƒæ•°æ®
        code_meta_data_str = self.format_code_metadata(code_meta_data)
        
        # æ„å»ºæç¤ºè¯
        prompt = f"""è¯·åŸºäºä»¥ä¸‹åˆ†æè¦æ±‚ï¼Œç›´æ¥è¾“å‡ºGORMä»£ç å¯¹åº”çš„SQLè¯­å¥JSONæ ¼å¼ç»“æœï¼š

**é¦–è¦åˆ¤æ–­ï¼šSQLç”Ÿæˆæœ‰æ•ˆæ€§**
åœ¨å¼€å§‹åˆ†æå‰ï¼Œè¯·åˆ¤æ–­ç»™å®šçš„ORMä»£ç æ˜¯å¦çœŸçš„ä¼šç”ŸæˆSQLè¯­å¥ï¼š
- ä»£ç å¿…é¡»åŒ…å«å®é™…çš„æ•°æ®åº“æ“ä½œæ–¹æ³•ï¼ˆFindã€Createã€Updateã€Deleteã€Countã€Firstç­‰ï¼‰
- ä»…æœ‰æŸ¥è¯¢æ„å»ºæ–¹æ³•ï¼ˆWhereã€Selectã€Joinç­‰ï¼‰è€Œæ²¡æœ‰æ‰§è¡Œæ–¹æ³•çš„ä»£ç ä¸ä¼šç”ŸæˆSQL
- å¦‚æœä»£ç ä¸ä¼šç”Ÿæˆä»»ä½•SQLï¼Œè¯·è¿”å›ç©ºæ•°ç»„[]

**åˆ†ææ­¥éª¤ï¼š**
1. **è¯†åˆ«è¡¨åå’Œå­—æ®µæ˜ å°„**ï¼š
   **è¡¨åä¼˜å…ˆçº§ï¼š**
   Â· å…ƒæ•°æ®ä¸­TableName()å‡½æ•°æ˜¾å¼è¿”å›å€¼ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
   Â· é…ç½®æ–‡ä»¶ä¸­çš„è¡¨åæ˜ å°„ï¼ˆconstå¸¸é‡ã€typeå®šä¹‰ç­‰ï¼‰
   Â· ä»£ç ä¸­ç›´æ¥å†™å‡ºçš„è¡¨åï¼ˆå¦‚Table("user_info")ï¼‰- å¿…é¡»åŸæ ·ä¿ç•™
   Â· é»˜è®¤å‘½åè§„åˆ™ï¼šé©¼å³°è½¬ä¸‹åˆ’çº¿ï¼Œä¸¥ç¦è‡ªåŠ¨å¤æ•°åŒ–ï¼ˆUserInfoâ†’user_infoï¼Œä¸æ˜¯user_infosï¼‰
   
   **å­—æ®µåä¼˜å…ˆçº§ï¼š**
   Â· ç»“æ„ä½“tagä¸­çš„columnæ ‡ç­¾ï¼ˆå¦‚gorm:"column:user_name"ï¼‰
   Â· é…ç½®æ–‡ä»¶ä¸­çš„å­—æ®µæ˜ å°„
   Â· ä»£ç ä¸­ç›´æ¥å†™å‡ºçš„å­—æ®µåï¼ˆå¦‚Where("user_id = ?")ï¼‰- å¿…é¡»åŸæ ·ä¿ç•™
   Â· é»˜è®¤è½¬æ¢ï¼šé©¼å³°è½¬ä¸‹åˆ’çº¿ï¼ˆUserNameâ†’user_nameï¼‰

2. **å¤„ç†JOINæ“ä½œå’Œè¡¨åˆ«å**ï¼š
   Â· ä¸»è¡¨ä½¿ç”¨ç®€çŸ­åˆ«åï¼Œå…³è”è¡¨ä½¿ç”¨æœ‰æ„ä¹‰çš„åˆ«å
   Â· SELECTã€WHEREã€ORDER BYã€GROUP BYã€HAVINGå­å¥ä¸­çš„æ‰€æœ‰åˆ—åå¿…é¡»å¸¦è¡¨åˆ«åå‰ç¼€
   Â· ONæ¡ä»¶å¿…é¡»ä½¿ç”¨å®Œæ•´æ ¼å¼ï¼š`ON t1.foreign_key = t2.primary_key`
   Â· ç¡®ä¿é¿å…åˆ—åæ­§ä¹‰ï¼Œä¿æŒè¡¨åˆ«åä¸€è‡´æ€§

3. **æšä¸¾æ‰€æœ‰å¯èƒ½çš„SQLç»“æ„**ï¼š
   Â· **å¿½ç•¥æ³¨é‡Šä»£ç **ï¼šå®Œå…¨å¿½ç•¥//å’Œ/* */æ³¨é‡Šä¸­çš„æ‰€æœ‰ä»£ç 
   Â· åˆ†ææ‰€æœ‰å¯èƒ½çš„WHEREæ¡ä»¶å­—æ®µç»„åˆï¼ˆå•æ¡ä»¶ã€å¤šæ¡ä»¶ANDã€ORç»„åˆï¼‰
   Â· è€ƒè™‘åŠ¨æ€æ¡ä»¶æ„å»ºï¼ˆifåˆ¤æ–­ã€å¾ªç¯éå†ã€switchåˆ†æ”¯ç­‰ï¼‰
   Â· è¯†åˆ«GORMç‰¹æ€§å½±å“ï¼ˆå…³è”æŸ¥è¯¢ã€ä½œç”¨åŸŸã€äº‹åŠ¡ã€è½¯åˆ /ç¡¬åˆ ç­‰ï¼‰
   Â· DELETEæ“ä½œéœ€åŒ…å«æ˜¾å¼Whereæ¡ä»¶ï¼‹ä¸»é”®è‡ªåŠ¨æ¡ä»¶

4. **ä¸Šä¸‹æ–‡çº¦æŸåˆ†æ**ï¼ˆæ ¹æ®æä¾›çš„ä¿¡æ¯è¿›è¡Œï¼‰ï¼š
   Â· å¦‚æœæä¾›è°ƒç”¨è€…ä¿¡æ¯ï¼šåªåˆ†æå½“å‰è°ƒç”¨è€…è§¦å‘çš„æ‰§è¡Œè·¯å¾„ï¼Œæ’é™¤å…¶ä»–ç‹¬ç«‹è·¯å¾„
   Â· å¦‚æœæä¾›è¢«è°ƒç”¨è€…ä¿¡æ¯ï¼šè€ƒè™‘å†…éƒ¨è°ƒç”¨å¯èƒ½äº§ç”Ÿçš„é¢å¤–SQLæ“ä½œ
   Â· å¦‚æœä¿¡æ¯ä¸å®Œæ•´ï¼šåŸºäºç°æœ‰ä¿¡æ¯è¿›è¡Œæœ€ä½³æ¨æ–­ï¼Œä½†ä¸è‡†æµ‹ç¼ºå¤±éƒ¨åˆ†

5. **ç”Ÿæˆæ ‡å‡†SQLè¯­å¥**ï¼š
   Â· ç¡®ä¿SQLå®Œæ•´å¯æ‰§è¡Œï¼Œå‚æ•°ç”¨?å ä½
   Â· ä¸å«çœç•¥å·æˆ–[å…¶ä»–å­—æ®µ]ç­‰å ä½ç¬¦
   Â· æ¯æ¡SQLä»¥åˆ†å·ç»“å°¾
   Â· åŒç»“æ„SQLä»…ä¿ç•™ä¸€æ¡ä»£è¡¨æ€§æ¨¡æ¿

**è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š**
è¾“å‡ºæ ‡å‡†JSONæ•°ç»„ï¼Œç»“æ„å¦‚ä¸‹ï¼š
[
  "å›ºå®šSQLè¯­å¥;",
  {{
    "type": "param_dependent",
    "variants": [
      {{"scenario": "æ¡ä»¶æè¿°", "sql": "å®Œæ•´SQLè¯­å¥;"}},
      {{"scenario": "æ¡ä»¶æè¿°", "sql": "å®Œæ•´SQLè¯­å¥;"}}
    ]
  }},
  "å¦ä¸€ä¸ªå›ºå®šSQL;"
]

**ä¸¥æ ¼è¦æ±‚ï¼š**
- ä»…è¾“å‡ºçº¯JSONæ•°ç»„ï¼Œæ— å…¶ä»–æ–‡å­—è¯´æ˜
- SQLè¯­å¥å¿…é¡»å®Œæ•´å¯æ‰§è¡Œï¼Œä»¥åˆ†å·ç»“å°¾
- ä¸å«çœç•¥å·ã€å ä½ç¬¦æˆ–è§£é‡Šæ€§æ–‡æœ¬
- å‚æ•°ä½¿ç”¨é—®å·(?)è¡¨ç¤º
- åªæœ‰SQLç»“æ„ä¸åŒæ‰è§†ä¸ºä¸åŒå˜ä½“

**åˆ†æç›®æ ‡ä»£ç ï¼š**
å‡½æ•°åç§°ï¼š{function_name}
{orm_code}

**å…ƒæ•°æ®ä¿¡æ¯ï¼š**
ä»¥ä¸‹å…ƒæ•°æ®å¯èƒ½åŒ…å«è¡¨åå’Œåˆ—åçš„å…³é”®ä¿¡æ¯ï¼Œè¯·æ ¹æ®å®é™…æä¾›çš„å†…å®¹è¿›è¡Œåˆ†æï¼š

Â· **è¡¨ç»“æ„ä¿¡æ¯**ï¼ˆå¦‚æä¾›ï¼‰ï¼šæ•°æ®åº“è¡¨çš„å®šä¹‰ã€å­—æ®µæ ‡ç­¾ã€ä¸»é”®ä¿¡æ¯ç­‰ï¼Œç”¨äºç¡®å®šå‡†ç¡®çš„è¡¨åå’Œå­—æ®µå
Â· **è°ƒç”¨è€…ä»£ç **ï¼ˆå¦‚æä¾›ï¼‰ï¼šä¸Šå±‚å‡½æ•°çš„è°ƒç”¨æ–¹å¼ã€ä¼ é€’å‚æ•°ã€ä¸šåŠ¡æ¡ä»¶ç­‰ï¼Œç”¨äºé™å®šæ‰§è¡Œè·¯å¾„
Â· **è¢«è°ƒç”¨è€…ä»£ç **ï¼ˆå¦‚æä¾›ï¼‰ï¼šå†…éƒ¨è°ƒç”¨çš„å‡½æ•°ã€åµŒå¥—æŸ¥è¯¢ã€å›è°ƒæ–¹æ³•ç­‰ï¼Œå¯èƒ½äº§ç”Ÿé¢å¤–SQL

**æ³¨æ„**ï¼šå¦‚æœæŸç±»ä¿¡æ¯æœªæä¾›ï¼Œè¯·åŸºäºORMä»£ç æœ¬èº«å’Œå·²æœ‰ä¿¡æ¯è¿›è¡Œåˆ†æï¼Œä¸è¦ä¸ºç¼ºå¤±ä¿¡æ¯åˆ›é€ å‡è®¾ã€‚

è°ƒç”¨è€…ï¼š{caller}
å…ƒæ•°æ®ï¼š{code_meta_data_str}
è¢«è°ƒç”¨è€…ï¼š{callee if callee else ''}
**æœ€ç»ˆè¦æ±‚ï¼šä»…è¾“å‡ºçº¯JSONæ•°ç»„ï¼Œæ— å…¶ä»–æ–‡å­—è¯´æ˜ã€‚**"""

        return prompt.strip()
    
    def create_training_response(self, record: Dict) -> str:
        """
        åˆ›å»ºè®­ç»ƒå“åº”ï¼ˆæ ‡å‡†ç­”æ¡ˆï¼‰
        
        Args:
            record: å•æ¡ORMè®°å½•
            
        Returns:
            JSONæ ¼å¼çš„SQLè¯­å¥åˆ—è¡¨
        """
        sql_statement_list = record.get('sql_statement_list', [])
        return json.dumps(sql_statement_list, ensure_ascii=False, indent=None)
    
    def convert_to_training_format(self, data: List[Dict]) -> List[Dict]:
        """
        å°†ORMæ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
        
        Args:
            data: workflowå¤„ç†åçš„æ•°æ®
            
        Returns:
            è½¬æ¢åçš„è®­ç»ƒæ•°æ®
        """
        training_data = []
        
        logger.info("å¼€å§‹è½¬æ¢è®­ç»ƒæ•°æ®...")
        
        for i, record in enumerate(data):
            if i % 1000 == 0:
                logger.info(f"å·²å¤„ç† {i}/{len(data)} æ¡è®°å½•")
            
            try:
                # åˆ›å»ºæç¤ºè¯å’Œå“åº”
                prompt = self.create_training_prompt(record)
                response = self.create_training_response(record)
                
                # æ„å»ºè®­ç»ƒæ ·æœ¬
                training_sample = {
                    "instruction": prompt,
                    "output": response
                }
                
                # å¯é€‰ï¼šæ·»åŠ é¢å¤–çš„å…ƒä¿¡æ¯ç”¨äºè°ƒè¯•
                training_sample["metadata"] = {
                    "function_name": record.get('function_name', ''),
                    "source_file": record.get('source_file', ''),
                    "sql_pattern_cnt": record.get('sql_pattern_cnt', 0),
                    "sql_types": record.get('sql_types', [])
                }
                
                training_data.append(training_sample)
                
            except Exception as e:
                logger.error(f"å¤„ç†ç¬¬ {i} æ¡è®°å½•æ—¶å‡ºé”™: {e}")
                continue
        
        logger.info(f"è½¬æ¢å®Œæˆï¼Œå…±ç”Ÿæˆ {len(training_data)} æ¡è®­ç»ƒæ ·æœ¬")
        return training_data
    
    def save_training_data(self, training_data: List[Dict], output_name: Optional[str] = None) -> Path:
        """
        ä¿å­˜è®­ç»ƒæ•°æ®
        
        Args:
            training_data: è½¬æ¢åçš„è®­ç»ƒæ•°æ®
            output_name: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if output_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_name = f"orm2sql_training_data_{timestamp}.json"
        
        output_path = self.training_data_dir / output_name
        
        logger.info(f"æ­£åœ¨ä¿å­˜è®­ç»ƒæ•°æ®åˆ°: {output_path}")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        # è®¡ç®—æ–‡ä»¶å¤§å°
        file_size = output_path.stat().st_size / (1024 * 1024)
        logger.info(f"è®­ç»ƒæ•°æ®ä¿å­˜å®Œæˆï¼Œæ–‡ä»¶å¤§å°: {file_size:.1f} MB")
        
        return output_path
    
    def create_dataset_info(self, training_data: List[Dict], dataset_name: str) -> Dict:
        """
        åˆ›å»ºæ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
        
        Args:
            training_data: è®­ç»ƒæ•°æ®
            dataset_name: æ•°æ®é›†åç§°
            
        Returns:
            æ•°æ®é›†ä¿¡æ¯å­—å…¸
        """
        return {
            dataset_name: {
                "file_name": f"{dataset_name}.json",
                "columns": {
                    "prompt": "instruction",
                    "response": "output"
                },
                "file_sha1": "",  # å¯ä»¥åç»­è®¡ç®—
                "num_samples": len(training_data),
                "description": "ORMåˆ°SQLè½¬æ¢è®­ç»ƒæ•°æ®é›†ï¼ŒåŸºäºçœŸå®ä»£ç åˆ†æç”Ÿæˆ"
            }
        }
    
    def run_conversion(self, workflow_dir: Optional[Path] = None, output_name: Optional[str] = None) -> Tuple[Path, Dict]:
        """
        æ‰§è¡Œå®Œæ•´çš„æ•°æ®è½¬æ¢æµç¨‹
        
        Args:
            workflow_dir: æŒ‡å®šçš„workflowç›®å½•ï¼ˆå¯é€‰ï¼‰
            output_name: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
            
        Returns:
            (è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„, æ•°æ®é›†ä¿¡æ¯)
        """
        # 1. æŸ¥æ‰¾æˆ–ä½¿ç”¨æŒ‡å®šçš„workflowç›®å½•
        if workflow_dir is None:
            workflow_dir = self.find_latest_workflow_output()
            if workflow_dir is None:
                raise FileNotFoundError("æœªæ‰¾åˆ°workflowè¾“å‡ºç›®å½•")
        
        # 2. åŠ è½½æ•°æ®
        data = self.load_workflow_data(workflow_dir)
        
        # 3. è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
        training_data = self.convert_to_training_format(data)
        
        # 4. ä¿å­˜è®­ç»ƒæ•°æ®
        output_path = self.save_training_data(training_data, output_name)
        
        # 5. åˆ›å»ºæ•°æ®é›†ä¿¡æ¯
        dataset_name = output_path.stem
        dataset_info = self.create_dataset_info(training_data, dataset_name)
        
        # 6. ä¿å­˜æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
        info_path = self.training_data_dir / "dataset_info.json"
        if info_path.exists():
            with open(info_path, 'r', encoding='utf-8') as f:
                existing_info = json.load(f)
            existing_info.update(dataset_info)
        else:
            existing_info = dataset_info
        
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(existing_info, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ•°æ®é›†ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")
        
        return output_path, dataset_info


def main():
    """ä¸»å‡½æ•°"""
    converter = TrainingDataConverter()
    
    try:
        # æ‰§è¡Œè½¬æ¢
        output_path, dataset_info = converter.run_conversion()
        
        print(f"\nâœ… æ•°æ®è½¬æ¢å®Œæˆ!")
        print(f"ğŸ“ è®­ç»ƒæ•°æ®ä¿å­˜è·¯å¾„: {output_path}")
        print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {dataset_info[list(dataset_info.keys())[0]]['num_samples']}")
        print(f"ğŸ“ æ•°æ®é›†ä¿¡æ¯: {converter.training_data_dir / 'dataset_info.json'}")
        
    except Exception as e:
        logger.error(f"æ•°æ®è½¬æ¢å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main() 