#!/usr/bin/env python3
"""
RLè®­ç»ƒæ•°æ®è½¬æ¢å™¨

å°†è®­ç»ƒæ•°æ®è½¬æ¢ä¸ºRLHFè®­ç»ƒæ ¼å¼ï¼Œè¾“å‡ºä¸ºparquetæ–‡ä»¶
"""

import json
import os
import logging
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import glob
import sys
import asyncio
from tqdm import tqdm
import yaml

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# ç°åœ¨å¯ä»¥å¯¼å…¥é¡¹ç›®å†…çš„æ¨¡å—
from config.rl.data_conversion.orm2sql_prompt_template import PROMPT_TEMPLATE
from utils.preprocess import preprocess_record

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RLDataConverter:
    """RLè®­ç»ƒæ•°æ®è½¬æ¢å™¨"""
    
    def __init__(self, project_root: Optional[str] = None, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        if project_root is None:
            self.project_root = Path(__file__).parent.parent.parent
        else:
            self.project_root = Path(project_root)
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        if config_path is None:
            config_path = self.project_root / "config" / "rl" / "data_conversion" / "conversion_config.yaml"
        
        self.config = self.load_config(config_path)
        
        # åˆ›å»ºRLæ•°æ®ç›®å½•
        self.rl_data_dir = self.project_root / "model" / "data" / "orm2sql_rl_data"
        self.rl_data_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"RLæ•°æ®è½¬æ¢å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")
        logger.info(f"RLæ•°æ®ç›®å½•: {self.rl_data_dir}")
        logger.info(f"é…ç½®æ–‡ä»¶: {config_path}")
    
    def load_config(self, config_path: Path) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not config_path.exists():
            logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return {}
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            return config
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def find_latest_workflow_output(self) -> Optional[Path]:
        """æŸ¥æ‰¾æœ€æ–°çš„workflowè¾“å‡ºç›®å½•"""
        workflow_dir = self.project_root / "workflow_output"
        if not workflow_dir.exists():
            return None
        
        # æŸ¥æ‰¾æœ€æ–°çš„å­ç›®å½•
        subdirs = [d for d in workflow_dir.iterdir() if d.is_dir()]
        if not subdirs:
            return None
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
        latest_dir = max(subdirs, key=lambda d: d.stat().st_mtime)
        logger.info(f"æ‰¾åˆ°æœ€æ–°workflowè¾“å‡º: {latest_dir}")
        return latest_dir

    def load_workflow_data(self, workflow_dir: Path) -> List[Dict]:
        """åŠ è½½workflowå¤„ç†åçš„æ•°æ®"""
        # ä»é…ç½®ä¸­è·å–æ–‡ä»¶åï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼
        final_data_filename = self.config.get('input', {}).get('final_data_filename', 'final_processed_dataset.json')
        
        # å°è¯•å¤šä¸ªå¯èƒ½çš„æ•°æ®æ–‡ä»¶å
        possible_files = [
            workflow_dir / final_data_filename,
            workflow_dir / "final_processed_dataset.json"  # ä¿ç•™é»˜è®¤æ–‡ä»¶åä½œä¸ºå¤‡é€‰
        ]
        
        data_file = None
        for file_path in possible_files:
            if file_path.exists():
                data_file = file_path
                break
        
        if not data_file:
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•è¿‡çš„æ–‡ä»¶: {[str(f) for f in possible_files]}")
        
        logger.info(f"ä½¿ç”¨æ•°æ®æ–‡ä»¶: {data_file}")
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åå†³å®šè¯»å–æ–¹å¼
        if data_file.suffix == '.jsonl':
            # JSONLæ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡
            data = []
            with open(data_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data.append(json.loads(line))
        else:
            # JSONæ ¼å¼ï¼šæ•´ä¸ªæ–‡ä»¶æ˜¯ä¸€ä¸ªJSONæ•°ç»„
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        
        logger.info(f"åŠ è½½äº† {len(data)} æ¡æ•°æ®è®°å½•")
        return data

    def format_code_metadata(self, code_meta_data: List[dict]) -> str:
        """æ ¼å¼åŒ–ä»£ç å…ƒæ•°æ®ä¸ºå­—ç¬¦ä¸²"""
        if not code_meta_data:
            return ""
        
        formatted_parts = []
        for meta in code_meta_data:
            if isinstance(meta, dict):
                # æå–å…³é”®ä¿¡æ¯
                code_key = meta.get('code_key', meta.get('key', meta.get('name', '')))
                code_value = meta.get('code_value', meta.get('value', meta.get('content', '')))
                code_file = meta.get('code_file', meta.get('file', meta.get('source', '')))
                
                if code_key and code_value:
                    part = f"**{code_key}**: {code_value}"
                    if code_file:
                        part += f" (æ–‡ä»¶: {code_file})"
                    formatted_parts.append(part)
            else:
                # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œç›´æ¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                formatted_parts.append(str(meta))
        
        return "\n".join(formatted_parts)

    def create_rl_prompt(self, record: Dict) -> List[Dict]:
        """åˆ›å»ºRLè®­ç»ƒç”¨çš„èŠå¤©æ ¼å¼æç¤ºè¯"""
        # æå–åŸºæœ¬ä¿¡æ¯
        function_name = record.get('function_name', 'N/A')
        orm_code = record.get('orm_code', '')
        caller = record.get('caller', '')
        code_meta_data = record.get('code_meta_data', [])
        
        # æ ¼å¼åŒ–ä»£ç å…ƒæ•°æ®
        meta_data_str = self.format_code_metadata(code_meta_data)
        
        # ä½¿ç”¨orm2sql_prompt_templateä¸­çš„æ¨¡æ¿
        user_content = PROMPT_TEMPLATE.format(
            function_name=function_name,
            code_value=orm_code,
            caller=caller,
            code_meta_data_str=meta_data_str
        )

        return [
            {"role": "user", "content": user_content}
        ]

    def extract_ground_truth(self, record: Dict) -> str:
        """æå–æ ‡å‡†ç­”æ¡ˆï¼ˆSQLè¯­å¥ï¼‰"""
        # ä»è®°å½•ä¸­æå–SQLè¯­å¥
        sql_statements = record.get('sql_statements', [])
        if not sql_statements:
            return "[]"
        
        # è¿”å›JSONæ ¼å¼çš„SQLè¯­å¥æ•°ç»„
        return json.dumps(sql_statements, ensure_ascii=False)

    async def process_single_record(self, record: Dict, index: int) -> Optional[Dict]:
        """å¤„ç†å•æ¡è®°å½•ï¼ˆå¼‚æ­¥ï¼‰"""
        try:
            # é¢„å¤„ç†æ­¥éª¤ï¼ˆä»…è¡¨åå­—æ®µåæŠ½å–ï¼‰
            ok, pre_tables, pre_columns = await preprocess_record(record)
            if not ok:
                return None
            
            # åˆ›å»ºèŠå¤©æ ¼å¼çš„æç¤ºè¯
            prompt = self.create_rl_prompt(record)
            
            # æå–æ ‡å‡†ç­”æ¡ˆ
            ground_truth = self.extract_ground_truth(record)
            
            # æ„å»ºreward_modelé…ç½®
            reward_model = {
                "style": "rule",  # ä½¿ç”¨è§„åˆ™è¯„åˆ†ï¼Œä¸æ˜¯æ¨¡å‹è¯„åˆ†
                "ground_truth": ground_truth
            }
            
            # æ„å»ºextra_infoï¼ŒåŒ…å«æ‰€æœ‰ORMç›¸å…³ä¿¡æ¯
            extra_info = {
                "index": index,
                "split": "train",  # é»˜è®¤ä¸ºè®­ç»ƒé›†
                "function_name": record.get('function_name', ''),
                "source_file": record.get('source_file', ''),
                "sql_pattern_cnt": record.get('sql_pattern_cnt', 0),
                "sql_types": record.get('sql_types', []),
                # ä¿æŒåŸæœ‰ORMä¿¡æ¯
                "orm_code": record.get('orm_code', ''),
                "caller": record.get('caller', ''),
                "callee": record.get('callee', ''),
                "code_meta_data": record.get('code_meta_data', []),
                # é¢„å¤„ç†çš„è¡¨åå­—æ®µåç»“æœ
                "pre_tables": list(pre_tables),
                "pre_columns": list(pre_columns),
                # ä¿æŒåŸæœ‰å…³é”®è¯ä¿¡æ¯ä¸å˜
                "llm_keyword_analysis": record.get("llm_keyword_analysis", {})
            }
            
            return {
                "data_source": "code2sql_orm",
                "prompt": prompt,
                "ability": "code_generation",
                "reward_model": reward_model,
                "extra_info": extra_info
            }
            
        except Exception as e:
            logger.error(f"å¤„ç†ç¬¬ {index} æ¡è®°å½•æ—¶å‡ºé”™: {e}")
            return None

    async def convert_to_rl_format(self, data: List[Dict]) -> pd.DataFrame:
        """
        å°†ORMæ•°æ®è½¬æ¢ä¸ºRLè®­ç»ƒæ ¼å¼ï¼ˆå¹¶å‘å¤„ç†ï¼‰
        
        Args:
            data: workflowå¤„ç†åçš„æ•°æ®
            
        Returns:
            è½¬æ¢åçš„RLè®­ç»ƒæ•°æ®DataFrame
        """
        logger.info("å¼€å§‹è½¬æ¢RLè®­ç»ƒæ•°æ®...")
        
        # æ‰“å°ç¬¬ä¸€æ¡æ•°æ®ç”¨äºè°ƒè¯•
        if data:
            logger.info("=== ç¬¬ä¸€æ¡æ•°æ®ç¤ºä¾‹ ===")
            first_record = data[0]
            logger.info(f"function_name: {first_record.get('function_name', 'N/A')}")
            logger.info(f"orm_code: {first_record.get('orm_code', 'N/A')[:100]}...")
            logger.info(f"caller: {first_record.get('caller', 'N/A')}")
            logger.info(f"code_meta_data ç±»å‹: {type(first_record.get('code_meta_data', []))}")
            logger.info(f"code_meta_data é•¿åº¦: {len(first_record.get('code_meta_data', []))}")
            if first_record.get('code_meta_data'):
                logger.info(f"ç¬¬ä¸€æ¡ code_meta_data: {first_record['code_meta_data'][0]}")
                logger.info(f"ç¬¬ä¸€æ¡ code_meta_data ç±»å‹: {type(first_record['code_meta_data'][0])}")
                if isinstance(first_record['code_meta_data'][0], dict):
                    logger.info(f"ç¬¬ä¸€æ¡ code_meta_data é”®: {list(first_record['code_meta_data'][0].keys())}")
            logger.info("=== æ•°æ®ç¤ºä¾‹ç»“æŸ ===")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_records = len(data)
        filtered_count = 0
        has_keywords_count = 0
        
        # è®¾ç½®å¹¶å‘æ•°ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
        max_concurrent = 10
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_with_semaphore(record, index):
            async with semaphore:
                return await self.process_single_record(record, index)
        
        # ä½¿ç”¨è¿›åº¦æ¡å’Œå¹¶å‘å¤„ç†
        results = []
        with tqdm(total=total_records, desc="å¤„ç†è®°å½•", unit="æ¡") as pbar:
            # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜å ç”¨è¿‡å¤§
            batch_size = 100
            for i in range(0, total_records, batch_size):
                batch = data[i:i + batch_size]
                batch_tasks = [process_with_semaphore(record, j) for j, record in enumerate(batch, i)]
                
                # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                
                # å¤„ç†ç»“æœ
                for j, result in enumerate(batch_results):
                    if isinstance(result, Exception):
                        logger.error(f"å¤„ç†ç¬¬ {i + j} æ¡è®°å½•æ—¶å‡ºé”™: {result}")
                        filtered_count += 1
                    elif result is None:
                        filtered_count += 1
                    else:
                        # æ£€æŸ¥æ˜¯å¦æœ‰å…³é”®è¯
                        original_record = data[i + j]
                        if original_record.get("llm_keyword_analysis", {}).get("has_special_keywords", False):
                            has_keywords_count += 1
                        results.append(result)
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.update(len(batch))
                pbar.set_postfix({
                    'å·²å¤„ç†': f"{i + len(batch)}/{total_records}",
                    'ä¿ç•™': len(results),
                    'è¿‡æ»¤': filtered_count,
                    'å¹¶å‘æ•°': max_concurrent
                })
        
        # è½¬æ¢ä¸ºDataFrame
        if not results:
            logger.warning("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•è®°å½•")
            return pd.DataFrame()
        
        rl_data = {
            "data_source": [r["data_source"] for r in results],
            "prompt": [r["prompt"] for r in results],
            "ability": [r["ability"] for r in results],
            "reward_model": [r["reward_model"] for r in results],
            "extra_info": [r["extra_info"] for r in results]
        }
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        final_count = len(results)
        logger.info(f"=== é¢„å¤„ç†ç»Ÿè®¡ä¿¡æ¯ ===")
        logger.info(f"åŸå§‹æ ·æœ¬æ•°: {total_records}")
        logger.info(f"è¿‡æ»¤æ ·æœ¬æ•°: {filtered_count}")
        logger.info(f"ä¿ç•™æ ·æœ¬æ•°: {final_count}")
        logger.info(f"ä¿ç•™ç‡: {final_count/total_records*100:.1f}%")
        logger.info(f"æœ‰å…³é”®è¯æ ·æœ¬æ•°: {has_keywords_count}")
        if final_count > 0:
            logger.info(f"å…³é”®è¯æ ·æœ¬å æ¯”: {has_keywords_count/final_count*100:.1f}%")
        
        logger.info(f"è½¬æ¢å®Œæˆï¼Œå…±ç”Ÿæˆ {final_count} æ¡RLè®­ç»ƒæ ·æœ¬")
        
        # æ‰“å°å‰3æ¡è½¬æ¢åçš„æ•°æ®ç¤ºä¾‹
        logger.info("=== è½¬æ¢åçš„RLè®­ç»ƒæ•°æ®ç¤ºä¾‹ ===")
        for i, record in enumerate(results[:3]):
            logger.info(f"--- ç¬¬ {i+1} æ¡æ•°æ® ---")
            logger.info(f"data_source: {record.get('data_source', 'N/A')}")
            logger.info(f"ability: {record.get('ability', 'N/A')}")
            
            # æ‰“å°promptå†…å®¹ï¼ˆæˆªå–å‰200å­—ç¬¦ï¼‰
            prompt = record.get('prompt', [])
            if prompt and len(prompt) > 0:
                user_content = prompt[0].get('content', '')
                logger.info(f"promptå†…å®¹é¢„è§ˆ: {user_content[:200]}...")
            
            # æ‰“å°reward_model
            reward_model = record.get('reward_model', {})
            logger.info(f"reward_model: {reward_model}")
            
            # æ‰“å°extra_infoçš„å…³é”®ä¿¡æ¯
            extra_info = record.get('extra_info', {})
            logger.info(f"function_name: {extra_info.get('function_name', 'N/A')}")
            logger.info(f"pre_tables: {extra_info.get('pre_tables', [])}")
            logger.info(f"pre_columns: {extra_info.get('pre_columns', [])}")
            logger.info("")
        
        logger.info("=== æ•°æ®ç¤ºä¾‹ç»“æŸ ===")
        
        return pd.DataFrame(rl_data)
    
    def split_train_val(self, df: pd.DataFrame, val_ratio: float = 0.1) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        
        Args:
            df: å®Œæ•´æ•°æ®é›†DataFrame
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            
        Returns:
            (è®­ç»ƒé›†DataFrame, éªŒè¯é›†DataFrame)
        """
        # éšæœºæ‰“ä¹±æ•°æ®
        df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        # è®¡ç®—åˆ’åˆ†ç‚¹
        val_size = int(len(df_shuffled) * val_ratio)
        
        val_df = df_shuffled.iloc[:val_size].copy()
        train_df = df_shuffled.iloc[val_size:].copy()
        
        # æ›´æ–°splitæ ‡è®°
        train_df.loc[:, 'extra_info'] = train_df['extra_info'].apply(
            lambda x: {**x, 'split': 'train'}
        )
        val_df.loc[:, 'extra_info'] = val_df['extra_info'].apply(
            lambda x: {**x, 'split': 'val'}
        )
        
        logger.info(f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ: è®­ç»ƒé›† {len(train_df)} æ¡, éªŒè¯é›† {len(val_df)} æ¡")
        return train_df, val_df
    
    def save_rl_data(self, train_df: pd.DataFrame, val_df: pd.DataFrame, 
                     output_name: Optional[str] = None) -> Tuple[Path, Path]:
        """
        ä¿å­˜RLè®­ç»ƒæ•°æ®ä¸ºparquetæ ¼å¼
        
        Args:
            train_df: è®­ç»ƒé›†DataFrame
            val_df: éªŒè¯é›†DataFrame
            output_name: è¾“å‡ºæ–‡ä»¶åå‰ç¼€ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            (è®­ç»ƒé›†æ–‡ä»¶è·¯å¾„, éªŒè¯é›†æ–‡ä»¶è·¯å¾„)
        """
        if output_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_name = f"orm2sql_rl_{timestamp}"
        
        train_path = self.rl_data_dir / f"{output_name}_train.parquet"
        val_path = self.rl_data_dir / f"{output_name}_val.parquet"
        
        logger.info(f"æ­£åœ¨ä¿å­˜è®­ç»ƒé›†åˆ°: {train_path}")
        train_df.to_parquet(train_path, index=False)
        train_size = train_path.stat().st_size / (1024 * 1024)
        logger.info(f"è®­ç»ƒé›†ä¿å­˜å®Œæˆï¼Œæ–‡ä»¶å¤§å°: {train_size:.1f} MB")
        
        logger.info(f"æ­£åœ¨ä¿å­˜éªŒè¯é›†åˆ°: {val_path}")
        val_df.to_parquet(val_path, index=False)
        val_size = val_path.stat().st_size / (1024 * 1024)
        logger.info(f"éªŒè¯é›†ä¿å­˜å®Œæˆï¼Œæ–‡ä»¶å¤§å°: {val_size:.1f} MB")
        
        return train_path, val_path
    
    def create_dataset_info(self, train_df: pd.DataFrame, val_df: pd.DataFrame, 
                           dataset_name: str) -> Dict:
        """
        åˆ›å»ºæ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
        
        Args:
            train_df: è®­ç»ƒé›†DataFrame
            val_df: éªŒè¯é›†DataFrame
            dataset_name: æ•°æ®é›†åç§°
            
        Returns:
            æ•°æ®é›†ä¿¡æ¯å­—å…¸
        """
        return {
            "dataset_name": dataset_name,
            "description": "ORMåˆ°SQLè½¬æ¢çš„RLè®­ç»ƒæ•°æ®é›†ï¼ŒåŸºäºçœŸå®ä»£ç åˆ†æç”Ÿæˆ",
            "data_source": "code2sql_orm",
            "ability": "code_generation",
            "train": {
                "file_name": f"{dataset_name}_train.parquet",
                "num_samples": len(train_df),
                "size_mb": f"{(self.rl_data_dir / f'{dataset_name}_train.parquet').stat().st_size / (1024*1024):.1f}"
            },
            "val": {
                "file_name": f"{dataset_name}_val.parquet",
                "num_samples": len(val_df),
                "size_mb": f"{(self.rl_data_dir / f'{dataset_name}_val.parquet').stat().st_size / (1024*1024):.1f}"
            },
            "total_samples": len(train_df) + len(val_df),
            "reward_model_style": "rule",
            "format": "RLHF parquet format with chat template"
        }
    
    async def run_conversion(self, workflow_dir: Optional[Path] = None, output_name: Optional[str] = None,
                      val_ratio: float = 0.1) -> Tuple[Path, Path, Dict]:
        """
        è¿è¡Œæ•°æ®è½¬æ¢æ­¥éª¤
        
        Args:
            workflow_dir: workflowè¾“å‡ºç›®å½•
            output_name: è¾“å‡ºæ–‡ä»¶å
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
        Returns:
            (è®­ç»ƒé›†è·¯å¾„, éªŒè¯é›†è·¯å¾„, æ•°æ®é›†ä¿¡æ¯)
        """
        # 1. æ ¹æ®é…ç½®ç¡®å®šworkflowç›®å½•
        if workflow_dir is None:
            use_latest_workflow = self.config.get('input', {}).get('use_latest_workflow', True)
            
            if use_latest_workflow:
                # ä½¿ç”¨æœ€æ–°çš„workflowè¾“å‡º
                workflow_dir = self.find_latest_workflow_output()
                if workflow_dir is None:
                    raise FileNotFoundError("æœªæ‰¾åˆ°workflowè¾“å‡ºç›®å½•")
                logger.info(f"ä½¿ç”¨æœ€æ–°workflowè¾“å‡º: {workflow_dir}")
            else:
                # ä½¿ç”¨æŒ‡å®šçš„workflowç›®å½•
                specific_dir = self.config.get('input', {}).get('specific_workflow_dir')
                if specific_dir:
                    workflow_dir = Path(specific_dir)
                    if not workflow_dir.exists():
                        raise FileNotFoundError(f"æŒ‡å®šçš„workflowç›®å½•ä¸å­˜åœ¨: {workflow_dir}")
                    logger.info(f"ä½¿ç”¨æŒ‡å®šworkflowç›®å½•: {workflow_dir}")
                else:
                    raise ValueError("é…ç½®ä¸­use_latest_workflowä¸ºfalseä½†æœªæŒ‡å®šspecific_workflow_dir")
        
        # 2. åŠ è½½æ•°æ®
        data = self.load_workflow_data(workflow_dir)
        
        # 3. è½¬æ¢ä¸ºRLæ ¼å¼
        rl_df = await self.convert_to_rl_format(data)
        
        # 4. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_df, val_df = self.split_train_val(rl_df, val_ratio)
        
        # 5. ä¿å­˜æ•°æ®
        if output_name is None:
            # ä»é…ç½®ä¸­è·å–è¾“å‡ºæ–‡ä»¶åå‰ç¼€
            output_prefix = self.config.get('output', {}).get('output_name_prefix', 'orm2sql_rl')
            include_timestamp = self.config.get('output', {}).get('include_timestamp', True)
            
            if include_timestamp:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_name = f"{output_prefix}_{timestamp}"
            else:
                output_name = output_prefix
        
        train_path, val_path = self.save_rl_data(train_df, val_df, output_name)
        
        # 6. åˆ›å»ºæ•°æ®é›†ä¿¡æ¯
        dataset_info = self.create_dataset_info(train_df, val_df, output_name or "dataset")
        
        # 7. ä¿å­˜æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
        info_filename = self.config.get('output', {}).get('dataset_info_filename', f"{output_name}_info.json")
        info_path = self.rl_data_dir / info_filename
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ•°æ®é›†ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")
        
        return train_path, val_path, dataset_info


async def main():
    """ä¸»å‡½æ•°"""
    converter = RLDataConverter()
    
    try:
        # æ‰§è¡Œè½¬æ¢
        train_path, val_path, dataset_info = await converter.run_conversion()
        
        print(f"\nâœ… RLæ•°æ®è½¬æ¢å®Œæˆ!")
        print(f"ğŸ“ è®­ç»ƒé›†ä¿å­˜è·¯å¾„: {train_path}")
        print(f"ğŸ“ éªŒè¯é›†ä¿å­˜è·¯å¾„: {val_path}")
        print(f"ğŸ“Š è®­ç»ƒé›†æ ·æœ¬æ•°: {dataset_info['train']['num_samples']}")
        print(f"ğŸ“Š éªŒè¯é›†æ ·æœ¬æ•°: {dataset_info['val']['num_samples']}")
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {dataset_info['total_samples']}")
        info_file = converter.rl_data_dir / f"{dataset_info['dataset_name']}_info.json"
        print(f"ğŸ“ æ•°æ®é›†ä¿¡æ¯: {info_file}")
        
    except Exception as e:
        logger.error(f"RLæ•°æ®è½¬æ¢å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main()) 