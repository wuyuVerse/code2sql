#!/usr/bin/env python3
"""
SQLæ¸…æ´—å·¥ä½œæµæ¼”ç¤ºè„šæœ¬

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ•°æ®æ¸…æ´—workflowæ¥æ¸…æ´—æå–çš„æ•°æ®ä¸­çš„æ— æ•ˆSQL
"""

import sys
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä»åŸå§‹æ•°æ®é›†çš„å®Œæ•´SQLæ¸…æ´—workflowæ¼”ç¤º...")
    
    try:
        # å¯¼å…¥å·¥ä½œæµç®¡ç†å™¨
        from data_processing import get_workflow_manager
        WorkflowManager, run_complete_sql_cleaning_workflow, run_complete_workflow_from_raw_data = get_workflow_manager()
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŸå§‹æ•°æ®é›†
        raw_data_path = "datasets/claude_output"
        if not Path(raw_data_path).exists():
            print(f"âŒ æœªæ‰¾åˆ°åŸå§‹æ•°æ®ç›®å½•: {raw_data_path}")
            print("è¯·ç¡®ä¿åŸå§‹æ•°æ®é›†å­˜åœ¨")
            return
        
        print(f"ğŸ“ ä½¿ç”¨åŸå§‹æ•°æ®é›†: {raw_data_path}")
        print("ğŸ¯ å°†æ‰§è¡Œï¼šæ•°æ®åŠ è½½ -> å…³é”®è¯æå– -> SQLæ¸…æ´— -> ç»“æœå¯¼å‡º")
        
        # è¿è¡Œä»åŸå§‹æ•°æ®é›†å¼€å§‹çš„å®Œæ•´å·¥ä½œæµ
        result = run_complete_workflow_from_raw_data(
            data_dir=raw_data_path,
            keywords=None,  # ä½¿ç”¨GORMé¢„å®šä¹‰å…³é”®è¯
            base_output_dir="workflow_output"
        )
        
        print("\nâœ… SQLæ¸…æ´—å·¥ä½œæµå®Œæˆ!")
        print(f"ğŸ“Š å·¥ä½œæµç›®å½•: {result['workflow_directory']}")
        print(f"ğŸ“„ æœ€ç»ˆæ•°æ®: {result['final_data_path']}")
        print(f"ğŸ“‹ æ‘˜è¦æ–‡ä»¶: {result['summary_path']}")
        
        # æ˜¾ç¤ºæ¸…æ´—ç»Ÿè®¡
        cleaning_stats = result['cleaning_result']
        print(f"\nğŸ“ˆ æ¸…æ´—ç»Ÿè®¡:")
        print(f"   è¾“å…¥è®°å½•: {cleaning_stats['input_records_count']:,}")
        print(f"   è¾“å‡ºè®°å½•: {cleaning_stats['output_records_count']:,}")
        print(f"   ä¿®æ”¹è®°å½•: {cleaning_stats['records_modified']:,}")
        print(f"   ç§»é™¤æ— æ•ˆSQL: {cleaning_stats['invalid_sql_removed']:,}")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿æ•°æ®å¤„ç†æ¨¡å—æ­£ç¡®å®‰è£…")
    except FileNotFoundError as e:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def run_custom_workflow():
    """è¿è¡Œè‡ªå®šä¹‰å·¥ä½œæµç¤ºä¾‹"""
    print("\nğŸ”§ è¿è¡Œè‡ªå®šä¹‰å·¥ä½œæµç¤ºä¾‹...")
    
    try:
        from data_processing import get_workflow_manager, get_data_cleaner
        WorkflowManager, _, _ = get_workflow_manager()
        SQLCleaner = get_data_cleaner()
        
        # åˆ›å»ºå·¥ä½œæµç®¡ç†å™¨
        workflow = WorkflowManager("custom_workflow_output")
        
        # æ­¥éª¤1: åŠ è½½æ•°æ®
        extracted_data_path = "extracted_data/gorm_keywords_20250703_121119"
        load_result = workflow.load_extracted_data(extracted_data_path)
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {load_result['records_loaded']:,} æ¡è®°å½•")
        
        # æ­¥éª¤2: SQLæ¸…æ´—
        cleaning_result = workflow.run_sql_cleaning("custom_sql_cleaning")
        print(f"âœ… SQLæ¸…æ´—å®Œæˆ: ç§»é™¤äº† {cleaning_result['invalid_sql_removed']:,} ä¸ªæ— æ•ˆSQL")
        
        # æ­¥éª¤3: å¯¼å‡ºå’Œæ€»ç»“
        final_path = workflow.export_final_data("custom_cleaned_data.json")
        summary_path = workflow.save_workflow_summary()
        
        workflow.print_workflow_summary()
        
        print(f"\nğŸ‰ è‡ªå®šä¹‰å·¥ä½œæµå®Œæˆ!")
        print(f"ğŸ“„ æœ€ç»ˆæ•°æ®: {final_path}")
        print(f"ğŸ“‹ æ‘˜è¦æ–‡ä»¶: {summary_path}")
        
    except Exception as e:
        print(f"âŒ è‡ªå®šä¹‰å·¥ä½œæµå¤±è´¥: {e}")


if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--custom":
        run_custom_workflow()
    else:
        main()
        
        # å¯é€‰ï¼šä¹Ÿè¿è¡Œè‡ªå®šä¹‰å·¥ä½œæµç¤ºä¾‹
        user_input = input("\næ˜¯å¦è¿è¡Œè‡ªå®šä¹‰å·¥ä½œæµç¤ºä¾‹? (y/N): ")
        if user_input.lower() in ['y', 'yes']:
            run_custom_workflow() 