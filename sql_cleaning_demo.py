#!/usr/bin/env python3
"""
SQLæ¸…æ´—å·¥ä½œæµæ¼”ç¤ºè„šæœ¬

å±•ç¤ºæ–°æ¶æ„çš„æ•°æ®å¤„ç†å·¥ä½œæµï¼š
1. åŠ è½½åŸå§‹æ•°æ®é›†
2. SQLæ¸…æ´—
3. å…³é”®è¯æå–
4. ç‰¹æ®Šå¤„ç†ï¼ˆé¢„ç•™ï¼‰
5. æ•°æ®åˆå¹¶

ä½¿ç”¨æ–¹æ³•:
python sql_cleaning_demo.py
"""

import sys
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).resolve().parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ æ•°æ®å¤„ç†å·¥ä½œæµæ¼”ç¤º - æ–°æ¶æ„")
    print("=" * 60)
    
    # åŸå§‹æ•°æ®ç›®å½•
    data_dir = "./datasets/claude_output"
    
    if not Path(data_dir).exists():
        print(f"âŒ é”™è¯¯: æ•°æ®ç›®å½• '{data_dir}' ä¸å­˜åœ¨")
        print("è¯·ç¡®ä¿æ•°æ®ç›®å½•åŒ…å«å¿…è¦çš„è¾“å…¥æ–‡ä»¶")
        return 1
    
    choice = None
    if len(sys.argv) > 1:
        choice = sys.argv[1]
        print(f"âœ… å·²é€šè¿‡å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ¨¡å¼: {choice}")
    else:
        # å·¥ä½œæµé€‰é¡¹
        print("è¯·é€‰æ‹©å·¥ä½œæµæ¨¡å¼:")
        print("1. å®Œæ•´æ–°æ¶æ„å·¥ä½œæµï¼ˆæ¨èï¼‰- ä»åŸå§‹æ•°æ®é›†å¼€å§‹")
        print("2. è‡ªå®šä¹‰æ­¥éª¤æ¼”ç¤º - é€æ­¥å±•ç¤ºå„å¤„ç†é˜¶æ®µ")
        print("3. æµ‹è¯•å·¥ä½œæµ - ä½¿ç”¨å°æ ·æœ¬æ•°æ®")
        print("\nğŸ’¡ æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ¨¡å¼, ä¾‹å¦‚: python sql_cleaning_demo.py 1")
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()
    
    try:
        if choice == "1":
            run_complete_new_workflow(data_dir)
        elif choice == "2":
            run_step_by_step_demo(data_dir)
        elif choice == "3":
            run_test_workflow(data_dir)
        else:
            print(f"âŒ æ— æ•ˆé€‰æ‹©: '{choice}'ã€‚å°†è¿è¡Œé»˜è®¤çš„å®Œæ•´å·¥ä½œæµã€‚")
            run_complete_new_workflow(data_dir)
        
        print("\nâœ… å·¥ä½œæµæ¼”ç¤ºå®Œæˆ!")
        return 0
        
    except Exception as e:
        logger.error(f"æ¼”ç¤ºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥æ•°æ®ç›®å½•å’Œä¾èµ–æ˜¯å¦æ­£ç¡®é…ç½®")
        return 1

def run_complete_new_workflow(data_dir: str):
    """è¿è¡Œå®Œæ•´çš„æ–°æ¶æ„å·¥ä½œæµ"""
    print("\nğŸ”„ è¿è¡Œå®Œæ•´æ–°æ¶æ„å·¥ä½œæµ")
    print("-" * 40)
    
    try:
        from data_processing.workflow import run_complete_workflow_from_raw_data
        
        # è¿è¡Œå®Œæ•´å·¥ä½œæµ
        result = run_complete_workflow_from_raw_data(
            data_dir=data_dir,
            keywords=None,  # ä½¿ç”¨é»˜è®¤GORMå…³é”®è¯
            base_output_dir="workflow_output"
        )
        
        print(f"\nğŸ‰ æ–°æ¶æ„å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ!")
        print(f"ğŸ“ å·¥ä½œæµç›®å½•: {result['workflow_directory']}")
        print(f"ğŸ“„ æœ€ç»ˆæ•°æ®: {result['final_data_path']}")
        print(f"ğŸ“Š å·¥ä½œæµæ‘˜è¦: {result['summary_path']}")
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡ä¿¡æ¯
        if 'cleaning_result' in result:
            cleaning = result['cleaning_result']
            print(f"\nğŸ“ˆ SQLæ¸…æ´—ç»Ÿè®¡:")
            print(f"   è¾“å…¥è®°å½•: {cleaning['input_records_count']:,}")
            print(f"   ç§»é™¤æ— æ•ˆSQL: {cleaning['invalid_sql_removed']:,}")
            print(f"   ä¿®æ”¹è®°å½•: {cleaning['records_modified']:,}")
        
        if 'extraction_result' in result:
            extraction = result['extraction_result']
            print(f"\nğŸ¯ å…³é”®è¯æå–ç»Ÿè®¡:")
            print(f"   è¾“å…¥è®°å½•: {extraction['input_records']:,}")
            print(f"   æå–è®°å½•: {extraction['extracted_records']:,}")
            print(f"   æå–ç‡: {extraction['extraction_rate']:.2f}%")
        
        if 'merge_result' in result:
            merge = result['merge_result']
            print(f"\nğŸ”„ æ•°æ®åˆå¹¶ç»Ÿè®¡:")
            print(f"   æ€»è®°å½•æ•°: {merge['total_records']:,}")
            print(f"   æ›´æ–°è®°å½•: {merge['updated_records']:,}")
            print(f"   æ›´æ–°ç‡: {merge['update_rate']:.2f}%")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿data_processingæ¨¡å—å·²æ­£ç¡®å®‰è£…")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œé”™è¯¯: {e}")
        raise

def run_step_by_step_demo(data_dir: str):
    """è¿è¡Œé€æ­¥æ¼”ç¤º"""
    print("\nğŸ”§ é€æ­¥æ¼”ç¤ºæ–°æ¶æ„å·¥ä½œæµ")
    print("-" * 40)
    
    try:
        from data_processing.workflow.workflow_manager import WorkflowManager
        
        # åˆ›å»ºå·¥ä½œæµç®¡ç†å™¨
        workflow = WorkflowManager("step_by_step_demo")
        
        print("ğŸ“¥ æ­¥éª¤1: åŠ è½½åŸå§‹æ•°æ®é›†...")
        load_result = workflow.load_raw_dataset(data_dir)
        print(f"   âœ… åŠ è½½äº† {load_result['total_records_loaded']:,} æ¡è®°å½•")
        
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€æ­¥...")
        
        print("ğŸ§¹ æ­¥éª¤2: SQLæ¸…æ´—å…¨ä½“æ•°æ®...")
        cleaning_result = workflow.run_sql_cleaning("sql_cleaning_step1")
        print(f"   âœ… ç§»é™¤äº† {cleaning_result['invalid_sql_removed']:,} ä¸ªæ— æ•ˆSQL")
        print(f"   âœ… ä¿®æ”¹äº† {cleaning_result['records_modified']:,} æ¡è®°å½•")
        
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€æ­¥...")
        
        print("ğŸ¯ æ­¥éª¤3: ä»æ¸…æ´—æ•°æ®ä¸­æå–å…³é”®è¯...")
        extraction_result = workflow.extract_keyword_data(None, "keyword_extraction_step2")
        print(f"   âœ… æå–äº† {extraction_result['extracted_records']:,} æ¡åŒ¹é…è®°å½•")
        print(f"   âœ… æå–ç‡: {extraction_result['extraction_rate']:.2f}%")
        
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€æ­¥...")
        
        print("ğŸ”§ æ­¥éª¤4: ç‰¹æ®Šå¤„ç†æå–çš„æ•°æ®...")
        processing_result = workflow.process_extracted_data("special_processing_step3")
        print(f"   âœ… å¤„ç†äº† {processing_result['input_records']:,} æ¡è®°å½•")
        print("   ğŸ’¡ å½“å‰ä¸ºé¢„ç•™æ¥å£ï¼Œå¯æ·»åŠ æ•°æ®å¢å¼ºã€æ ‡æ³¨ç­‰åŠŸèƒ½")
        
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€æ­¥...")
        
        print("ğŸ”„ æ­¥éª¤5: å°†å¤„ç†æ•°æ®åˆå¹¶å›åŸæ•°æ®é›†...")
        merge_result = workflow.merge_processed_data_back("merge_back_step4")
        print(f"   âœ… æ›´æ–°äº† {merge_result['updated_records']:,} æ¡è®°å½•")
        print(f"   âœ… ä¿æŒäº† {merge_result['unchanged_records']:,} æ¡åŸå§‹è®°å½•")
        
        print("\nğŸ“¤ å¯¼å‡ºæœ€ç»ˆæ•°æ®...")
        final_data_path = workflow.export_final_data("step_by_step_final.json")
        summary_path = workflow.save_workflow_summary()
        
        print(f"\nğŸ‰ é€æ­¥æ¼”ç¤ºå®Œæˆ!")
        print(f"ğŸ“ å·¥ä½œæµç›®å½•: {workflow.workflow_dir}")
        print(f"ğŸ“„ æœ€ç»ˆæ•°æ®: {final_data_path}")
        print(f"ğŸ“Š å·¥ä½œæµæ‘˜è¦: {summary_path}")
        
        # æ‰“å°æœ€ç»ˆæ‘˜è¦
        workflow.print_workflow_summary()
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿data_processingæ¨¡å—å·²æ­£ç¡®å®‰è£…")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œé”™è¯¯: {e}")
        raise

def run_test_workflow(data_dir: str):
    """è¿è¡Œæµ‹è¯•å·¥ä½œæµï¼ˆå°æ ·æœ¬ï¼‰"""
    print("\nğŸ§ª è¿è¡Œæµ‹è¯•å·¥ä½œæµ")
    print("-" * 40)
    
    try:
        from data_processing.workflow.workflow_manager import WorkflowManager
        
        # åˆ›å»ºå·¥ä½œæµç®¡ç†å™¨
        workflow = WorkflowManager("test_workflow")
        
        # æ­¥éª¤1: åŠ è½½åŸå§‹æ•°æ®é›†
        print("ğŸ“¥ åŠ è½½åŸå§‹æ•°æ®é›†...")
        load_result = workflow.load_raw_dataset(data_dir)
        original_count = len(workflow.current_data)
        
        # é™åˆ¶ä¸ºå°æ ·æœ¬è¿›è¡Œæµ‹è¯•ï¼ˆå‰100æ¡è®°å½•ï¼‰
        if original_count > 100:
            workflow.current_data = workflow.current_data[:100]
            print(f"   ğŸ“Š é™åˆ¶ä¸ºå‰100æ¡è®°å½•è¿›è¡Œæµ‹è¯•ï¼ˆåŸå§‹: {original_count:,} æ¡ï¼‰")
        
        # æ­¥éª¤2: SQLæ¸…æ´—
        print("ğŸ§¹ SQLæ¸…æ´—æµ‹è¯•...")
        cleaning_result = workflow.run_sql_cleaning("test_sql_cleaning")
        
        # æ­¥éª¤3: å…³é”®è¯æå–
        print("ğŸ¯ å…³é”®è¯æå–æµ‹è¯•...")
        extraction_result = workflow.extract_keyword_data(None, "test_keyword_extraction")
        
        # æ­¥éª¤4: ç‰¹æ®Šå¤„ç†
        print("ğŸ”§ ç‰¹æ®Šå¤„ç†æµ‹è¯•...")
        processing_result = workflow.process_extracted_data("test_special_processing")
        
        # æ­¥éª¤5: æ•°æ®åˆå¹¶
        print("ğŸ”„ æ•°æ®åˆå¹¶æµ‹è¯•...")
        merge_result = workflow.merge_processed_data_back("test_merge_back")
        
        # å¯¼å‡ºç»“æœ
        final_data_path = workflow.export_final_data("test_final.json")
        summary_path = workflow.save_workflow_summary()
        
        print(f"\nğŸ‰ æµ‹è¯•å·¥ä½œæµå®Œæˆ!")
        print(f"ğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        print(f"   åŸå§‹è®°å½•: {original_count:,}")
        print(f"   æµ‹è¯•è®°å½•: {len(workflow.current_data):,}")
        print(f"   æå–è®°å½•: {extraction_result['extracted_records']:,}")
        print(f"   æ›´æ–°è®°å½•: {merge_result['updated_records']:,}")
        
        print(f"\nğŸ“ è¾“å‡º:")
        print(f"   å·¥ä½œæµç›®å½•: {workflow.workflow_dir}")
        print(f"   æœ€ç»ˆæ•°æ®: {final_data_path}")
        print(f"   æ‘˜è¦æ–‡ä»¶: {summary_path}")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿data_processingæ¨¡å—å·²æ­£ç¡®å®‰è£…")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œé”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    sys.exit(main()) 