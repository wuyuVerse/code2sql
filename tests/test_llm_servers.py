"""LLMæœåŠ¡å™¨æµ‹è¯•"""
import pytest
import asyncio
import aiohttp
from openai import OpenAI
from config.llm_config import LLMConfig, ServerConfig


class TestLLMServers:
    """LLMæœåŠ¡å™¨æµ‹è¯•ç±»"""
    
    def test_server_configs(self):
        """æµ‹è¯•æœåŠ¡å™¨é…ç½®"""
        # æµ‹è¯•v3é…ç½®
        v3_config = LLMConfig.get_server_config("v3")
        print(f"V3é…ç½®: {v3_config.host}:{v3_config.port}, æ¨¡å‹: {v3_config.model_name}")
        
        # æµ‹è¯•r1é…ç½®
        r1_config = LLMConfig.get_server_config("r1")
        print(f"R1é…ç½®: {r1_config.host}:{r1_config.port}, æ¨¡å‹: {r1_config.model_name}")
        
        # æµ‹è¯•å®Œæ•´URL
        print(f"V3å®Œæ•´URL: {v3_config.full_url}")
        print(f"R1å®Œæ•´URL: {r1_config.full_url}")
        print(f"V3èŠå¤©API URL: {v3_config.chat_completions_url}")
        print(f"R1èŠå¤©API URL: {r1_config.chat_completions_url}")
    
    def test_openai_client_config(self):
        """æµ‹è¯•OpenAIå®¢æˆ·ç«¯é…ç½®"""
        v3_openai_config = LLMConfig.get_openai_client_config("v3")
        print(f"V3 OpenAIé…ç½®: {v3_openai_config}")
        
        r1_openai_config = LLMConfig.get_openai_client_config("r1")
        print(f"R1 OpenAIé…ç½®: {r1_openai_config}")
    
    def test_v3_connection_openai(self):
        """ä½¿ç”¨OpenAIåº“æµ‹è¯•v3æœåŠ¡å™¨è¿æ¥"""
        try:
            config = LLMConfig.get_openai_client_config("v3")
            client = OpenAI(
                api_key=config["api_key"],
                base_url=config["base_url"]
            )
            
            # å°è¯•ç®€å•çš„APIè°ƒç”¨
            response = client.chat.completions.create(
                model="v3",
                messages=[
                    {"role": "user", "content": "Hello, this is a test message for V3."}
                ],
                max_tokens=50,
                timeout=30
            )
            
            if response and hasattr(response, 'choices') and response.choices:
                print(f"âœ… V3æœåŠ¡å™¨OpenAIè¿æ¥æˆåŠŸ: {response.choices[0].message.content}")
            else:
                print("âŒ V3æœåŠ¡å™¨OpenAIè¿æ¥å¤±è´¥: å“åº”æ ¼å¼ä¸æ­£ç¡®")
                
        except Exception as e:
            print(f"âŒ V3æœåŠ¡å™¨OpenAIè¿æ¥å¤±è´¥: {str(e)}")
            pytest.skip(f"V3æœåŠ¡å™¨ä¸å¯ç”¨: {str(e)}")
    
    def test_r1_connection_openai(self):
        """ä½¿ç”¨OpenAIåº“æµ‹è¯•r1æœåŠ¡å™¨è¿æ¥"""
        try:
            config = LLMConfig.get_openai_client_config("r1")
            client = OpenAI(
                api_key=config["api_key"],
                base_url=config["base_url"]
            )
            
            # å°è¯•ç®€å•çš„APIè°ƒç”¨
            response = client.chat.completions.create(
                model="default",
                messages=[
                    {"role": "user", "content": "Hello, this is a test message for R1."}
                ],
                max_tokens=50,
                timeout=30
            )
            
            if response and hasattr(response, 'choices') and response.choices:
                print(f"âœ… R1æœåŠ¡å™¨OpenAIè¿æ¥æˆåŠŸ: {response.choices[0].message.content}")
            else:
                print("âŒ R1æœåŠ¡å™¨OpenAIè¿æ¥å¤±è´¥: å“åº”æ ¼å¼ä¸æ­£ç¡®")
                
        except Exception as e:
            print(f"âŒ R1æœåŠ¡å™¨OpenAIè¿æ¥å¤±è´¥: {str(e)}")
            pytest.skip(f"R1æœåŠ¡å™¨ä¸å¯ç”¨: {str(e)}")
    
    def test_v3_sync_api(self):
        """æµ‹è¯•V3åŒæ­¥APIè°ƒç”¨"""
        try:
            response = LLMConfig.call_api_sync("v3", "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªV3åŒæ­¥æµ‹è¯•ã€‚", max_tokens=100)
            if response:
                print(f"âœ… V3åŒæ­¥APIè°ƒç”¨æˆåŠŸ: {response}")
            else:
                print("âŒ V3åŒæ­¥APIè°ƒç”¨å¤±è´¥: ç©ºå“åº”")
        except Exception as e:
            print(f"âŒ V3åŒæ­¥APIè°ƒç”¨å¤±è´¥: {str(e)}")
            pytest.skip(f"V3åŒæ­¥APIä¸å¯ç”¨: {str(e)}")
    
    def test_r1_sync_api(self):
        """æµ‹è¯•R1åŒæ­¥APIè°ƒç”¨"""
        try:
            response = LLMConfig.call_api_sync("r1", "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªR1åŒæ­¥æµ‹è¯•ã€‚", max_tokens=100)
            if response:
                print(f"âœ… R1åŒæ­¥APIè°ƒç”¨æˆåŠŸ: {response}")
            else:
                print("âŒ R1åŒæ­¥APIè°ƒç”¨å¤±è´¥: ç©ºå“åº”")
        except Exception as e:
            print(f"âŒ R1åŒæ­¥APIè°ƒç”¨å¤±è´¥: {str(e)}")
            pytest.skip(f"R1åŒæ­¥APIä¸å¯ç”¨: {str(e)}")
    
    async def async_test_v3_api(self):
        """å¼‚æ­¥æµ‹è¯•V3 API"""
        async with aiohttp.ClientSession() as session:
            response = await LLMConfig.call_api_async(session, "v3", "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªV3å¼‚æ­¥æµ‹è¯•ã€‚", max_tokens=100)
            if response:
                print(f"âœ… V3å¼‚æ­¥APIè°ƒç”¨æˆåŠŸ: {response}")
            else:
                print("âŒ V3å¼‚æ­¥APIè°ƒç”¨å¤±è´¥: ç©ºå“åº”")
    
    async def async_test_r1_api(self):
        """å¼‚æ­¥æµ‹è¯•R1 API"""
        async with aiohttp.ClientSession() as session:
            response = await LLMConfig.call_api_async(session, "r1", "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªR1å¼‚æ­¥æµ‹è¯•ã€‚", max_tokens=100)
            if response:
                print(f"âœ… R1å¼‚æ­¥APIè°ƒç”¨æˆåŠŸ: {response}")
            else:
                print("âŒ R1å¼‚æ­¥APIè°ƒç”¨å¤±è´¥: ç©ºå“åº”")
    
    def test_v3_async_api(self):
        """æµ‹è¯•V3å¼‚æ­¥APIè°ƒç”¨ï¼ˆåŒ…è£…å™¨ï¼‰"""
        try:
            asyncio.run(self.async_test_v3_api())
        except Exception as e:
            print(f"âŒ V3å¼‚æ­¥APIè°ƒç”¨å¤±è´¥: {str(e)}")
            pytest.skip(f"V3å¼‚æ­¥APIä¸å¯ç”¨: {str(e)}")
    
    def test_r1_async_api(self):
        """æµ‹è¯•R1å¼‚æ­¥APIè°ƒç”¨ï¼ˆåŒ…è£…å™¨ï¼‰"""
        try:
            asyncio.run(self.async_test_r1_api())
        except Exception as e:
            print(f"âŒ R1å¼‚æ­¥APIè°ƒç”¨å¤±è´¥: {str(e)}")
            pytest.skip(f"R1å¼‚æ­¥APIä¸å¯ç”¨: {str(e)}")
    
    def test_list_servers(self):
        """æµ‹è¯•åˆ—å‡ºæ‰€æœ‰æœåŠ¡å™¨"""
        servers = LLMConfig.list_servers()
        print(f"å¯ç”¨æœåŠ¡å™¨: {servers}")


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæµ‹è¯•
    test_instance = TestLLMServers()
    
    print("ğŸ”§ æµ‹è¯•æœåŠ¡å™¨é…ç½®...")
    test_instance.test_server_configs()
    print("âœ… æœåŠ¡å™¨é…ç½®æµ‹è¯•é€šè¿‡")
    
    print("\nğŸ”§ æµ‹è¯•OpenAIå®¢æˆ·ç«¯é…ç½®...")
    test_instance.test_openai_client_config()
    print("âœ… OpenAIå®¢æˆ·ç«¯é…ç½®æµ‹è¯•é€šè¿‡")
    
    print("\nğŸ”§ æµ‹è¯•æœåŠ¡å™¨åˆ—è¡¨...")
    test_instance.test_list_servers()
    print("âœ… æœåŠ¡å™¨åˆ—è¡¨æµ‹è¯•é€šè¿‡")
    
    print("\nğŸŒ æµ‹è¯•V3æœåŠ¡å™¨OpenAIè¿æ¥...")
    test_instance.test_v3_connection_openai()
    
    print("\nğŸŒ æµ‹è¯•R1æœåŠ¡å™¨OpenAIè¿æ¥...")
    test_instance.test_r1_connection_openai()
    
    print("\nğŸ”— æµ‹è¯•V3åŒæ­¥API...")
    test_instance.test_v3_sync_api()
    
    print("\nğŸ”— æµ‹è¯•R1åŒæ­¥API...")
    test_instance.test_r1_sync_api()
    
    print("\nâš¡ æµ‹è¯•V3å¼‚æ­¥API...")
    test_instance.test_v3_async_api()
    
    print("\nâš¡ æµ‹è¯•R1å¼‚æ­¥API...")
    test_instance.test_r1_async_api()
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼") 