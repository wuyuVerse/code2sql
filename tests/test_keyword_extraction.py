"""å…³é”®è¯æå–åŠŸèƒ½æµ‹è¯•

æµ‹è¯•DataReaderçš„å…³é”®è¯æå–åŠŸèƒ½ï¼ŒéªŒè¯æå–ç»“æœçš„æ­£ç¡®æ€§
"""

import json
import os
import sys
import tempfile
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from data_processing import DataReader
except ImportError:
    from data_processing.data_reader import DataReader


def test_gorm_keyword_extraction():
    """æµ‹è¯•GORMå…³é”®è¯æå–åŠŸèƒ½"""
    print("ğŸ” å¼€å§‹GORMå…³é”®è¯æå–æµ‹è¯•...")
    
    # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        output_dir = Path(temp_dir) / "test_extracted"
        
        # åˆ›å»ºæ•°æ®è¯»å–å™¨
        reader = DataReader("datasets/claude_output")
        
        # æ‰§è¡ŒGORMå…³é”®è¯æå–
        print("ğŸš€ æ‰§è¡ŒGORMå…³é”®è¯æå–...")
        result = reader.extract_gorm_keywords(str(output_dir))
        
        # éªŒè¯è¿”å›ç»“æœ
        assert "total_records_processed" in result, "ç¼ºå°‘æ€»è®°å½•æ•°ç»Ÿè®¡"
        assert "matched_records" in result, "ç¼ºå°‘åŒ¹é…è®°å½•æ•°ç»Ÿè®¡"
        assert "match_rate" in result, "ç¼ºå°‘åŒ¹é…ç‡ç»Ÿè®¡"
        assert "keyword_frequency" in result, "ç¼ºå°‘å…³é”®è¯é¢‘ç‡ç»Ÿè®¡"
        assert "source_file_distribution" in result, "ç¼ºå°‘æºæ–‡ä»¶åˆ†å¸ƒç»Ÿè®¡"
        assert "output_directory" in result, "ç¼ºå°‘è¾“å‡ºç›®å½•ä¿¡æ¯"
        
        print(f"âœ… æ€»è®°å½•æ•°: {result['total_records_processed']:,}")
        print(f"âœ… åŒ¹é…è®°å½•æ•°: {result['matched_records']:,}")
        print(f"âœ… åŒ¹é…ç‡: {result['match_rate']:.2f}%")
        
        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        output_path = Path(result['output_directory'])
        assert output_path.exists(), "è¾“å‡ºç›®å½•ä¸å­˜åœ¨"
        
        main_file = output_path / "keyword_matched_records.json"
        stats_file = output_path / "extraction_statistics.json"
        keyword_dir = output_path / "by_keyword"
        
        assert main_file.exists(), "ä¸»æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨"
        assert stats_file.exists(), "ç»Ÿè®¡æ–‡ä»¶ä¸å­˜åœ¨"
        assert keyword_dir.exists(), "å…³é”®è¯åˆ†ç±»ç›®å½•ä¸å­˜åœ¨"
        
        # éªŒè¯ä¸»æ•°æ®æ–‡ä»¶å†…å®¹
        with open(main_file, 'r', encoding='utf-8') as f:
            main_data = json.load(f)
        assert len(main_data) == result['matched_records'], "ä¸»æ•°æ®æ–‡ä»¶è®°å½•æ•°ä¸åŒ¹é…"
        
        # éªŒè¯ç»Ÿè®¡æ–‡ä»¶å†…å®¹
        with open(stats_file, 'r', encoding='utf-8') as f:
            stats_data = json.load(f)
        assert stats_data['total_records_processed'] == result['total_records_processed'], "ç»Ÿè®¡æ•°æ®ä¸ä¸€è‡´"
        
        print("âœ… GORMå…³é”®è¯æå–æµ‹è¯•é€šè¿‡ï¼")
        return result


def test_custom_keyword_extraction():
    """æµ‹è¯•è‡ªå®šä¹‰å…³é”®è¯æå–åŠŸèƒ½"""
    print("\nğŸ¯ å¼€å§‹è‡ªå®šä¹‰å…³é”®è¯æå–æµ‹è¯•...")
    
    # è‡ªå®šä¹‰å…³é”®è¯åˆ—è¡¨
    custom_keywords = ["SELECT", "INSERT", "UPDATE", "DELETE", "JOIN"]
    
    with tempfile.TemporaryDirectory() as temp_dir:
        output_dir = Path(temp_dir) / "test_custom"
        
        # åˆ›å»ºæ•°æ®è¯»å–å™¨
        reader = DataReader("datasets/claude_output")
        
        # æ‰§è¡Œè‡ªå®šä¹‰å…³é”®è¯æå–
        print(f"ğŸš€ æ‰§è¡Œè‡ªå®šä¹‰å…³é”®è¯æå–: {custom_keywords}")
        result = reader.extract_by_keywords(
            keywords=custom_keywords,
            output_dir=str(output_dir),
            step_name="sql_keywords"
        )
        
        # éªŒè¯è¿”å›ç»“æœ
        assert "total_records_processed" in result, "ç¼ºå°‘æ€»è®°å½•æ•°ç»Ÿè®¡"
        assert "matched_records" in result, "ç¼ºå°‘åŒ¹é…è®°å½•æ•°ç»Ÿè®¡"
        assert "keyword_frequency" in result, "ç¼ºå°‘å…³é”®è¯é¢‘ç‡ç»Ÿè®¡"
        
        print(f"âœ… æ€»è®°å½•æ•°: {result['total_records_processed']:,}")
        print(f"âœ… åŒ¹é…è®°å½•æ•°: {result['matched_records']:,}")
        print(f"âœ… åŒ¹é…ç‡: {result['match_rate']:.2f}%")
        
        # éªŒè¯å…³é”®è¯é¢‘ç‡
        keyword_freq = result['keyword_frequency']
        for keyword in custom_keywords:
            assert keyword in keyword_freq, f"å…³é”®è¯ {keyword} æœªåœ¨ç»Ÿè®¡ä¸­"
            print(f"   {keyword}: {keyword_freq[keyword]} æ¬¡")
        
        # éªŒè¯è¾“å‡ºç›®å½•ç»“æ„
        output_path = Path(result['output_directory'])
        assert output_path.name.startswith("sql_keywords_"), "è¾“å‡ºç›®å½•å‘½åä¸æ­£ç¡®"
        
        print("âœ… è‡ªå®šä¹‰å…³é”®è¯æå–æµ‹è¯•é€šè¿‡ï¼")
        return result


def test_small_dataset_extraction():
    """æµ‹è¯•å°æ•°æ®é›†çš„å…³é”®è¯æå–"""
    print("\nğŸ“Š å¼€å§‹å°æ•°æ®é›†æµ‹è¯•...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        output_dir = Path(temp_dir) / "test_small"
        
        # åˆ›å»ºæ•°æ®è¯»å–å™¨ï¼Œåªè¯»å–å°‘é‡æ–‡ä»¶
        reader = DataReader("datasets/claude_output")
        
        # åªè¯»å–æµ‹è¯•æ–‡ä»¶
        test_files = ["test_results.json"]
        reader.read_files(test_files)
        
        print(f"ğŸ“ è¯»å–äº† {len(reader)} æ¡æµ‹è¯•è®°å½•")
        
        # æ‰§è¡Œå…³é”®è¯æå–
        keywords = ["save", "Association", "Preload"]
        result = reader.extract_by_keywords(
            keywords=keywords,
            output_dir=str(output_dir),
            step_name="small_test"
        )
        
        print(f"âœ… å°æ•°æ®é›†æµ‹è¯•å®Œæˆ")
        print(f"   å¤„ç†è®°å½•: {result['total_records_processed']}")
        print(f"   åŒ¹é…è®°å½•: {result['matched_records']}")
        
        # éªŒè¯å…·ä½“åŒ¹é…å†…å®¹
        output_path = Path(result['output_directory'])
        main_file = output_path / "keyword_matched_records.json"
        
        if main_file.exists():
            with open(main_file, 'r', encoding='utf-8') as f:
                matched_data = json.load(f)
            
            print(f"   å®é™…åŒ¹é…æ•°æ®: {len(matched_data)} æ¡")
            
            # æ˜¾ç¤ºç¬¬ä¸€æ¡åŒ¹é…è®°å½•çš„è¯¦ç»†ä¿¡æ¯
            if matched_data:
                first_record = matched_data[0]
                print(f"   ç¬¬ä¸€æ¡è®°å½•å‡½æ•°å: {first_record.get('function_name', 'N/A')}")
                print(f"   åŒ¹é…å…³é”®è¯: {first_record.get('matched_keywords', [])}")
        
        print("âœ… å°æ•°æ®é›†æµ‹è¯•é€šè¿‡ï¼")
        return result


def test_keyword_frequency_accuracy():
    """æµ‹è¯•å…³é”®è¯é¢‘ç‡ç»Ÿè®¡çš„å‡†ç¡®æ€§"""
    print("\nğŸ”¢ å¼€å§‹å…³é”®è¯é¢‘ç‡å‡†ç¡®æ€§æµ‹è¯•...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        output_dir = Path(temp_dir) / "test_accuracy"
        
        # ä½¿ç”¨ç‰¹å®šå…³é”®è¯è¿›è¡Œæµ‹è¯•
        keywords = ["save", "Transaction"]
        
        reader = DataReader("datasets/claude_output")
        result = reader.extract_by_keywords(
            keywords=keywords,
            output_dir=str(output_dir),
            step_name="accuracy_test"
        )
        
        # éªŒè¯æŒ‰å…³é”®è¯åˆ†ç±»çš„æ–‡ä»¶
        output_path = Path(result['output_directory'])
        keyword_dir = output_path / "by_keyword"
        
        total_from_files = 0
        for keyword in keywords:
            keyword_file = keyword_dir / f"{keyword}_records.json"
            if keyword_file.exists():
                with open(keyword_file, 'r', encoding='utf-8') as f:
                    keyword_data = json.load(f)
                file_count = len(keyword_data)
                freq_count = result['keyword_frequency'][keyword]
                
                print(f"   {keyword}:")
                print(f"     ç»Ÿè®¡é¢‘ç‡: {freq_count}")
                print(f"     æ–‡ä»¶è®°å½•æ•°: {file_count}")
                
                # æ³¨æ„ï¼šç”±äºä¸€æ¡è®°å½•å¯èƒ½åŒ…å«å¤šä¸ªç›¸åŒå…³é”®è¯ï¼Œæ‰€ä»¥æ–‡ä»¶è®°å½•æ•°å¯èƒ½å°äºé¢‘ç‡ç»Ÿè®¡
                assert file_count <= freq_count, f"{keyword} æ–‡ä»¶è®°å½•æ•°ä¸åº”å¤§äºé¢‘ç‡ç»Ÿè®¡"
                total_from_files += file_count
        
        print(f"âœ… å…³é”®è¯é¢‘ç‡ç»Ÿè®¡å‡†ç¡®æ€§æµ‹è¯•é€šè¿‡ï¼")
        return result


def test_output_file_structure():
    """æµ‹è¯•è¾“å‡ºæ–‡ä»¶ç»“æ„çš„å®Œæ•´æ€§"""
    print("\nğŸ“ å¼€å§‹è¾“å‡ºæ–‡ä»¶ç»“æ„æµ‹è¯•...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        output_dir = Path(temp_dir) / "test_structure"
        
        reader = DataReader("datasets/claude_output")
        keywords = ["Preload", "save"]
        
        result = reader.extract_by_keywords(
            keywords=keywords,
            output_dir=str(output_dir),
            step_name="structure_test"
        )
        
        output_path = Path(result['output_directory'])
        
        # éªŒè¯å¿…éœ€æ–‡ä»¶
        required_files = [
            "keyword_matched_records.json",
            "extraction_statistics.json"
        ]
        
        for filename in required_files:
            file_path = output_path / filename
            assert file_path.exists(), f"å¿…éœ€æ–‡ä»¶ {filename} ä¸å­˜åœ¨"
            assert file_path.stat().st_size > 0, f"æ–‡ä»¶ {filename} ä¸ºç©º"
            print(f"   âœ… {filename}: {file_path.stat().st_size} bytes")
        
        # éªŒè¯å…³é”®è¯åˆ†ç±»ç›®å½•
        keyword_dir = output_path / "by_keyword"
        assert keyword_dir.exists(), "å…³é”®è¯åˆ†ç±»ç›®å½•ä¸å­˜åœ¨"
        
        # æ£€æŸ¥æ¯ä¸ªå…³é”®è¯çš„æ–‡ä»¶
        for keyword in keywords:
            keyword_file = keyword_dir / f"{keyword}_records.json"
            if result['keyword_frequency'][keyword] > 0:
                assert keyword_file.exists(), f"å…³é”®è¯æ–‡ä»¶ {keyword}_records.json ä¸å­˜åœ¨"
                print(f"   âœ… {keyword}_records.json: {keyword_file.stat().st_size} bytes")
        
        print("âœ… è¾“å‡ºæ–‡ä»¶ç»“æ„æµ‹è¯•é€šè¿‡ï¼")
        return result


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰å…³é”®è¯æå–æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹å…³é”®è¯æå–å®Œæ•´æµ‹è¯•å¥—ä»¶...")
    print("=" * 60)
    
    try:
        # æµ‹è¯•1ï¼šGORMå…³é”®è¯æå–
        gorm_result = test_gorm_keyword_extraction()
        
        # æµ‹è¯•2ï¼šè‡ªå®šä¹‰å…³é”®è¯æå–
        custom_result = test_custom_keyword_extraction()
        
        # æµ‹è¯•3ï¼šå°æ•°æ®é›†æµ‹è¯•
        small_result = test_small_dataset_extraction()
        
        # æµ‹è¯•4ï¼šå‡†ç¡®æ€§æµ‹è¯•
        accuracy_result = test_keyword_frequency_accuracy()
        
        # æµ‹è¯•5ï¼šæ–‡ä»¶ç»“æ„æµ‹è¯•
        structure_result = test_output_file_structure()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰å…³é”®è¯æå–æµ‹è¯•é€šè¿‡ï¼")
        print("=" * 60)
        
        # æµ‹è¯•æ€»ç»“
        print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
        print(f"âœ… GORMå…³é”®è¯æå–: {gorm_result['matched_records']:,} æ¡åŒ¹é…è®°å½•")
        print(f"âœ… è‡ªå®šä¹‰å…³é”®è¯æå–: {custom_result['matched_records']:,} æ¡åŒ¹é…è®°å½•")
        print(f"âœ… å°æ•°æ®é›†æµ‹è¯•: {small_result['matched_records']:,} æ¡åŒ¹é…è®°å½•")
        print(f"âœ… å…³é”®è¯é¢‘ç‡å‡†ç¡®æ€§: é€šè¿‡")
        print(f"âœ… æ–‡ä»¶ç»“æ„å®Œæ•´æ€§: é€šè¿‡")
        
        print("\nğŸ’¡ åŠŸèƒ½ç‰¹æ€§éªŒè¯:")
        print("- âœ… GORMé¢„å®šä¹‰å…³é”®è¯æå–")
        print("- âœ… è‡ªå®šä¹‰å…³é”®è¯åˆ—è¡¨æå–")
        print("- âœ… å¤§è§„æ¨¡æ•°æ®å¤„ç†èƒ½åŠ›")
        print("- âœ… ç»Ÿè®¡ä¿¡æ¯å‡†ç¡®æ€§")
        print("- âœ… æ–‡ä»¶åˆ†ç±»å’Œç»„ç»‡")
        print("- âœ… æ—¶é—´æˆ³å‘½åè§„èŒƒ")
        print("- âœ… JSONæ ¼å¼è¾“å‡º")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    run_all_tests() 