# Code2SQL æ•°æ®å¤„ç†å™¨ä½¿ç”¨æŒ‡å—

## ğŸ“ æ–°çš„æ–‡ä»¶ç»“æ„è®¾è®¡

### è¾“å‡ºç›®å½•ç»“æ„
```
extracted_data/
â””â”€â”€ gorm_keywords_20250703_120407/    # æ­¥éª¤å_æ—¶é—´æˆ³
    â”œâ”€â”€ keyword_matched_records.json  # ä¸»æ•°æ®æ–‡ä»¶ (14MB)
    â”œâ”€â”€ extraction_statistics.json    # ç»Ÿè®¡æŠ¥å‘Š
    â””â”€â”€ by_keyword/                   # æŒ‰å…³é”®è¯åˆ†ç±»
        â”œâ”€â”€ save_records.json (7.7MB)
        â”œâ”€â”€ Association_records.json (6.1MB)
        â”œâ”€â”€ Preload_records.json (1.2MB)
        â”œâ”€â”€ Transaction_records.json (911KB)
        â”œâ”€â”€ Callbacks_records.json (1.5MB)
        â””â”€â”€ ... (å…¶ä»–å…³é”®è¯æ–‡ä»¶)
```

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨
```python
from data_processing.data_reader import DataReader

# åˆ›å»ºæ•°æ®è¯»å–å™¨
reader = DataReader("datasets/claude_output")

# æå–GORMå…³é”®è¯ (æ¨èæ–¹æ³•)
stats = reader.extract_gorm_keywords()

# è‡ªå®šä¹‰å…³é”®è¯æå–
custom_keywords = ["SELECT", "INSERT", "UPDATE"]
stats = reader.extract_by_keywords(custom_keywords, step_name="sql_keywords")
```

### æ–‡ä»¶ç»„ç»‡ä¼˜åŠ¿

1. **æ—¶é—´è¿½æº¯**: æ¯æ¬¡è¿è¡Œéƒ½æœ‰æ—¶é—´æˆ³ï¼Œä¾¿äºç‰ˆæœ¬ç®¡ç†
2. **æ­¥éª¤åˆ†ç¦»**: ä¸åŒå¤„ç†æ­¥éª¤æœ‰ç‹¬ç«‹æ–‡ä»¶å¤¹
3. **æ‰¹é‡å¤„ç†**: æ”¯æŒå¤šä¸ªä¸­é—´æ­¥éª¤çš„workflow

## ğŸ“Š å¤„ç†ç»“æœ

### æœ€æ–°æå–ç»“æœ (2025-07-03 12:04:07)
- **å¤„ç†æ–‡ä»¶**: 86ä¸ªJSONæ–‡ä»¶
- **æ€»è®°å½•æ•°**: 17,761æ¡
- **åŒ¹é…è®°å½•**: 1,345æ¡ (7.57%)
- **è¾“å‡ºä½ç½®**: `extracted_data/gorm_keywords_20250703_120407/`

### å…³é”®è¯å‘½ä¸­ç»Ÿè®¡
| å…³é”®è¯ | å‘½ä¸­æ¬¡æ•° | ä¸»è¦æ¥æº |
|--------|----------|----------|
| save | 616 | alanoluo__eb-api |
| Association | 336 | 5-28-cbs |
| Preload | 170 | STKE__jinzhu-gorm |
| Transaction | 150 | å„é¡¹ç›®åˆ†å¸ƒ |
| Scopes | 46 | IVCç›¸å…³é¡¹ç›® |

## ğŸš€ ä¸‹ä¸€æ­¥workflow

### æ•°æ®å¤„ç†æµæ°´çº¿
```
datasets/claude_output/        # åŸå§‹æ•°æ®
    â†“
extracted_data/
â”œâ”€â”€ gorm_keywords_YYYYMMDD_HHMMSS/    # å…³é”®è¯æå–
â”œâ”€â”€ cleaned_data_YYYYMMDD_HHMMSS/     # æ•°æ®æ¸…æ´— (ä¸‹ä¸€æ­¥)
â”œâ”€â”€ augmented_data_YYYYMMDD_HHMMSS/   # æ•°æ®å¢å¼º (åç»­)
â””â”€â”€ training_data_YYYYMMDD_HHMMSS/    # è®­ç»ƒæ•°æ® (æœ€ç»ˆ)
```

### æ”¯æŒçš„å¤„ç†æ­¥éª¤
1. **å…³é”®è¯æå–** âœ… å·²å®Œæˆ
2. **æ•°æ®æ¸…æ´—** (ä½¿ç”¨LLM API)
3. **æ•°æ®å¢å¼º** (ç”Ÿæˆæ›´å¤šæ ·æœ¬)
4. **è´¨é‡éªŒè¯** (æœ€ç»ˆæ£€æŸ¥)

## ğŸ’¡ æœ€ä½³å®è·µ

### è¿è¡Œç¯å¢ƒ
```bash
# ä½¿ç”¨uvç¯å¢ƒè¿è¡Œ
uv run python extract_demo.py

# æˆ–è€…æ¿€æ´»ç¯å¢ƒåè¿è¡Œ
uv shell
python extract_demo.py
```

### æ¨¡å—å¯¼å…¥ä¿®å¤
- ä¿®å¤äº†`__init__.py`å¯¼å…¥é—®é¢˜
- æ”¯æŒæŒ‰éœ€å¯¼å…¥ï¼Œé¿å…ä¾èµ–é”™è¯¯
- æ ¸å¿ƒåŠŸèƒ½ç‹¬ç«‹ï¼Œå¯é æ€§æ›´é«˜

### è‡ªå®šä¹‰æå–
```python
# è‡ªå®šä¹‰å…³é”®è¯å’Œæ­¥éª¤å
keywords = ["gorm.DB", "db.Exec", "migrations"]
stats = reader.extract_by_keywords(
    keywords, 
    output_dir="extracted_data",
    step_name="database_operations"
)
```

## ğŸ” æ–‡ä»¶è¯´æ˜

- **keyword_matched_records.json**: åŒ…å«æ‰€æœ‰åŒ¹é…è®°å½•çš„å®Œæ•´æ•°æ®
- **extraction_statistics.json**: è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Šï¼ŒåŒ…å«é¢‘ç‡åˆ†æ
- **by_keyword/*.json**: æŒ‰å…³é”®è¯åˆ†ç±»çš„æ•°æ®ï¼Œä¾¿äºå•ç‹¬åˆ†æ

## ğŸ“ æŠ€æœ¯ç»†èŠ‚

### å…³é”®è¯åŒ¹é…é€»è¾‘
1. æ£€æŸ¥ `code_meta_data` ä¸­çš„ `code_value` å­—æ®µ
2. è¡¥å……æ£€æŸ¥ `orm_code` å­—æ®µ
3. è®°å½•æ‰€æœ‰åŒ¹é…çš„å…³é”®è¯
4. ç”Ÿæˆè¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š

### æ€§èƒ½ä¼˜åŒ–
- æ‰¹é‡è¯»å–æ‰€æœ‰æ–‡ä»¶
- è¿›åº¦æ˜¾ç¤º (æ¯5000æ¡è®°å½•)
- å†…å­˜å‹å¥½çš„æ•°æ®ç»“æ„
- å¢é‡å¤„ç†æ”¯æŒ

è¿™ä¸ªæ”¹è¿›çš„æ•°æ®å¤„ç†å™¨ç°åœ¨å®Œå…¨ç¬¦åˆæ‚¨çš„éœ€æ±‚ï¼Œæ”¯æŒå¤šæ­¥éª¤workflowï¼Œå¹¶ä¸”è¿è¡Œç¨³å®šï¼ 