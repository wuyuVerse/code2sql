#!/usr/bin/env python3
"""
ç®€åŒ–çš„æ¨¡å‹è¯„ä¼°è¿è¡Œè„šæœ¬

ç›´æ¥ä½¿ç”¨transformersåº“è¿›è¡Œæ¨ç†ï¼Œé¿å…LLaMA-Factory CLIçš„å¤æ‚æ€§
"""

import os
import sys
import json
import logging
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import argparse
from tqdm import tqdm
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°sys.path
PROJECT_ROOT = Path(__file__).resolve().parents[4]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…ç¯å¢ƒé—®é¢˜
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
    from data_processing.cleaning.sql_feature_extractor import match_single_sql
    from config.training.data_conversion.orm2sql_prompt_template import PROMPT_TEMPLATE
except ImportError as e:
    print(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…transformerså’Œå…¶ä»–å¿…è¦ä¾èµ–")
    sys.exit(1)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SimpleModelEvaluator:
    """ç®€åŒ–çš„æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, config_path: str, output_dir_override: Optional[str] = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self.load_config(config_path)
        self.model = None
        self.tokenizer = None
        
        # ä»é…ç½®åŠ è½½æˆ–ä½¿ç”¨è¦†ç›–çš„è¾“å‡ºç›®å½•
        self.output_dir = self.config.get('output_config', {}).get('output_dir', 'evaluation_results')
        if output_dir_override:
            self.output_dir = output_dir_override
        self.output_dir = Path(self.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç»“æœå­˜å‚¨
        self.eval_results = []
        self.stats = {
            'total_samples': 0,
            'successful_inference': 0,
            'valid_sql_generated': 0,
            'fingerprint_matched': 0,
            'parse_errors': 0,
            'inference_errors': 0
        }
        
        logger.info(f"è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_file = Path(config_path)
        if not config_file.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        logger.info(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        return config
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        model_path = self.config['model_config']['model_path']
        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        
        try:
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=self.config['environment_config']['trust_remote_code']
            )
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16 if self.config['environment_config']['bf16'] else torch.float32,
                device_map="auto",
                trust_remote_code=self.config['environment_config']['trust_remote_code']
            )
            
            logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def load_eval_data(self) -> List[Dict]:
        """åŠ è½½éªŒè¯é›†æ•°æ®"""
        eval_data_path = Path(self.config['data_config']['eval_data_path'])
        logger.info(f"æ­£åœ¨åŠ è½½éªŒè¯é›†: {eval_data_path}")
        
        with open(eval_data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        eval_samples = []
        for key, value in data.items():
            sample = value.copy()
            sample['sample_id'] = key
            eval_samples.append(sample)
        
        # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        max_samples = self.config['data_config'].get('max_samples')
        if max_samples:
            eval_samples = eval_samples[:max_samples]
            logger.info(f"é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡ä¸º: {max_samples}")
        
        # æµ‹è¯•æ¨¡å¼
        if self.config['debug_config'].get('test_mode', False):
            test_samples = self.config['debug_config'].get('test_samples', 10)
            eval_samples = eval_samples[:test_samples]
            logger.info(f"æµ‹è¯•æ¨¡å¼ï¼Œä½¿ç”¨ {test_samples} ä¸ªæ ·æœ¬")
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(eval_samples)} æ¡éªŒè¯æ ·æœ¬")
        return eval_samples
    
    def create_prompt(self, sample: Dict) -> str:
        """åˆ›å»ºæ¨ç†æç¤ºè¯"""
        function_name = sample.get('code_key', 'æœªçŸ¥å‡½æ•°')
        orm_code = sample.get('code_value', '')
        
        # å¤„ç†callers
        callers = sample.get('callers', [])
        caller = json.dumps(callers[0], ensure_ascii=False) if callers else ""
        callee = ""
        
        # æ„å»ºcode_meta_data
        code_meta_data = [{
            'code_file': sample.get('code_file', ''),
            'code_start_line': sample.get('code_start_line', 0),
            'code_end_line': sample.get('code_end_line', 0),
            'code_key': sample.get('code_key', ''),
            'code_value': sample.get('code_value', ''),
            'code_label': sample.get('code_label', 0),
            'code_type': sample.get('code_type', 0),
            'code_version': sample.get('code_version', '')
        }]
        code_meta_data_str = json.dumps(code_meta_data, ensure_ascii=False, indent=2)
        
        prompt = PROMPT_TEMPLATE.format(
            function_name=function_name,
            orm_code=orm_code,
            caller=caller,
            callee=callee,
            code_meta_data_str=code_meta_data_str
        )
        return prompt.strip()

    def run_inference(self, prompt: str) -> str:
        """è¿è¡Œå•ä¸ªæ ·æœ¬çš„æ¨ç†"""
        if self.model is None or self.tokenizer is None:
            logger.error("æ¨¡å‹æˆ–åˆ†è¯å™¨æœªåŠ è½½")
            return ""
            
        try:
            # æ„å»ºå¯¹è¯æ ¼å¼çš„è¾“å…¥
            messages = [
                {"role": "user", "content": prompt}
            ]
            
            # ä½¿ç”¨åˆ†è¯å™¨çš„chat template
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer.encode(text, return_tensors="pt")
            inputs = inputs.to(self.model.device)
            
            # ç”Ÿæˆé…ç½®
            gen_config = GenerationConfig(
                max_new_tokens=self.config['inference_config']['generate_config']['max_new_tokens'],
                temperature=self.config['inference_config']['generate_config']['temperature'],
                top_p=self.config['inference_config']['generate_config']['top_p'],
                do_sample=self.config['inference_config']['generate_config']['do_sample'],
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            # æ¨ç†
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    generation_config=gen_config
                )
            
            # è§£ç è¾“å‡º
            response = self.tokenizer.decode(
                outputs[0][len(inputs[0]):], 
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            logger.warning(f"æ¨ç†å¤±è´¥: {e}")
            return ""

    def _recursively_extract_sql(self, data: Any) -> List[str]:
        """
        é€’å½’åœ°éå†æ•°æ®ç»“æ„ä»¥æå–æ‰€æœ‰SQLå­—ç¬¦ä¸²ã€‚
        """
        extracted_sql = []
        if isinstance(data, str):
            # åŸºæœ¬æƒ…å†µï¼šå®ƒæ˜¯ä¸€ä¸ªSQLå­—ç¬¦ä¸²
            if data.strip():
                extracted_sql.append(data.strip())
        elif isinstance(data, dict):
            # å¦‚æœæ˜¯param_dependentç»“æ„
            if data.get("type") == "param_dependent" and "variants" in data:
                for variant in data.get("variants", []):
                    # ä»å˜ä½“çš„sqlå­—æ®µä¸­é€’å½’æå–
                    extracted_sql.extend(self._recursively_extract_sql(variant.get("sql")))
            # è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–å­—å…¸ç»“æ„çš„å¤„ç†
        elif isinstance(data, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œåˆ™è¿­ä»£å¹¶é€’å½’
            for item in data:
                extracted_sql.extend(self._recursively_extract_sql(item))
        
        return extracted_sql

    def parse_sql_response(self, response: str) -> List[str]:
        """
        è§£ææ¨¡å‹çš„JSONç±»å“åº”ä»¥æå–æ‰€æœ‰SQLè¯­å¥ï¼Œ
        èƒ½å¤„ç†åƒ 'param_dependent' è¿™æ ·çš„å¤æ‚ç»“æ„ã€‚
        """
        if not response.strip():
            return []

        # å°è¯•å°†å“åº”è§£æä¸ºJSONå¯¹è±¡
        try:
            # LLMå¯èƒ½è¿”å›ä¸€ä¸ªä¸å®Œå…¨æ˜¯JSONçš„å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬å°è¯•æ‰¾åˆ°å…¶ä¸­çš„JSONéƒ¨åˆ†
            # é€šå¸¸æ˜¯æˆ‘ä»¬æœŸæœ›çš„åˆ—è¡¨æ ¼å¼ `[...]`
            start = response.find('[')
            end = response.rfind(']')
            if start != -1 and end != -1 and start < end:
                json_string = response[start:end+1]
                parsed_data = json.loads(json_string)
            else:
                # å¦‚æœæ‰¾ä¸åˆ° `[]`ï¼Œå°è¯•ç›´æ¥è§£ææ•´ä¸ªå­—ç¬¦ä¸²
                try:
                    parsed_data = json.loads(response)
                except json.JSONDecodeError:
                    # å¦‚æœä¸èƒ½è§£æä¸ºJSONï¼Œåˆ™å°†å…¶è§†ä¸ºå•ä¸ªåŸå§‹SQLè¯­å¥
                    logger.debug(f"å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONï¼Œå°†å…¶è§†ä¸ºåŸå§‹å­—ç¬¦ä¸²: {response}")
                    return [response.strip()] if response.strip() else []
        except json.JSONDecodeError:
            logger.warning(f"æ— æ³•å°†æ¨¡å‹å“åº”è§£æä¸ºJSONã€‚å°†å…¶è§†ä¸ºåŸå§‹å­—ç¬¦ä¸²ã€‚å“åº”: {response}")
            return [response.strip()] if response.strip() else []
        
        # è·å¾—è§£æåçš„æ•°æ®å (å¾ˆå¯èƒ½æ˜¯ä¸€ä¸ªåˆ—è¡¨)ï¼Œé€’å½’åœ°æå–SQL
        return self._recursively_extract_sql(parsed_data)
    
    def evaluate_sql_quality(self, sql_list: List[str]) -> Dict:
        """è¯„ä¼°SQLè´¨é‡"""
        if not sql_list:
            return {
                'total_sql': 0,
                'valid_sql': 0,
                'matched_sql': 0,
                'excluded_sql': 0,
                'fingerprint_results': []
            }
        
        fingerprint_cache_path = self.config['data_config']['fingerprint_cache_path']
        fingerprint_results = []
        valid_count = 0
        matched_count = 0
        excluded_count = 0
        
        for sql in sql_list:
            if not sql.strip():
                continue
            
            try:
                match_result = match_single_sql(sql.strip(), fingerprint_cache_path)
                fingerprint_results.append({
                    'sql': sql,
                    'match_result': match_result
                })
                
                if not match_result.get('excluded', False):
                    valid_count += 1
                    if match_result.get('matched', False):
                        matched_count += 1
                else:
                    excluded_count += 1
                    
            except Exception as e:
                logger.warning(f"SQLéªŒè¯å¤±è´¥: {e}")
                fingerprint_results.append({
                    'sql': sql,
                    'match_result': {'error': str(e)}
                })
        
        return {
            'total_sql': len(sql_list),
            'valid_sql': valid_count,
            'matched_sql': matched_count,
            'excluded_sql': excluded_count,
            'fingerprint_results': fingerprint_results
        }
    
    def run_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
        
        # åŠ è½½éªŒè¯æ•°æ®
        eval_samples = self.load_eval_data()
        self.stats['total_samples'] = len(eval_samples)
        
        # é€ä¸ªå¤„ç†æ ·æœ¬
        for i, sample in enumerate(tqdm(eval_samples, desc="è¯„ä¼°è¿›åº¦")):
            
            # åˆ›å»ºæç¤ºè¯
            prompt = self.create_prompt(sample)
            
            # æ¨ç†
            response = self.run_inference(prompt)
            
            # å¤„ç†ç»“æœ
            result = {
                'sample_id': sample['sample_id'],
                'prompt': prompt,
                'response': response,
                'parsed_sql': [],
                'sql_evaluation': {},
                'inference_success': bool(response.strip())
            }
            
            if response.strip():
                self.stats['successful_inference'] += 1
                
                try:
                    # è§£æSQL
                    sql_list = self.parse_sql_response(response)
                    result['parsed_sql'] = sql_list
                    
                    if sql_list:
                        # SQLè´¨é‡è¯„ä¼°
                        sql_eval = self.evaluate_sql_quality(sql_list)
                        result['sql_evaluation'] = sql_eval
                        
                        # æ›´æ–°ç»Ÿè®¡
                        if sql_eval['valid_sql'] > 0:
                            self.stats['valid_sql_generated'] += 1
                        if sql_eval['matched_sql'] > 0:
                            self.stats['fingerprint_matched'] += 1
                    
                except Exception as e:
                    logger.warning(f"å¤„ç†ç¬¬ {i} ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {e}")
                    result['parse_error'] = str(e)
                    self.stats['parse_errors'] += 1
            else:
                self.stats['inference_errors'] += 1
            
            self.eval_results.append(result)
        
        # ç”Ÿæˆå’Œä¿å­˜ç»“æœ
        final_stats = self.generate_final_statistics()
        self.save_results(final_stats)
        
        logger.info("è¯„ä¼°å®Œæˆ!")
        return final_stats
    
    def generate_final_statistics(self) -> Dict:
        """ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡"""
        stats = self.stats.copy()
        
        total = stats['total_samples']
        if total > 0:
            stats['inference_success_rate'] = stats['successful_inference'] / total
            stats['valid_sql_rate'] = stats['valid_sql_generated'] / total
            stats['fingerprint_match_rate'] = stats['fingerprint_matched'] / total
            stats['parse_error_rate'] = stats['parse_errors'] / total
            stats['inference_error_rate'] = stats['inference_errors'] / total
        
        # SQLçº§åˆ«ç»Ÿè®¡
        total_sql = sum(len(r.get('parsed_sql', [])) for r in self.eval_results)
        valid_sql = sum(r.get('sql_evaluation', {}).get('valid_sql', 0) for r in self.eval_results)
        matched_sql = sum(r.get('sql_evaluation', {}).get('matched_sql', 0) for r in self.eval_results)
        
        stats['total_sql_generated'] = total_sql
        stats['total_valid_sql'] = valid_sql
        stats['total_matched_sql'] = matched_sql
        
        if total_sql > 0:
            stats['sql_validity_rate'] = valid_sql / total_sql
            stats['sql_match_rate'] = matched_sql / total_sql
        
        return stats
    
    def save_results(self, final_stats: Dict):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = self.output_dir / "evaluation_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'config': self.config,
                'statistics': final_stats,
                'detailed_results': self.eval_results
            }, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
        summary_file = self.output_dir / f"evaluation_summary_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(final_stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç»“æœå·²ä¿å­˜: {results_file}")
        logger.info(f"æ‘˜è¦å·²ä¿å­˜: {summary_file}")
        
        # æ‰“å°æ‘˜è¦
        self.print_summary(final_stats)
    
    def print_summary(self, stats: Dict):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "="*80)
        print("æ¨¡å‹è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*80)
        print(f"æ¨¡å‹è·¯å¾„: {self.config['model_config']['model_path']}")
        print(f"éªŒè¯é›†: {self.config['data_config']['eval_data_path']}")
        print(f"æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        
        print("\nğŸ“Š æ¨ç†ç»“æœ:")
        print(f"  âœ… æˆåŠŸæ¨ç†: {stats['successful_inference']}/{stats['total_samples']} ({stats.get('inference_success_rate', 0):.2%})")
        print(f"  âŒ æ¨ç†é”™è¯¯: {stats['inference_errors']} ({stats.get('inference_error_rate', 0):.2%})")
        print(f"  âš ï¸  è§£æé”™è¯¯: {stats['parse_errors']} ({stats.get('parse_error_rate', 0):.2%})")
        
        print("\nğŸ¯ SQLç”Ÿæˆè´¨é‡:")
        print(f"  ğŸ“ ç”Ÿæˆæœ‰æ•ˆSQLæ ·æœ¬: {stats['valid_sql_generated']}/{stats['total_samples']} ({stats.get('valid_sql_rate', 0):.2%})")
        print(f"  ğŸ¯ æŒ‡çº¹åŒ¹é…æ ·æœ¬: {stats['fingerprint_matched']}/{stats['total_samples']} ({stats.get('fingerprint_match_rate', 0):.2%})")
        
        print(f"\nğŸ“ˆ SQLè¯­å¥çº§åˆ«ç»Ÿè®¡:")
        print(f"  æ€»ç”ŸæˆSQLæ•°: {stats.get('total_sql_generated', 0)}")
        print(f"  æœ‰æ•ˆSQLæ•°: {stats.get('total_valid_sql', 0)} ({stats.get('sql_validity_rate', 0):.2%})")
        print(f"  æŒ‡çº¹åŒ¹é…SQLæ•°: {stats.get('total_matched_sql', 0)} ({stats.get('sql_match_rate', 0):.2%})")
        print("="*80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--config", type=str, required=True, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default=None, help="è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–è¯„ä¼°å™¨
        evaluator = SimpleModelEvaluator(config_path=args.config, output_dir_override=args.output_dir)
        
        # è¿è¡Œè¯„ä¼°
        results = evaluator.run_evaluation()
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {evaluator.output_dir}")
        
    except Exception as e:
        logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main() 