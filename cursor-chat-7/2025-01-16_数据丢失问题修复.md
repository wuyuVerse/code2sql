# 数据丢失问题修复总结

## 问题描述

在workflow_summary.json中发现数据流异常：
- 原始数据：5102条
- 关键词提取后：2863条
- 非关键词数据：1511条
- 丢失：728条记录（5102 - 2863 - 1511 = 728）

## 根本原因分析

### 1. 数据分离逻辑问题
**位置**：`data_processing/workflow/workflow_manager.py` 第2625-2640行

**问题**：使用`function_name:caller:source_file`作为唯一标识，但关键词提取时可能产生重复记录。

**修复**：改为使用`function_name:orm_code:caller`三元组作为唯一标识。

```python
# 修复前
def get_record_id(record):
    return f"{record.get('function_name', '')}:{record.get('caller', '')}:{record.get('source_file', '')}"

# 修复后
def get_record_id(record):
    orm_code = record.get('orm_code', '')
    orm_code_short = orm_code[:100] if len(orm_code) > 100 else orm_code
    return f"{record.get('function_name', '')}:{orm_code_short}:{record.get('caller', '')}"
```

### 2. 关键词提取重复问题
**位置**：`data_processing/data_reader.py` 第460-470行

**问题**：一个记录可能匹配多个关键词，导致同一记录被多次添加到结果中。

**修复**：在关键词提取过程中添加去重逻辑，使用`function_name:orm_code:caller`三元组确保每个记录只被处理一次。

```python
# 添加去重逻辑
processed_record_ids = set()

for record in self.records:
    # 生成记录的唯一标识
    orm_code_short = record.orm_code[:100] if len(record.orm_code) > 100 else record.orm_code
    record_id = f"{record.function_name}:{orm_code_short}:{record.caller}"
    
    # 如果记录已经处理过，跳过
    if record_id in processed_record_ids:
        continue
    
    # 处理匹配逻辑...
    if matched_keywords:
        matched_records.append(record_dict)
        processed_record_ids.add(record_id)  # 标记为已处理
```

## 修复效果

1. **数据完整性**：确保原始数据、关键词数据和非关键词数据的总和等于原始数据量
2. **去重一致性**：在数据分离和关键词提取两个步骤中使用相同的去重标准
3. **准确性提升**：使用`orm_code`作为标识的一部分，能更准确地识别唯一记录

## 涉及文件

1. `data_processing/workflow/workflow_manager.py` - 数据分离逻辑修复
2. `data_processing/data_reader.py` - 关键词提取去重逻辑修复

## 验证方法

重新运行工作流，检查workflow_summary.json中的数据流是否满足：
- 原始数据 = 关键词数据 + 非关键词数据
- 无数据丢失

## 注意事项

1. 数据集比较脚本仍使用`source_file:code_line:function_name`三元组，与工作流不一致
2. 如需统一，建议后续修改数据集比较脚本的匹配逻辑 