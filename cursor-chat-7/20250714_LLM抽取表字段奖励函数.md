# 20250714 LLM抽取表字段奖励函数

## 任务概述

基于已有的`code2sql_reward.py`，新增一个奖励函数，使用LLM从代码和已有的元信息做抽取表/字段操作，与sqlglot的parser结果对比看是否一致。

## 任务范围分析

### 需求理解
1. 使用LLM从ORM代码和元信息中抽取表名和字段名
2. 与sqlglot解析器的结果进行对比
3. 使用`llm_client.py`调用LLM服务
4. 从数据转换器读取的数据中获取源信息（function_name, orm_code, caller, code_meta_data）

### 执行计划
1. **定位代码插入点**：在`code2sql_reward.py`中新增LLM抽取表/字段的奖励函数
2. **实现LLM抽取逻辑**：使用`llm_client.py`调用LLM进行两次抽取（表名和字段名）
3. **实现对比逻辑**：与`sql_feature_extractor.py`的解析结果进行对比
4. **集成到现有奖励系统**：将新奖励函数集成到主奖励函数中

## 代码修改内容

### 1. 新增导入模块
```python
import json
import re
import yaml
from typing import Dict, Any, Optional, Set, List
from utils.llm_client import LLMClientManager
```

### 2. 新增配置管理功能
- `load_llm_prompts_config()`: 加载LLM提示词配置文件
- `get_llm_prompts_config()`: 获取LLM提示词配置（单例模式）
- 支持从`config/rl/qwen/llm_prompts.yaml`读取配置

### 3. 新增LLM抽取函数
- `extract_tables_and_columns_with_llm()`: 使用LLM从代码和元信息中抽取表名和字段名
- 调用两次LLM：一次抽取表名，一次抽取字段名
- 使用v3服务器进行LLM调用
- 解析JSON格式的响应结果
- **新增详细调试信息**：打印原始响应、JSON解析过程、最终结果

### 4. 新增对比函数
- `compare_extraction_results()`: 比较LLM抽取结果与sqlglot解析结果的一致性
- 计算表名和字段名的Jaccard相似度
- 使用加权平均计算综合分数（表名60% + 字段名40%）

### 5. 新增评估函数
- `evaluate_llm_extraction_reward()`: LLM抽取表/字段一致性奖励函数
- 从data_source中提取源信息（orm_code, code_meta_data, function_name, caller）
- 对每个SQL语句进行对比评估
- 计算平均一致性分数
- **新增详细调试信息**：检查data_source类型、打印提取的信息、SQL解析过程

### 6. 修改主奖励函数
- `format_and_llm_reward()`: 综合奖励函数
- 结合SQL有效性评估（60%）和LLM抽取一致性评估（40%）
- 输出详细的评估结果
- **新增详细调试信息**：打印输入参数、SQL提取过程、各阶段得分

### 7. 修改VERL框架入口函数
- `code2sql_reward()`: 支持两种调用方式
  - 单个样本：`code2sql_reward(data_source, solution_str, ground_truth, extra_info)`
  - 批量样本：`code2sql_reward(data_sources=data_sources, solution_strs=solution_strs, ...)`
- 添加参数验证和错误处理

### 8. 新增测试函数
- `test_llm_extraction()`: 测试LLM抽取功能
- `test_config_loading()`: 测试配置加载功能
- 提供示例ORM代码和元数据进行测试

## 配置文件创建

### LLM提示词配置文件
创建了`config/rl/qwen/llm_prompts.yaml`，包含：

```yaml
# 表名抽取提示词
table_extraction_prompt: |
  请从以下GORM代码中提取所有涉及的表名。
  ...

# 字段名抽取提示词
column_extraction_prompt: |
  请从以下GORM代码中提取所有涉及的字段名。
  ...

# LLM配置
llm_config:
  server_name: "v3"
  max_tokens: 1024
  temperature: 0.0
  max_retries: 3
  retry_delay: 1.0

# 一致性评估配置
consistency_config:
  table_weight: 0.6
  column_weight: 0.4
  consistency_weight: 0.4
  validity_weight: 0.6
```

## 调试信息增强

### 1. LLM抽取调试
```python
print(f"[LLM抽取] 开始调用LLM抽取表名，使用服务器: {server_name}")
print(f"[LLM抽取] 表名抽取原始响应: {repr(table_response)}")
print(f"[LLM抽取] 表名JSON字符串: {repr(json_str)}")
print(f"[LLM抽取] 解析到的表名: {tables}")
print(f"[LLM抽取] 最终抽取结果: {result}")
```

### 2. 评估过程调试
```python
print(f"[LLM抽取评估] data_source类型: {type(data_source)}")
print(f"[LLM抽取评估] data_source内容: {repr(data_source)}")
print(f"[LLM抽取评估] 提取的信息:")
print(f"  - orm_code长度: {len(orm_code)}")
print(f"  - code_meta_data类型: {type(code_meta_data)}, 长度: {len(code_meta_data)}")
print(f"[LLM抽取评估] 提取到的SQL数量: {len(extracted_sqls)}")
print(f"[LLM抽取评估] SQL {i+1} 一致性得分: {consistency_score}")
```

### 3. 综合评估调试
```python
print(f"[综合评估] 输入参数:")
print(f"  - data_source类型: {type(data_source)}")
print(f"  - solution_str类型: {type(solution_str)}, 长度: {len(solution_str) if solution_str else 0}")
print(f"  - solution_str内容: {repr(solution_str[:200])}...")
print(f"[综合评估] 提取到的SQL数量: {len(extracted_sqls)}")
print(f"[综合评估] SQL {i+1} 有效性得分: {score}")
```

## 核心功能实现

### LLM抽取逻辑
```python
def extract_tables_and_columns_with_llm(orm_code: str, code_meta_data: List[Dict], 
                                       function_name: str = "", caller: str = "") -> Dict[str, Set[str]]:
    # 获取配置
    config = get_llm_prompts_config()
    
    # 从配置中获取提示词模板
    table_extraction_prompt_template = config.get("table_extraction_prompt", "")
    column_extraction_prompt_template = config.get("column_extraction_prompt", "")
    
    # 格式化提示词
    table_extraction_prompt = table_extraction_prompt_template.format(
        function_name=function_name,
        caller=caller,
        orm_code=orm_code,
        meta_data_str=meta_data_str
    )
    
    # 从配置中获取LLM参数
    llm_config = config.get("llm_config", {})
    server_name = llm_config.get("server_name", "v3")
    max_tokens = llm_config.get("max_tokens", 1024)
    temperature = llm_config.get("temperature", 0.0)
    
    # 调用LLM并解析结果
    table_response = client.call_openai(table_extraction_prompt, max_tokens=max_tokens, temperature=temperature)
    column_response = client.call_openai(column_extraction_prompt, max_tokens=max_tokens, temperature=temperature)
```

### 一致性对比逻辑
```python
def compare_extraction_results(llm_result: Dict[str, Set[str]], 
                             sqlglot_result: Dict[str, Any]) -> float:
    # 获取配置
    config = get_llm_prompts_config()
    consistency_config = config.get("consistency_config", {})
    table_weight = consistency_config.get("table_weight", 0.6)
    column_weight = consistency_config.get("column_weight", 0.4)
    
    # 计算Jaccard相似度
    table_intersection = len(llm_tables & sqlglot_tables)
    table_union = len(llm_tables | sqlglot_tables)
    table_similarity = table_intersection / table_union if table_union > 0 else 0.0
    
    # 加权平均
    final_score = (table_similarity * table_weight + column_similarity * column_weight)
```

### 综合奖励计算
```python
def format_and_llm_reward(data_source: dict, solution_str: str, ground_truth: str, extra_info: Optional[dict] = None) -> float:
    # 获取配置
    config = get_llm_prompts_config()
    consistency_config = config.get("consistency_config", {})
    validity_weight = consistency_config.get("validity_weight", 0.6)
    consistency_weight = consistency_config.get("consistency_weight", 0.4)
    
    # 评估SQL有效性
    avg_validity_score = validity_score / len(extracted_sqls) if extracted_sqls else 0.0
    
    # 评估LLM抽取一致性
    consistency_score = evaluate_llm_extraction_reward(data_source, solution_str, ground_truth, extra_info)
    
    # 综合分数
    final_score = avg_validity_score * validity_weight + consistency_score * consistency_weight
```

## 数据源信息获取

### 从data_source中提取的字段
- `orm_code`: ORM代码
- `code_meta_data`: 代码元数据列表
- `function_name`: 函数名称
- `caller`: 调用者信息

### 代码元数据格式
```python
code_meta_data = [
    {
        "code_key": "User",
        "code_value": "type User struct {\n    ID   int    `gorm:\"column:id;primary_key\"`\n    Name string `gorm:\"column:name\"`\n    Email string `gorm:\"column:email\"`\n}",
        "code_file": "models/user.go"
    }
]
```

## 使用方式

### 1. 直接调用奖励函数
```python
from model.rl.code2sql_reward import code2sql_reward

# 准备数据
data_source = {
    "orm_code": "func GetUserInfo(ctx context.Context, db *gorm.DB, userID int) (*User, error) {...}",
    "code_meta_data": [...],
    "function_name": "GetUserInfo",
    "caller": ""
}

solution_str = "生成的SQL语句"
ground_truth = "标准答案"
extra_info = {}

# 计算奖励分数
reward_score = code2sql_reward(data_source, solution_str, ground_truth, extra_info)
```

### 2. 测试LLM抽取功能
```python
from model.rl.code2sql_reward import test_llm_extraction

# 运行测试
test_llm_extraction()
```

### 3. 在训练配置中使用
```yaml
reward_model:
  name: code2sql_reward
  # 其他配置...
```

## 技术特点

### 1. 配置化管理
- 提示词从配置文件读取，便于调整和优化
- LLM参数可配置，支持不同服务器和参数
- 权重参数可配置，便于调优

### 2. 双重LLM调用
- 分别调用LLM抽取表名和字段名
- 使用不同的提示词优化抽取效果
- 解析JSON格式的响应结果

### 3. 智能对比算法
- 使用Jaccard相似度计算一致性
- 表名和字段名分别计算，加权平均
- 处理空集合和异常情况

### 4. 综合奖励机制
- SQL有效性评估（可配置权重）
- LLM抽取一致性评估（可配置权重）
- 详细的评估日志输出

### 5. 完善的错误处理
- LLM调用失败时的降级处理
- JSON解析异常的处理
- SQL解析失败的处理
- 参数验证和类型检查

### 6. 详细的调试信息
- 打印原始LLM响应内容
- 显示JSON解析过程
- 输出各阶段评估分数
- 便于问题诊断和优化

## 预期效果

### 1. 更准确的奖励评估
- 不仅评估SQL语法有效性
- 还评估生成的SQL与源代码的一致性
- 提高模型生成SQL的准确性

### 2. 更好的训练效果
- 通过一致性评估引导模型学习正确的表名和字段名
- 减少生成的SQL与源代码不匹配的情况
- 提高模型的泛化能力

### 3. 详细的评估反馈
- 输出有效性分数和一致性分数
- 提供详细的评估日志
- 便于调试和优化

### 4. 灵活的配置管理
- 提示词可配置，便于优化
- 参数可调整，适应不同场景
- 权重可调优，平衡不同评估维度

## 注意事项

### 1. LLM服务依赖
- 需要确保v3服务器可用
- 需要配置正确的API密钥和端点
- 网络连接稳定性影响评估速度

### 2. 性能考虑
- LLM调用会增加评估时间
- 建议在训练时适当调整批次大小
- 可以考虑缓存LLM抽取结果

### 3. 数据格式要求
- data_source必须包含orm_code字段
- code_meta_data应该是列表格式
- 确保数据转换器正确传递源信息

### 4. 调试信息
- 添加了大量调试信息，便于问题诊断
- 在生产环境中可能需要调整日志级别
- 调试信息会显示原始响应内容，便于分析

## 总结

本次任务成功在`code2sql_reward.py`中新增了LLM抽取表/字段的奖励功能，实现了：

1. **配置化管理**：将LLM提示词和相关参数提取到配置文件中
2. **LLM抽取功能**：使用LLM从ORM代码和元信息中抽取表名和字段名
3. **一致性对比**：与sqlglot解析结果进行智能对比
4. **综合奖励机制**：结合SQL有效性和一致性评估
5. **完善的错误处理**：处理各种异常情况
6. **详细的调试信息**：提供详细的评估日志，便于问题诊断
7. **灵活的调用方式**：支持单个样本和批量样本两种调用方式

该奖励函数将显著提高模型生成SQL的准确性，特别是在表名和字段名的一致性方面，为Code2SQL任务提供了更精确的奖励信号。通过配置化管理，可以方便地调整和优化提示词和参数，适应不同的训练需求。 