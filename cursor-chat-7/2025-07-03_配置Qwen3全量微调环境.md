# 配置 Qwen3 全量微调环境

## 工作概述

本次工作主要围绕配置 Qwen3-14B 模型的全量微调环境展开，包括环境配置、代码结构优化和训练脚本编写。

## 主要工作内容

### 1. 环境配置
- 使用 Python 3.10 创建虚拟环境
- 安装 LLaMA-Factory 及其依赖
- 配置 SwanLab 实验跟踪系统
- 配置 CUDA 和深度学习相关依赖

### 2. 配置文件结构优化
- 重组 `config` 目录结构：
  ```
  config/
  ├── llm/            # LLM服务相关配置
  ├── validation/     # 验证相关配置
  └── training/       # 训练相关配置
      └── qwen/
          └── qwen3_14b_ft.yaml
  ```
- 优化配置文件组织，提高可维护性

### 3. 训练配置文件 (qwen3_14b_ft.yaml)
- 模型基本配置：使用 Qwen3-14B 基础模型
- 全量微调配置：
  - `finetuning_type: full`
  - 启用 flash attention 2
  - 使用 linear rope scaling
- 训练参数配置：
  - 学习率：5e-5
  - 训练轮数：3
  - 批次大小：2
  - 梯度累积步数：8
  - 使用 cosine 学习率调度
- 实验跟踪：启用 SwanLab

### 4. 训练脚本开发
- 创建 `train_qwen3_ft.py` 训练脚本
- 实现功能：
  - 动态生成带时间戳的输出目录
  - 支持分布式训练（8卡）
  - SwanLab 实验跟踪集成
  - 完整的日志记录系统
  - 配置文件验证和错误处理

### 5. 训练环境特点
- 支持 8 卡分布式训练
- 每次训练自动生成唯一输出目录
- 实验过程可在 SwanLab 中追踪
- 使用 bf16 混合精度训练

## 使用说明

1. 启动训练：
```bash
cd /home/wuyu/code2sql/model
./training/train_qwen3_ft.py
```

2. 训练输出：
- 模型保存在 `saves/qwen3-14b-ft-{timestamp}` 目录
- 训练日志可在 SwanLab 界面查看
- 支持断点续训和模型检查点保存

## 注意事项
- 确保有足够的 GPU 显存（8卡环境）
- 训练前检查模型路径是否正确
- 确保 SwanLab 服务器已启动
- 注意保存的模型文件大小，及时清理不需要的检查点 