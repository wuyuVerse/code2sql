# Model目录结构重构与配置优化

**日期**: 2025年7月4日  
**主题**: model目录结构重构，优化数据集配置，修改qwen3_14b_ft.yaml训练脚本

## 问题背景

用户发现model目录中存在多个data目录，结构混乱：
- `model/data/` - 包含一些示例数据和ORM2SQL训练数据
- `model/LLaMA-Factory/data/` - LLaMA-Factory框架自带的数据目录  
- `model/training/data/` - 重复的数据文件

需要重新梳理目录结构，并修改训练配置以使用ORM2SQL数据集。

## 解决方案

### 1. 目录结构重构

**重构前**:
```
model/
├── data/                    # 混乱的数据目录
│   ├── orm2sql_training_data/
│   ├── identity.json
│   ├── alpaca_zh_demo.json
│   └── dataset_info.json
├── LLaMA-Factory/
│   └── data/               # 框架自带数据目录
├── training/
│   └── data/               # 重复的数据目录
├── evaluation/
└── merging/
```

**重构后**:
```
model/
├── LLaMA-Factory/          # 训练框架（保持不变）
│   └── data/              # 包含所有数据集配置
├── datasets/               # 统一的数据集存储目录
│   └── orm2sql/           # ORM到SQL数据集
├── configs/               # 训练配置文件
├── training/              # 训练脚本和日志
│   └── saves/            # 模型保存目录
├── evaluation/            # 评估相关
└── merging/              # 模型合并相关
```

### 2. 具体操作步骤

1. **创建新目录结构**:
   ```bash
   mkdir -p model/datasets model/configs
   ```

2. **移动ORM2SQL数据集**:
   ```bash
   mv model/data/orm2sql_training_data model/datasets/orm2sql
   ```

3. **移动配置文件**:
   ```bash
   mv config/training/qwen/qwen3_14b_ft.yaml model/configs/
   ```

4. **复制数据文件到LLaMA-Factory**:
   ```bash
   cp model/datasets/orm2sql/dataset_info_llamafactory.json model/LLaMA-Factory/data/
   cp model/datasets/orm2sql/orm2sql_training_data_20250704_121704.json model/LLaMA-Factory/data/
   ```

5. **更新dataset_info.json配置**:
   在`model/LLaMA-Factory/data/dataset_info.json`中添加：
   ```json
   "orm2sql_training": {
     "file_name": "orm2sql_training_data_20250704_121704.json",
     "formatting": "sharegpt",
     "columns": {
       "messages": "conversations"
     },
     "tags": {
       "role_tag": "role",
       "content_tag": "content", 
       "user_tag": "user",
       "assistant_tag": "assistant"
     },
     "num_samples": 17761,
     "description": "ORM到SQL转换训练数据集，基于真实GORM代码分析生成SQL语句"
   }
   ```

6. **清理重复目录**:
   ```bash
   rm -rf model/data model/training/data
   ```

### 3. 训练配置优化

修改`model/configs/qwen3_14b_ft.yaml`的关键配置：

**数据集配置**:
- `dataset`: `alpaca_zh_demo` → `orm2sql_training`
- `max_samples`: `1000` → `17761` (使用全部样本)
- `dataset_dir`: 指向`/home/wuyu/code2sql/model/LLaMA-Factory/data`

**训练参数调整**:
- `learning_rate`: `1.0e-5` → `2.0e-5` (针对代码生成任务)
- `num_train_epochs`: `3.0` → `2.0` (减少训练轮次)
- `per_device_train_batch_size`: `2` → `1` (考虑内存限制)
- `gradient_accumulation_steps`: `8` → `16` (保持有效batch size)

**日志和保存**:
- `logging_steps`: `5` → `10`
- `save_steps`: `500` → `278` (每个epoch保存一次)
- `output_dir`: 指向`/home/wuyu/code2sql/model/training/saves/qwen3-14b-orm2sql-ft`

### 4. 训练步数计算

```
实际Batch Size = 1 * 4 GPUs * 16 = 64
Steps per Epoch = 17761 samples / 64 = ~278 steps  
Total Steps = 278 * 2 epochs = 556 steps
```

## 结果

✅ **目录结构清晰化**: 数据集统一管理，配置文件集中存放  
✅ **配置文件优化**: 使用ORM2SQL数据集，参数针对代码生成任务调优  
✅ **重复文件清理**: 移除冗余的data目录，避免混淆  
✅ **路径标准化**: 所有路径使用绝对路径，避免相对路径问题

## 使用方法

训练启动命令：
```bash
cd /home/wuyu/code2sql/model/LLaMA-Factory
python src/train.py --config /home/wuyu/code2sql/model/configs/qwen3_14b_ft.yaml
```

## 数据集信息

- **数据集名称**: orm2sql_training  
- **样本数量**: 17,761  
- **数据格式**: Alpaca格式  
- **数据描述**: 基于真实GORM代码分析生成的ORM到SQL转换训练数据集

## 训练脚本问题解决

### 问题1: 数据集格式错误
**错误**: `KeyError: 'conversations'`
**原因**: 配置为ShareGPT格式但数据是Alpaca格式
**解决**: 修正dataset_info.json配置
```json
"formatting": "alpaca",
"columns": {
  "prompt": "instruction", 
  "response": "output"
}
```

### 问题2: 配置文件路径
**错误**: 脚本中的配置路径指向旧目录结构
**解决**: 更新train_qwen3_ft.py中的路径为新结构

### 训练成功启动
- ✅ **GPU状态**: 4×H20 GPU 100%利用率，72GB显存使用
- ✅ **分布式训练**: DeepSpeed ZeRO Stage 3正常运行  
- ✅ **数据加载**: Alpaca格式数据集加载成功
- ✅ **模型加载**: Qwen3-14B模型正常加载

## nohup双实例部署

### 部署方案
使用一个脚本传入不同配置文件，启动两个训练实例：

**实例1（GPU 0-3）**:
```bash
nohup bash -c "cd model/LLaMA-Factory && source .venv/bin/activate && cd /home/wuyu/code2sql && python model/training/train_qwen3_ft.py --config qwen3_14b_ft.yaml" > model/training/logs/train_gpu0123_$(date +%Y%m%d_%H%M%S).log 2>&1 &
```

**实例2（GPU 4-7）**:
```bash
nohup bash -c "cd model/LLaMA-Factory && source .venv/bin/activate && cd /home/wuyu/code2sql && python model/training/train_qwen3_ft.py --config qwen3_14b_ft_gpu4567.yaml" > model/training/logs/train_gpu4567_$(date +%Y%m%d_%H%M%S).log 2>&1 &
```

### 监控工具
- **监控脚本**: `./model/training/monitor_training.sh`
- **日志目录**: `model/training/logs/`
- **SwanLab跟踪**: 两个独立的实验运行

### 最终状态
- ✅ **8×H20 GPU**: 全部100%利用率运行
- ✅ **双实例训练**: 两个Qwen3-14B模型同时微调
- ✅ **预计时间**: 每个实例约5.5小时完成(556步)
- ✅ **资源隔离**: GPU 0-3 vs GPU 4-7，避免冲突 