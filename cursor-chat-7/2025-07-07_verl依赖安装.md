# 2025-07-07 verl 依赖安装问题排查与解决总结

## 场景
- 在 `model/rl` 目录下克隆了 verl 仓库，需要在 Python 3.10 + CUDA 12.4 环境完成依赖安装。
- 关键库：torch 2.6.0、vLLM 0.8.5.post1、sglang 0.4.6.post1、flashinfer 0.2.2.post1、flash-attn 2.7.4.post1、Transformer-Engine、Megatron-core 等。
- 已准备本地 wheel：`flashinfer_python-0.2.2.post1+cu124torch2.6-cp38-abi3-linux_x86_64.whl`。

## 过程中遇到的主要问题
1. **虚拟环境与 pip 版本错配**：uv 创建的环境被系统 Python 3.13 的 pip 污染。
2. **大包网络下载缓慢 / 超时**：vLLM、sglang、Transformer-Engine 源码拉取速度慢。
3. **依赖冲突**：
   - `vllm` 需要 `opentelemetry-{api,sdk}==1.26.*`，但自动解析装到了 1.34.1；
   - `opencv-python-headless 4.12.0.88` 需要 `numpy>=2,<2.3`，而当前环境中为 `numpy 1.26.4`；
   - flash-attn wheel 校验失败，需要重新获取与 torch 2.6 对应的 cu124 版本。
4. **试错多次后决定重建环境**：删除 `.venv-verl`，重新用 `python -m venv .venv` 创建纯净 py3.10 环境。

## 解决策略
1. **规范安装顺序**
   ```bash
   source .venv/bin/activate
   python -m pip install -U pip setuptools wheel
   # 1) 先装 GPU 核心库和 flashinfer 本地 wheel
   python -m pip install --no-index ./flashinfer_python-0.2.2.post1+cu124torch2.6-cp38-abi3-linux_x86_64.whl
   python -m pip install --extra-index-url https://download.pytorch.org/whl/nightly/cu124 "torch==2.6.0" "torchvision==0.18.0" "torchaudio==2.6.0"
   # 必要时锁定低层 CUDA 依赖
   python -m pip install "nvidia-cudnn-cu12==9.1.0.70" "nvidia-cublas-cu12==12.4.5.8"
   # 2) 装 transformer-engine / megatron-core wheel（已存在 cu12 预编译）
   python -m pip install transformer-engine==2.2.0 megatron-core==0.12.1
   # 3) 解决 opentelemetry 依赖
   python -m pip install "opentelemetry-api==1.26.0" "opentelemetry-sdk==1.26.0" \
                          "opentelemetry-exporter-otlp-proto-grpc==1.26.0" \
                          "opentelemetry-exporter-otlp-proto-http==1.26.0" \
                          opentelemetry-semantic-conventions-ai==0.42b0
   # 4) 安装 vLLM / sglang / flash-attn（若有本地 wheel 优先）
   python -m pip install vllm==0.8.5.post1 sglang==0.4.6.post1
   # flash-attn 建议重新下载匹配 torch2.6+cu124 的 wheel；若无则源码编译。
   ```
2. **处理 numpy / opencv 冲突**
   - 若保持 `numpy 1.26.4`：
     ```bash
     python -m pip install "opencv-python-headless<4.12"
     ```
   - 或者升级 numpy 至 2.2.x（确认与 torch2.6 兼容）再装新 opencv。
3. **验证**
   ```python
   python - <<'PY'
   import torch, flashinfer, vllm, sglang, transformer_engine, megatron
   print(torch.__version__, torch.cuda.is_available())
   print(flashinfer.__version__)
   print(vllm.__version__)
   PY
   ```
4. **最终检查**
   ```bash
   python -m pip check
   ```
   若无输出表示依赖全部 satisfied。

## 未解决 / 风险
- flash-attn 2.7.4.post1 若没有官方 cu124 / torch2.6 预编译 wheel，需要纯源码编译，需预先设置 `CUTLASS_PATH` 并安装 `cmake` `ninja`。

---
> 提醒：如后续依赖仍有冲突，应优先通过 `pip check` 或 `pip install --dry-run` 观察 resolver 结果，再决定升 / 降级策略。 